{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 假设文档嵌入 (HyDE) 用于 RAG\n",
    "\n",
    "在这个笔记本中，我实现了 HyDE（假设文档嵌入）- 一种创新的检索技术，它在执行检索之前将用户查询转换为假设的答案文档。这种方法弥合了短查询和长文档之间的语义差距。\n",
    "\n",
    "传统的 RAG 系统直接嵌入用户的短查询，但这往往无法捕获最佳检索所需的语义丰富性。HyDE 通过以下方式解决这个问题：\n",
    "\n",
    "- 生成一个回答查询的假设文档\n",
    "- 嵌入这个扩展的文档而不是原始查询\n",
    "- 检索与这个假设文档相似的文档\n",
    "- 创建更具上下文相关性的答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置环境\n",
    "我们首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置 OpenAI API 客户端\n",
    "我们初始化 OpenAI 客户端来生成嵌入向量和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用基础 URL 和 API 密钥初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 从环境变量中获取 API 密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    从 PDF 文件中提取文本内容，并按页面分离。\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 文件路径\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 包含文本内容和元数据的页面列表\n",
    "    \"\"\"\n",
    "    print(f\"正在从 {pdf_path} 提取文本...\")  # 打印正在处理的 PDF 路径\n",
    "    pdf = fitz.open(pdf_path)  # 使用 PyMuPDF 打开 PDF 文件\n",
    "    pages = []  # 初始化空列表来存储包含文本内容的页面\n",
    "    \n",
    "    # 遍历 PDF 中的每一页\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]  # 获取当前页面\n",
    "        text = page.get_text()  # 从当前页面提取文本\n",
    "        \n",
    "        # 跳过文本很少的页面（少于 50 个字符）\n",
    "        if len(text.strip()) > 50:\n",
    "            # 将页面文本和元数据追加到列表中\n",
    "            pages.append({\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": pdf_path,  # 源文件路径\n",
    "                    \"page\": page_num + 1  # 页码（从 1 开始的索引）\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"提取了 {len(pages)} 页内容\")  # 打印提取的页面数量\n",
    "    return pages  # 返回包含文本内容和元数据的页面列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    将文本分割为重叠的块。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 要分块的输入文本\n",
    "        chunk_size (int): 每个块的字符大小\n",
    "        overlap (int): 块之间的重叠字符数\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 包含元数据的块列表\n",
    "    \"\"\"\n",
    "    chunks = []  # 初始化空列表来存储块\n",
    "    \n",
    "    # 以 (chunk_size - overlap) 的步长遍历文本\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_text = text[i:i + chunk_size]  # 提取文本块\n",
    "        if chunk_text:  # 确保不添加空块\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,  # 添加块文本\n",
    "                \"metadata\": {\n",
    "                    \"start_pos\": i,  # 块在原始文本中的起始位置\n",
    "                    \"end_pos\": i + len(chunk_text)  # 块在原始文本中的结束位置\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"创建了 {len(chunks)} 个文本块\")  # 打印创建的块数量\n",
    "    return chunks  # 返回包含元数据的块列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单向量存储实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    使用 NumPy 的简单向量存储实现。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # 存储向量嵌入的列表\n",
    "        self.texts = []  # 存储文本内容的列表\n",
    "        self.metadata = []  # 存储元数据的列表\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        向向量存储中添加项目。\n",
    "        \n",
    "        Args:\n",
    "            text (str): 文本内容\n",
    "            embedding (List[float]): 向量嵌入\n",
    "            metadata (Dict, optional): 附加元数据\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 将嵌入作为 numpy 数组追加\n",
    "        self.texts.append(text)  # 追加文本内容\n",
    "        self.metadata.append(metadata or {})  # 追加元数据或空字典（如果为 None）\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入最相似的项目。\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): 查询嵌入向量\n",
    "            k (int): 要返回的结果数量\n",
    "            filter_func (callable, optional): 过滤结果的函数\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: 前 k 个最相似的项目\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 如果没有向量，返回空列表\n",
    "        \n",
    "        # 将查询嵌入转换为 numpy 数组\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 使用余弦相似度计算相似性\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 如果不通过过滤器则跳过\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # 计算余弦相似度\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 追加索引和相似度分数\n",
    "        \n",
    "        # 按相似度排序（降序）\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 返回前 k 个结果\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # 添加文本内容\n",
    "                \"metadata\": self.metadata[idx],  # 添加元数据\n",
    "                \"similarity\": float(score)  # 添加相似度分数\n",
    "            })\n",
    "        \n",
    "        return results  # 返回前 k 个结果列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    为给定的文本创建嵌入向量。\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): 输入文本\n",
    "        model (str): 嵌入模型名称\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: 嵌入向量\n",
    "    \"\"\"\n",
    "    # 处理空输入\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # 如果需要，分批处理（OpenAI API 限制）\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # 分批遍历输入文本\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # 获取当前批次的文本\n",
    "        \n",
    "        # 为当前批次创建嵌入向量\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # 从响应中提取嵌入向量\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # 将批次嵌入向量添加到列表中\n",
    "    \n",
    "    return all_embeddings  # 返回所有嵌入向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档处理流水线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    为 RAG 处理文档。\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 文件路径\n",
    "        chunk_size (int): 每个块的字符大小\n",
    "        chunk_overlap (int): 块之间的重叠字符数\n",
    "        \n",
    "    Returns:\n",
    "        SimpleVectorStore: 包含文档块的向量存储\n",
    "    \"\"\"\n",
    "    # 从 PDF 文件中提取文本\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 处理每个页面并创建块\n",
    "    all_chunks = []\n",
    "    for page in pages:\n",
    "        # 将文本内容（字符串）传递给 chunk_text，而不是字典\n",
    "        page_chunks = chunk_text(page[\"text\"], chunk_size, chunk_overlap)\n",
    "        \n",
    "        # 用页面的元数据更新每个块的元数据\n",
    "        for chunk in page_chunks:\n",
    "            chunk[\"metadata\"].update(page[\"metadata\"])\n",
    "        \n",
    "        all_chunks.extend(page_chunks)\n",
    "    \n",
    "    # 为文本块创建嵌入向量\n",
    "    print(\"正在为块创建嵌入向量...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # 创建向量存储来保存块及其嵌入向量\n",
    "    vector_store = SimpleVectorStore()\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        vector_store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=chunk_embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"创建了包含 {len(all_chunks)} 个块的向量存储\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 假设文档生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_document(query, desired_length=1000):\n",
    "    \"\"\"\n",
    "    生成一个回答查询的假设文档。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        desired_length (int): 假设文档的目标长度\n",
    "        \n",
    "    Returns:\n",
    "        str: 生成的假设文档\n",
    "    \"\"\"\n",
    "    # 定义指导模型如何生成文档的系统提示\n",
    "    system_prompt = f\"\"\"您是一位专业的文档创建者。\n",
    "    给定一个问题，生成一个能够直接回答这个问题的详细文档。\n",
    "    文档应该大约 {desired_length} 个字符长，并提供对问题的深入、\n",
    "    信息丰富的答案。写作时就像这个文档来自该主题的权威来源。\n",
    "    包含具体的细节、事实和解释。\n",
    "    不要提及这是一个假设文档 - 直接写内容即可。\"\"\"\n",
    "\n",
    "    # 定义包含查询的用户提示\n",
    "    user_prompt = f\"问题：{query}\\n\\n生成一个完全回答这个问题的文档：\"\n",
    "    \n",
    "    # 向 OpenAI API 发出请求以生成假设文档\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # 指定要使用的模型\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # 指导助手的系统消息\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # 包含查询的用户消息\n",
    "        ],\n",
    "        temperature=0.1  # 设置响应生成的温度\n",
    "    )\n",
    "    \n",
    "    # 返回生成的文档内容\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的 HyDE RAG 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyde_rag(query, vector_store, k=5, should_generate_response=True):\n",
    "    \"\"\"\n",
    "    使用假设文档嵌入执行 RAG。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        vector_store (SimpleVectorStore): 包含文档块的向量存储\n",
    "        k (int): 要检索的块数量\n",
    "        should_generate_response (bool): 是否生成最终响应\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 包括假设文档和检索块的结果\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 使用 HyDE 处理查询：{query} ===\\n\")\n",
    "    \n",
    "    # 步骤 1：生成一个回答查询的假设文档\n",
    "    print(\"正在生成假设文档...\")\n",
    "    hypothetical_doc = generate_hypothetical_document(query)\n",
    "    print(f\"生成了 {len(hypothetical_doc)} 个字符的假设文档\")\n",
    "    \n",
    "    # 步骤 2：为假设文档创建嵌入向量\n",
    "    print(\"正在为假设文档创建嵌入向量...\")\n",
    "    hypothetical_embedding = create_embeddings([hypothetical_doc])[0]\n",
    "    \n",
    "    # 步骤 3：基于假设文档检索相似块\n",
    "    print(f\"正在检索 {k} 个最相似的块...\")\n",
    "    retrieved_chunks = vector_store.similarity_search(hypothetical_embedding, k=k)\n",
    "    \n",
    "    # 准备结果字典\n",
    "    results = {\n",
    "        \"query\": query,\n",
    "        \"hypothetical_document\": hypothetical_doc,\n",
    "        \"retrieved_chunks\": retrieved_chunks\n",
    "    }\n",
    "    \n",
    "    # 步骤 4：如果需要，生成响应\n",
    "    if should_generate_response:\n",
    "        print(\"正在生成最终响应...\")\n",
    "        response = generate_response(query, retrieved_chunks)\n",
    "        results[\"response\"] = response\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于比较的标准（直接）RAG 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_rag(query, vector_store, k=5, should_generate_response=True):\n",
    "    \"\"\"\n",
    "    使用直接查询嵌入执行标准 RAG。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        vector_store (SimpleVectorStore): 包含文档块的向量存储\n",
    "        k (int): 要检索的块数量\n",
    "        should_generate_response (bool): 是否生成最终响应\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 包括检索块的结果\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 使用标准 RAG 处理查询：{query} ===\\n\")\n",
    "    \n",
    "    # 步骤 1：为查询创建嵌入向量\n",
    "    print(\"正在为查询创建嵌入向量...\")\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # 步骤 2：基于查询嵌入向量检索相似块\n",
    "    print(f\"正在检索 {k} 个最相似的块...\")\n",
    "    retrieved_chunks = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # 准备结果字典\n",
    "    results = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_chunks\": retrieved_chunks\n",
    "    }\n",
    "    \n",
    "    # 步骤 3：如果需要，生成响应\n",
    "    if should_generate_response:\n",
    "        print(\"正在生成最终响应...\")\n",
    "        response = generate_response(query, retrieved_chunks)\n",
    "        results[\"response\"] = response\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 响应生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    基于检索到的块生成响应。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        retrieved_chunks (List[Dict]): 检索到的文本块\n",
    "        \n",
    "    Returns:\n",
    "        str: 生成的响应\n",
    "    \"\"\"\n",
    "    # 从检索到的块中提取文本\n",
    "    context_texts = [chunk[\"text\"] for chunk in retrieved_chunks]\n",
    "    \n",
    "    # 将提取的文本合并为单个上下文字符串，用 \"---\" 分隔\n",
    "    combined_context = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    \n",
    "    # 定义上下文的最大允许长度（OpenAI 限制）\n",
    "    max_context = 14000\n",
    "    \n",
    "    # 如果合并的上下文超过最大长度，则截断\n",
    "    if len(combined_context) > max_context:\n",
    "        combined_context = combined_context[:max_context] + \"... [已截断]\"\n",
    "    \n",
    "    # 定义指导 AI 助手的系统消息\n",
    "    system_message = \"\"\"您是一个有用的 AI 助手。基于提供的上下文回答用户的问题。\n",
    "如果信息不在上下文中，请说明。在可能的情况下，在您的答案中引用上下文的特定部分。\"\"\"\n",
    "\n",
    "    # 使用 OpenAI API 生成响应\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # 指定要使用的模型\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # 指导助手的系统消息\n",
    "            {\"role\": \"user\", \"content\": f\"上下文：\\n{combined_context}\\n\\n问题：{query}\"}  # 包含上下文和查询的用户消息\n",
    "        ],\n",
    "        temperature=0.2  # 设置响应生成的温度\n",
    "    )\n",
    "    \n",
    "    # 返回生成的响应内容\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(query, vector_store, reference_answer=None):\n",
    "    \"\"\"\n",
    "    比较 HyDE 和标准 RAG 方法对查询的处理。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        vector_store (SimpleVectorStore): 包含文档块的向量存储\n",
    "        reference_answer (str, optional): 用于评估的参考答案\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 比较结果\n",
    "    \"\"\"\n",
    "    # 运行 HyDE RAG\n",
    "    hyde_result = hyde_rag(query, vector_store)\n",
    "    hyde_response = hyde_result[\"response\"]\n",
    "    \n",
    "    # 运行标准 RAG\n",
    "    standard_result = standard_rag(query, vector_store)\n",
    "    standard_response = standard_result[\"response\"]\n",
    "    \n",
    "    # 比较结果\n",
    "    comparison = compare_responses(query, hyde_response, standard_response, reference_answer)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"hyde_response\": hyde_response,\n",
    "        \"hyde_hypothetical_doc\": hyde_result[\"hypothetical_document\"],\n",
    "        \"standard_response\": standard_response,\n",
    "        \"reference_answer\": reference_answer,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, hyde_response, standard_response, reference=None):\n",
    "    \"\"\"\n",
    "    比较 HyDE 和标准 RAG 的响应。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        hyde_response (str): HyDE RAG 的响应\n",
    "        standard_response (str): 标准 RAG 的响应\n",
    "        reference (str, optional): 参考答案\n",
    "        \n",
    "    Returns:\n",
    "        str: 比较分析\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"您是信息检索系统的专家评估员。\n",
    "比较对同一查询的两个响应，一个使用 HyDE（假设文档嵌入）生成，\n",
    "另一个使用直接查询嵌入的标准 RAG 生成。\n",
    "\n",
    "基于以下方面评估它们：\n",
    "1. 准确性：哪个响应提供了更多事实正确的信息？\n",
    "2. 相关性：哪个响应更好地解决了查询？\n",
    "3. 完整性：哪个响应提供了更全面的主题覆盖？\n",
    "4. 清晰度：哪个响应组织得更好，更容易理解？\n",
    "\n",
    "在分析每种方法的优缺点时要具体。\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"查询：{query}\n",
    "\n",
    "HyDE RAG 的响应：\n",
    "{hyde_response}\n",
    "\n",
    "标准 RAG 的响应：\n",
    "{standard_response}\"\"\"\n",
    "\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "\n",
    "参考答案：\n",
    "{reference}\"\"\"\n",
    "\n",
    "    user_prompt += \"\"\"\n",
    "\n",
    "请提供这两个响应的详细比较，突出哪种方法表现更好以及原因。\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例使用和评估\n",
    "\n",
    "以下代码演示了如何使用 HyDE RAG 系统并与标准 RAG 进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # PDF 文档路径\n",
    "    pdf_path = \"data/AI_Information.pdf\"\n",
    "    \n",
    "    # 测试查询\n",
    "    query = \"人工智能开发中的主要伦理考虑因素有哪些？\"\n",
    "    \n",
    "    # 处理文档并创建向量存储\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 运行 HyDE RAG\n",
    "    hyde_result = hyde_rag(query, vector_store)\n",
    "    \n",
    "    print(f\"查询：{hyde_result['query']}\")\n",
    "    print(f\"假设文档长度：{len(hyde_result['hypothetical_document'])} 个字符\")\n",
    "    print(f\"检索的块数量：{len(hyde_result['retrieved_chunks'])}\")\n",
    "    print(f\"HyDE 响应：{hyde_result['response']}\")\n",
    "    \n",
    "    # 运行标准 RAG 进行比较\n",
    "    standard_result = standard_rag(query, vector_store)\n",
    "    print(f\"\\n标准 RAG 响应：{standard_result['response']}\")\n",
    "    \n",
    "    # 比较两种方法\n",
    "    comparison = compare_approaches(query, vector_store)\n",
    "    print(f\"\\n比较分析：{comparison['comparison']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "假设文档嵌入 (HyDE) 通过以下方式改进了传统的 RAG 系统：\n",
    "\n",
    "1. **语义桥接**：通过生成假设文档来弥合短查询和长文档之间的语义差距\n",
    "2. **上下文扩展**：将简短的查询扩展为丰富的假设答案，提供更好的检索上下文\n",
    "3. **改进的匹配**：假设文档通常与实际相关文档具有更好的语义相似性\n",
    "4. **查询理解**：通过生成过程更好地理解查询意图\n",
    "\n",
    "这种方法特别适用于：\n",
    "- 复杂或抽象的查询\n",
    "- 需要深度理解的问题\n",
    "- 查询和文档之间存在词汇差距的情况\n",
    "- 需要综合多个概念的查询\n",
    "\n",
    "HyDE 代表了检索增强生成技术的重要进步，为构建更智能、更具上下文感知能力的问答系统提供了强大的框架。通过首先生成假设答案然后检索相关内容，HyDE 能够更好地理解用户意图并提供更准确的响应。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}