{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 纠错 RAG (CRAG) 实现\n",
    "\n",
    "在这个笔记本中，我实现了纠错 RAG - 一种先进的方法，它动态评估检索到的信息，并在必要时纠正检索过程，使用网络搜索作为后备方案。\n",
    "\n",
    "CRAG 通过以下方式改进传统 RAG：\n",
    "\n",
    "- 在使用检索内容之前对其进行评估\n",
    "- 根据相关性动态切换知识源\n",
    "- 当本地知识不足时，通过网络搜索纠正检索\n",
    "- 在适当时结合来自多个源的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置环境\n",
    "我们首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import re\n",
    "from urllib.parse import quote_plus\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置 OpenAI API 客户端\n",
    "我们初始化 OpenAI 客户端来生成嵌入向量和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用基础 URL 和 API 密钥初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 从环境变量中获取 API 密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    从 PDF 文件中提取文本内容。\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 文件路径\n",
    "        \n",
    "    Returns:\n",
    "        str: 提取的文本内容\n",
    "    \"\"\"\n",
    "    print(f\"正在从 {pdf_path} 提取文本...\")\n",
    "    \n",
    "    # 打开 PDF 文件\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # 遍历 PDF 中的每一页\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]\n",
    "        # 从当前页面提取文本并追加到 text 变量中\n",
    "        text += page.get_text()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    将文本分割为重叠的块，以便高效检索和处理。\n",
    "    \n",
    "    此函数将大文本分割为较小的、可管理的块，并在连续块之间\n",
    "    指定重叠。分块对 RAG 系统至关重要，因为它允许更精确地\n",
    "    检索相关信息。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 要分块的输入文本\n",
    "        chunk_size (int): 每个块的最大字符大小\n",
    "        overlap (int): 连续块之间的重叠字符数，\n",
    "                       用于在块边界之间保持上下文\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 文本块列表，每个包含：\n",
    "                   - text: 块内容\n",
    "                   - metadata: 包含位置信息和源类型的字典\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # 使用滑动窗口方法遍历文本\n",
    "    # 移动 (chunk_size - overlap) 确保块之间的适当重叠\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        # 提取当前块，受 chunk_size 限制\n",
    "        chunk_text = text[i:i + chunk_size]\n",
    "        \n",
    "        # 只添加非空块\n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,  # 实际文本内容\n",
    "                \"metadata\": {\n",
    "                    \"start_pos\": i,  # 在原始文本中的起始位置\n",
    "                    \"end_pos\": i + len(chunk_text),  # 结束位置\n",
    "                    \"source_type\": \"document\"  # 表示此文本的来源\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"创建了 {len(chunks)} 个文本块\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单向量存储实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    使用 NumPy 的简单向量存储实现。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初始化列表来存储向量、文本和元数据\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        向向量存储中添加项目。\n",
    "        \n",
    "        Args:\n",
    "            text (str): 文本内容\n",
    "            embedding (List[float]): 嵌入向量\n",
    "            metadata (Dict, optional): 附加元数据\n",
    "        \"\"\"\n",
    "        # 将嵌入、文本和元数据追加到各自的列表中\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        向向量存储中添加多个项目。\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): 包含文本和元数据的项目列表\n",
    "            embeddings (List[List[float]]): 嵌入向量列表\n",
    "        \"\"\"\n",
    "        # 遍历项目和嵌入并将它们添加到存储中\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                text=item[\"text\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入最相似的项目。\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): 查询嵌入向量\n",
    "            k (int): 要返回的结果数量\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: 前 k 个最相似的项目\n",
    "        \"\"\"\n",
    "        # 如果存储中没有向量，返回空列表\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 将查询嵌入转换为 numpy 数组\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 使用余弦相似度计算相似性\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 按相似度排序（降序）\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 返回前 k 个结果\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的嵌入模型为文本输入创建向量嵌入。\n",
    "    \n",
    "    嵌入是捕获语义含义的文本的密集向量表示，\n",
    "    允许进行相似性比较。在 RAG 系统中，嵌入对于\n",
    "    将查询与相关文档块匹配至关重要。\n",
    "    \n",
    "    Args:\n",
    "        texts (str or List[str]): 要嵌入的输入文本。可以是单个字符串\n",
    "                                  或字符串列表。\n",
    "        model (str): 要使用的嵌入模型名称。默认为 \"text-embedding-3-small\"。\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: 如果输入是列表，返回嵌入向量列表。\n",
    "                          如果输入是单个字符串，返回单个嵌入向量。\n",
    "    \"\"\"\n",
    "    # 通过将单个字符串转换为列表来处理单个字符串和列表输入\n",
    "    input_texts = texts if isinstance(texts, list) else [texts]\n",
    "    \n",
    "    # 分批处理以避免 API 速率限制和负载大小限制\n",
    "    # OpenAI API 通常对请求大小和速率有限制\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # 处理每批文本\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        # 提取当前批次的文本\n",
    "        batch = input_texts[i:i + batch_size]\n",
    "        \n",
    "        # 调用 API 为当前批次生成嵌入\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # 从响应中提取嵌入向量\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    # 如果原始输入是单个字符串，只返回第一个嵌入\n",
    "    if isinstance(texts, str):\n",
    "        return all_embeddings[0]\n",
    "    \n",
    "    # 否则返回完整的嵌入列表\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档处理流水线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    将文档处理为向量存储。\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 文件路径\n",
    "        chunk_size (int): 每个块的字符大小\n",
    "        chunk_overlap (int): 块之间的重叠字符数\n",
    "        \n",
    "    Returns:\n",
    "        SimpleVectorStore: 包含文档块的向量存储\n",
    "    \"\"\"\n",
    "    # 从 PDF 文件中提取文本\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 将提取的文本分割为指定大小和重叠的块\n",
    "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # 为每个文本块创建嵌入\n",
    "    print(\"正在为块创建嵌入向量...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # 初始化新的向量存储\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # 将块及其嵌入添加到向量存储中\n",
    "    vector_store.add_items(chunks, chunk_embeddings)\n",
    "    \n",
    "    print(f\"创建了包含 {len(chunks)} 个块的向量存储\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关性评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_document_relevance(query, document):\n",
    "    \"\"\"\n",
    "    评估文档与查询的相关性。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        document (str): 文档文本\n",
    "        \n",
    "    Returns:\n",
    "        float: 相关性分数 (0-1)\n",
    "    \"\"\"\n",
    "    # 定义系统提示，指导模型如何评估相关性\n",
    "    system_prompt = \"\"\"\n",
    "    您是评估文档相关性的专家。\n",
    "    请评估给定文档与查询的相关性，评分范围为 0 到 1。\n",
    "    0 表示完全不相关，1 表示完全相关。\n",
    "    只提供 0 到 1 之间的浮点数分数。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 定义包含查询和文档的用户提示\n",
    "    user_prompt = f\"查询：{query}\\n\\n文档：{document}\"\n",
    "    \n",
    "    try:\n",
    "        # 向 OpenAI API 发出请求以评估相关性\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # 指定要使用的模型\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # 指导助手的系统消息\n",
    "                {\"role\": \"user\", \"content\": user_prompt}  # 包含查询和文档的用户消息\n",
    "            ],\n",
    "            temperature=0,  # 设置响应生成的温度\n",
    "            max_tokens=5  # 需要非常短的响应\n",
    "        )\n",
    "        \n",
    "        # 从响应中提取分数\n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "        # 使用正则表达式在响应中查找浮点值\n",
    "        score_match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "        if score_match:\n",
    "            return float(score_match.group(1))  # 将提取的分数作为浮点数返回\n",
    "        return 0.5  # 如果解析失败，默认为中间值\n",
    "    \n",
    "    except Exception as e:\n",
    "        # 出错时打印错误消息并返回默认值\n",
    "        print(f\"评估文档相关性时出错：{e}\")\n",
    "        return 0.5  # 出错时默认为中间值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络搜索函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duck_duck_go_search(query, num_results=3):\n",
    "    \"\"\"\n",
    "    使用 DuckDuckGo 执行网络搜索。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 搜索查询\n",
    "        num_results (int): 要返回的结果数量\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, List[Dict]]: 合并的搜索结果文本和源元数据\n",
    "    \"\"\"\n",
    "    # 为 URL 编码查询\n",
    "    encoded_query = quote_plus(query)\n",
    "    \n",
    "    # DuckDuckGo 搜索 API 端点（非官方）\n",
    "    url = f\"https://api.duckduckgo.com/?q={encoded_query}&format=json\"\n",
    "    \n",
    "    try:\n",
    "        # 执行网络搜索请求\n",
    "        response = requests.get(url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        })\n",
    "        data = response.json()\n",
    "        \n",
    "        # 初始化变量来存储结果文本和源\n",
    "        results_text = \"\"\n",
    "        sources = []\n",
    "        \n",
    "        # 如果可用，添加摘要\n",
    "        if data.get(\"AbstractText\"):\n",
    "            results_text += f\"{data['AbstractText']}\\n\\n\"\n",
    "            sources.append({\n",
    "                \"title\": data.get(\"AbstractSource\", \"Wikipedia\"),\n",
    "                \"url\": data.get(\"AbstractURL\", \"\")\n",
    "            })\n",
    "        \n",
    "        # 添加相关主题\n",
    "        for topic in data.get(\"RelatedTopics\", [])[:num_results]:\n",
    "            if \"Text\" in topic and \"FirstURL\" in topic:\n",
    "                results_text += f\"{topic['Text']}\\n\\n\"\n",
    "                sources.append({\n",
    "                    \"title\": topic.get(\"Text\", \"\").split(\" - \")[0],\n",
    "                    \"url\": topic.get(\"FirstURL\", \"\")\n",
    "                })\n",
    "        \n",
    "        return results_text, sources\n",
    "    \n",
    "    except Exception as e:\n",
    "        # 如果主搜索失败，打印错误消息\n",
    "        print(f\"执行网络搜索时出错：{e}\")\n",
    "        \n",
    "        # 回退到备用搜索 API\n",
    "        try:\n",
    "            backup_url = f\"https://serpapi.com/search.json?q={encoded_query}&engine=duckduckgo\"\n",
    "            response = requests.get(backup_url)\n",
    "            data = response.json()\n",
    "            \n",
    "            # 初始化变量来存储结果文本和源\n",
    "            results_text = \"\"\n",
    "            sources = []\n",
    "            \n",
    "            # 从备用 API 提取结果\n",
    "            for result in data.get(\"organic_results\", [])[:num_results]:\n",
    "                results_text += f\"{result.get('title', '')}：{result.get('snippet', '')}\\n\\n\"\n",
    "                sources.append({\n",
    "                    \"title\": result.get(\"title\", \"\"),\n",
    "                    \"url\": result.get(\"link\", \"\")\n",
    "                })\n",
    "            \n",
    "            return results_text, sources\n",
    "        except Exception as backup_error:\n",
    "            # 如果备用搜索也失败，打印错误消息\n",
    "            print(f\"备用搜索也失败：{backup_error}\")\n",
    "            return \"无法检索搜索结果。\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_search_query(query):\n",
    "    \"\"\"\n",
    "    重写查询以使其更适合网络搜索。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 原始查询\n",
    "        \n",
    "    Returns:\n",
    "        str: 重写的查询\n",
    "    \"\"\"\n",
    "    # 定义系统提示，指导模型如何重写查询\n",
    "    system_prompt = \"\"\"\n",
    "    您是创建有效搜索查询的专家。\n",
    "    重写给定的查询，使其更适合网络搜索引擎。\n",
    "    专注于关键词和事实，删除不必要的词语，使其简洁。\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 向 OpenAI API 发出请求以重写查询\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # 指定要使用的模型\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # 指导助手的系统消息\n",
    "                {\"role\": \"user\", \"content\": f\"原始查询：{query}\\n\\n重写的查询：\"}  # 包含原始查询的用户消息\n",
    "            ],\n",
    "            temperature=0.3,  # 设置响应生成的温度\n",
    "            max_tokens=50  # 限制响应长度\n",
    "        )\n",
    "        \n",
    "        # 从响应中返回重写的查询\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        # 出错时打印错误消息并返回原始查询\n",
    "        print(f\"重写搜索查询时出错：{e}\")\n",
    "        return query  # 出错时返回原始查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_web_search(query):\n",
    "    \"\"\"\n",
    "    执行带查询重写的网络搜索。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 原始用户查询\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, List[Dict]]: 搜索结果文本和源元数据\n",
    "    \"\"\"\n",
    "    # 重写查询以改善搜索结果\n",
    "    rewritten_query = rewrite_search_query(query)\n",
    "    print(f\"重写的搜索查询：{rewritten_query}\")\n",
    "    \n",
    "    # 使用重写的查询执行网络搜索\n",
    "    results_text, sources = duck_duck_go_search(rewritten_query)\n",
    "    \n",
    "    # 返回搜索结果文本和源元数据\n",
    "    return results_text, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 知识精炼函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_knowledge(text):\n",
    "    \"\"\"\n",
    "    从文本中提取和精炼关键信息。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 要精炼的输入文本\n",
    "        \n",
    "    Returns:\n",
    "        str: 从文本中精炼的关键要点\n",
    "    \"\"\"\n",
    "    # 定义系统提示，指导模型如何提取关键信息\n",
    "    system_prompt = \"\"\"\n",
    "    从以下文本中提取关键信息，形成一组清晰、简洁的要点。\n",
    "    专注于最相关的事实和重要细节。\n",
    "    将您的响应格式化为项目符号列表，每个要点在新行上以 \"• \" 开头。\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 向 OpenAI API 发出请求以精炼文本\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # 指定要使用的模型\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # 指导助手的系统消息\n",
    "                {\"role\": \"user\", \"content\": f\"要精炼的文本：\\n\\n{text}\"}  # 包含要精炼文本的用户消息\n",
    "            ],\n",
    "            temperature=0.3  # 设置响应生成的温度\n",
    "        )\n",
    "        \n",
    "        # 从响应中返回精炼的关键要点\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        # 出错时打印错误消息并返回原始文本\n",
    "        print(f\"精炼知识时出错：{e}\")\n",
    "        return text  # 出错时返回原始文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心 CRAG 流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crag_process(query, vector_store, k=3):\n",
    "    \"\"\"\n",
    "    运行纠错 RAG 流程。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        vector_store (SimpleVectorStore): 包含文档块的向量存储\n",
    "        k (int): 要检索的初始文档数量\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 包括响应和调试信息的流程结果\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 使用 CRAG 处理查询：{query} ===\\n\")\n",
    "    \n",
    "    # 步骤 1：创建查询嵌入并检索文档\n",
    "    print(\"正在检索初始文档...\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "    retrieved_docs = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # 步骤 2：评估文档相关性\n",
    "    print(\"正在评估文档相关性...\")\n",
    "    relevance_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        score = evaluate_document_relevance(query, doc[\"text\"])\n",
    "        relevance_scores.append(score)\n",
    "        doc[\"relevance\"] = score\n",
    "        print(f\"文档相关性评分：{score:.2f}\")\n",
    "    \n",
    "    # 步骤 3：根据最佳相关性分数确定行动\n",
    "    max_score = max(relevance_scores) if relevance_scores else 0\n",
    "    best_doc_idx = relevance_scores.index(max_score) if relevance_scores else -1\n",
    "    \n",
    "    # 跟踪源以进行归属\n",
    "    sources = []\n",
    "    final_knowledge = \"\"\n",
    "    \n",
    "    # 步骤 4：执行适当的知识获取策略\n",
    "    if max_score > 0.7:\n",
    "        # 情况 1：高相关性 - 直接使用文档\n",
    "        print(f\"高相关性（{max_score:.2f}）- 直接使用文档\")\n",
    "        best_doc = retrieved_docs[best_doc_idx][\"text\"]\n",
    "        final_knowledge = best_doc\n",
    "        sources.append({\n",
    "            \"title\": \"文档\",\n",
    "            \"url\": \"\"\n",
    "        })\n",
    "        \n",
    "    elif max_score < 0.3:\n",
    "        # 情况 2：低相关性 - 使用网络搜索\n",
    "        print(f\"低相关性（{max_score:.2f}）- 执行网络搜索\")\n",
    "        web_results, web_sources = perform_web_search(query)\n",
    "        final_knowledge = refine_knowledge(web_results)\n",
    "        sources.extend(web_sources)\n",
    "        \n",
    "    else:\n",
    "        # 情况 3：中等相关性 - 结合文档与网络搜索\n",
    "        print(f\"中等相关性（{max_score:.2f}）- 结合文档与网络搜索\")\n",
    "        best_doc = retrieved_docs[best_doc_idx][\"text\"]\n",
    "        refined_doc = refine_knowledge(best_doc)\n",
    "        \n",
    "        # 获取网络结果\n",
    "        web_results, web_sources = perform_web_search(query)\n",
    "        refined_web = refine_knowledge(web_results)\n",
    "        \n",
    "        # 结合知识\n",
    "        final_knowledge = f\"来自文档：\\n{refined_doc}\\n\\n来自网络搜索：\\n{refined_web}\"\n",
    "        \n",
    "        # 添加源\n",
    "        sources.append({\n",
    "            \"title\": \"文档\",\n",
    "            \"url\": \"\"\n",
    "        })\n",
    "        sources.extend(web_sources)\n",
    "    \n",
    "    # 步骤 5：生成最终响应\n",
    "    print(\"正在生成最终响应...\")\n",
    "    response = generate_response(query, final_knowledge, sources)\n",
    "    \n",
    "    # 返回综合结果\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"relevance_scores\": relevance_scores,\n",
    "        \"max_relevance\": max_score,\n",
    "        \"final_knowledge\": final_knowledge,\n",
    "        \"sources\": sources\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 响应生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, knowledge, sources):\n",
    "    \"\"\"\n",
    "    基于查询和知识生成响应。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        knowledge (str): 用于生成响应的知识\n",
    "        sources (List[Dict]): 包含标题和 URL 的源列表\n",
    "        \n",
    "    Returns:\n",
    "        str: 生成的响应\n",
    "    \"\"\"\n",
    "    # 格式化源以包含在提示中\n",
    "    sources_text = \"\"\n",
    "    for source in sources:\n",
    "        title = source.get(\"title\", \"未知来源\")\n",
    "        url = source.get(\"url\", \"\")\n",
    "        if url:\n",
    "            sources_text += f\"- {title}：{url}\\n\"\n",
    "        else:\n",
    "            sources_text += f\"- {title}\\n\"\n",
    "    \n",
    "    # 定义系统提示，指导模型如何生成响应\n",
    "    system_prompt = \"\"\"\n",
    "    您是一个有用的 AI 助手。基于提供的知识对查询生成全面、信息丰富的响应。\n",
    "    包含所有相关信息，同时保持答案清晰简洁。\n",
    "    如果知识不能完全回答查询，请承认这一限制。\n",
    "    在响应末尾包含源归属。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 定义包含查询、知识和源的用户提示\n",
    "    user_prompt = f\"\"\"\n",
    "    查询：{query}\n",
    "    \n",
    "    知识：\n",
    "    {knowledge}\n",
    "    \n",
    "    来源：\n",
    "    {sources_text}\n",
    "    \n",
    "    请基于这些信息对查询提供信息丰富的响应。\n",
    "    在响应末尾包含来源。\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 向 OpenAI API 发出请求以生成响应\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",  # 使用 GPT-4 获得高质量响应\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        # 返回生成的响应\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        # 打印错误消息并返回错误响应\n",
    "        print(f\"生成响应时出错：{e}\")\n",
    "        return f\"抱歉，我在生成对您查询的响应时遇到了错误：'{query}'。错误是：{str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_crag_response(query, response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    评估 CRAG 响应的质量。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        response (str): 生成的响应\n",
    "        reference_answer (str, optional): 用于比较的参考答案\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 评估指标\n",
    "    \"\"\"\n",
    "    # 评估标准的系统提示\n",
    "    system_prompt = \"\"\"\n",
    "    您是评估问题响应质量的专家。\n",
    "    请根据以下标准评估提供的响应：\n",
    "    \n",
    "    1. 相关性（0-10）：响应多大程度上直接解决了查询？\n",
    "    2. 准确性（0-10）：信息的事实正确性如何？\n",
    "    3. 完整性（0-10）：响应多全面地回答了查询的所有方面？\n",
    "    4. 清晰度（0-10）：响应多清晰易懂？\n",
    "    5. 源质量（0-10）：响应引用相关源的效果如何？\n",
    "    \n",
    "    将您的评估作为 JSON 对象返回，包含每个标准的分数和每个分数的简要解释。\n",
    "    还包括一个 \"overall_score\"（0-10）和您评估的简要 \"summary\"。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 包含要评估的查询和响应的用户提示\n",
    "    user_prompt = f\"\"\"\n",
    "    查询：{query}\n",
    "    \n",
    "    要评估的响应：\n",
    "    {response}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 如果提供了参考答案，将其包含在提示中\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "    参考答案（用于比较）：\n",
    "    {reference_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 从 GPT-4 模型请求评估\n",
    "        evaluation_response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # 解析评估响应\n",
    "        evaluation = json.loads(evaluation_response.choices[0].message.content)\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        # 处理评估过程中的任何错误\n",
    "        print(f\"评估响应时出错：{e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"overall_score\": 0,\n",
    "            \"summary\": \"由于错误，评估失败。\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_crag_vs_standard_rag(query, vector_store, reference_answer=None):\n",
    "    \"\"\"\n",
    "    比较 CRAG 与标准 RAG 对查询的处理。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        vector_store (SimpleVectorStore): 包含文档块的向量存储\n",
    "        reference_answer (str, optional): 用于比较的参考答案\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 比较结果\n",
    "    \"\"\"\n",
    "    # 运行 CRAG 流程\n",
    "    print(\"\\n=== 运行 CRAG ===\")\n",
    "    crag_result = crag_process(query, vector_store)\n",
    "    crag_response = crag_result[\"response\"]\n",
    "    \n",
    "    # 运行标准 RAG（直接检索和响应）\n",
    "    print(\"\\n=== 运行标准 RAG ===\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "    retrieved_docs = vector_store.similarity_search(query_embedding, k=3)\n",
    "    combined_text = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    standard_sources = [{\"title\": \"文档\", \"url\": \"\"}]\n",
    "    standard_response = generate_response(query, combined_text, standard_sources)\n",
    "    \n",
    "    # 评估两种方法\n",
    "    print(\"\\n=== 评估 CRAG 响应 ===\")\n",
    "    crag_eval = evaluate_crag_response(query, crag_response, reference_answer)\n",
    "    \n",
    "    print(\"\\n=== 评估标准 RAG 响应 ===\")\n",
    "    standard_eval = evaluate_crag_response(query, standard_response, reference_answer)\n",
    "    \n",
    "    # 比较方法\n",
    "    print(\"\\n=== 比较方法 ===\")\n",
    "    comparison = compare_responses(query, crag_response, standard_response, reference_answer)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"crag_response\": crag_response,\n",
    "        \"standard_response\": standard_response,\n",
    "        \"reference_answer\": reference_answer,\n",
    "        \"crag_evaluation\": crag_eval,\n",
    "        \"standard_evaluation\": standard_eval,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, crag_response, standard_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    比较 CRAG 和标准 RAG 响应。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户查询\n",
    "        crag_response (str): CRAG 响应\n",
    "        standard_response (str): 标准 RAG 响应\n",
    "        reference_answer (str, optional): 参考答案\n",
    "        \n",
    "    Returns:\n",
    "        str: 比较分析\n",
    "    \"\"\"\n",
    "    # 比较两种方法的系统提示\n",
    "    system_prompt = \"\"\"\n",
    "    您是比较两种响应生成方法的专家评估员：\n",
    "    \n",
    "    1. CRAG（纠错 RAG）：一个评估文档相关性并在需要时动态切换到网络搜索的系统。\n",
    "    2. 标准 RAG：一个基于嵌入相似性直接检索文档并用于响应生成的系统。\n",
    "    \n",
    "    基于以下方面比较这两个系统的响应：\n",
    "    - 准确性和事实正确性\n",
    "    - 与查询的相关性\n",
    "    - 答案的完整性\n",
    "    - 清晰度和组织\n",
    "    - 源归属质量\n",
    "    \n",
    "    解释哪种方法在这个特定查询上表现更好以及原因。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 包含要比较的查询和响应的用户提示\n",
    "    user_prompt = f\"\"\"\n",
    "    查询：{query}\n",
    "    \n",
    "    CRAG 响应：\n",
    "    {crag_response}\n",
    "    \n",
    "    标准 RAG 响应：\n",
    "    {standard_response}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 如果提供了参考答案，将其包含在提示中\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "    参考答案：\n",
    "    {reference_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 从 GPT-4 模型请求比较\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"比较响应时出错：{e}\")\n",
    "        return f\"比较失败：{str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例使用和评估\n",
    "\n",
    "以下代码演示了如何使用 CRAG 系统并与标准 RAG 进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # PDF 文档路径\n",
    "    pdf_path = \"data/AI_Information.pdf\"\n",
    "    \n",
    "    # 测试查询\n",
    "    query = \"人工智能在医疗保健中的最新应用有哪些？\"\n",
    "    \n",
    "    # 处理文档并创建向量存储\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # 运行 CRAG 流程\n",
    "    crag_result = crag_process(query, vector_store)\n",
    "    \n",
    "    print(f\"查询：{crag_result['query']}\")\n",
    "    print(f\"最大相关性分数：{crag_result['max_relevance']:.2f}\")\n",
    "    print(f\"检索的文档数量：{len(crag_result['retrieved_docs'])}\")\n",
    "    print(f\"源数量：{len(crag_result['sources'])}\")\n",
    "    print(f\"CRAG 响应：{crag_result['response']}\")\n",
    "    \n",
    "    # 比较 CRAG 与标准 RAG\n",
    "    comparison_result = compare_crag_vs_standard_rag(query, vector_store)\n",
    "    print(f\"\\n比较分析：{comparison_result['comparison']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "纠错 RAG (CRAG) 通过以下方式改进了传统的 RAG 系统：\n",
    "\n",
    "1. **动态相关性评估**：在使用检索到的文档之前评估其相关性\n",
    "2. **自适应知识源选择**：根据相关性分数动态选择最佳知识源\n",
    "3. **网络搜索集成**：当本地知识不足时，无缝集成网络搜索作为后备\n",
    "4. **知识精炼**：提取和精炼关键信息以提高响应质量\n",
    "5. **多源知识融合**：在适当时结合来自多个源的信息\n",
    "\n",
    "这种方法特别适用于：\n",
    "- 需要最新信息的查询\n",
    "- 本地文档可能不完整的情况\n",
    "- 需要高准确性和相关性的应用\n",
    "- 需要透明源归属的系统\n",
    "\n",
    "CRAG 代表了 RAG 系统设计中的重要进步，为构建更智能、更可靠的问答系统提供了框架，能够动态适应不同的查询需求和知识可用性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}