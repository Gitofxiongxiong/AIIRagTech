{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Retrieval \n",
    "一个自适应检索系统，该系统根据查询类型动态选择最合适的检索策略。这种方法大大增强了RAG系统在各种问题上提供准确和相关回答的能力。\n",
    "不同的问题需要不同的检索策略。这个系统：\n",
    "- 分类查询类型（事实、分析、意见或上下文）\n",
    "- 选择适当的检索策略\n",
    "- 执行专门的检索技术\n",
    "- 生成定制响应\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相关的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从pdf提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    提取PDF文件中的文本并打印前`num_chars`个字符。\n",
    "\n",
    "    参数：\n",
    "    pdf_path (str): PDF文件的路径。\n",
    "\n",
    "    返回：\n",
    "    str: 从PDF中提取的文本。\n",
    "\n",
    "    \"\"\"\n",
    "    # 打开PDF文件\n",
    "    mypdf = pymupdf.open(pdf_path)\n",
    "    all_text = \"\"  # 初始化一个空字符串来存储提取的文本\n",
    "\n",
    "    # 迭代PDF中的每个页面\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 获取页面\n",
    "        text = page.get_text(\"text\")  # 从页面中提取文本\n",
    "        all_text += text  # 将提取的文本附加到all_text字符串\n",
    "\n",
    "    return all_text  # 返回提取的文本\n",
    "\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    将文本分割为多个块，每个块的大小为n，重叠部分为overlap。\n",
    "    参数：\n",
    "    text: 输入的文本\n",
    "    n: 每个块的大小\n",
    "    overlap: 相邻块之间的重叠部分大小\n",
    "\n",
    "    返回：\n",
    "    文本块列表\n",
    "    \"\"\"\n",
    "    chunks = []  \n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        \n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如果你没有配置环境变量，请在此处用你的API Key进行替换\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 百炼服务的base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简易向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    简易的向量存储库。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        添加一个新的项到存储库。\n",
    "\n",
    "        参数:\n",
    "        text (str): 文本内容。\n",
    "        embedding (List[float]): 文本的嵌入向量。\n",
    "        metadata (Dict, optional): 与文本相关的元数据。\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入向量最相似的文本。\n",
    "\n",
    "        参数:\n",
    "        query_embedding (List[float]): 查询的嵌入向量。\n",
    "        k (int, optional): 返回最相似的k个结果。\n",
    "\n",
    "        返回:\n",
    "        List[Dict]: 最相似的文本及其相关信息。\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_in_batches(text_chunks, model=\"text-embedding-v3\", batch_size_limit=10): # 我改成了官方模型名，你可以换回 \"text-embedding-v3\"\n",
    "    \"\"\"\n",
    "    调用 OpenAI 的 Embedding API 来创建文本列表的嵌入向量，处理批处理大小限制。\n",
    "\n",
    "    参数:\n",
    "    text_chunks (List[str]): 需要创建嵌入的文本字符串列表。\n",
    "    model (str): 使用的嵌入模型。\n",
    "    batch_size_limit (int): API 允许的最大批处理大小。根据错误信息，这里是10。\n",
    "\n",
    "    返回:\n",
    "    List[List[float]]: 所有文本的嵌入向量列表。\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    if not text_chunks:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(text_chunks, list): # 确保输入是列表\n",
    "        text_chunks = [text_chunks]\n",
    "\n",
    "    for i in range(0, len(text_chunks), batch_size_limit):\n",
    "        batch = text_chunks[i:i + batch_size_limit]\n",
    "        try:\n",
    "            #print(f\"Processing batch {i//batch_size_limit + 1}, size: {len(batch)}\")\n",
    "            response = client.embeddings.create(\n",
    "                input=batch,\n",
    "                model=model,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "            # 从响应中提取该批次的嵌入向量\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting with chunk: '{batch[0][:50]}...'\")\n",
    "            print(f\"API Error: {e}\")\n",
    "\n",
    "            raise e \n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "def create_embeddings(text, model=\"text-embedding-v3\"):\n",
    "    \"\"\"\n",
    "    字符串向量化\n",
    "    参数:\n",
    "    text (str): 需要创建嵌入的文本字符串。\n",
    "    model (str): 使用的嵌入模型。\n",
    "\n",
    "    返回:\n",
    "    List[float]: 文本的嵌入向量。\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    处理带有反馈循环的RAG（检索增强生成）文档。\n",
    "    此函数处理完整的文档处理管道：\n",
    "    1、从PDF中提取文本\n",
    "    2、重叠文本分块\n",
    "    3、嵌入区块创建\n",
    "    4、矢量数据库元数据存储\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings_in_batches(chunks)\n",
    "    \n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"index\": i,                \n",
    "                \"source\": pdf_path,     \n",
    "                \"relevance_score\": 1.0,   \n",
    "                \"feedback_count\": 0        \n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请求分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(query, model=\"qwen-turbo\"):\n",
    "    \"\"\"\n",
    "    将query分类为Factual、Analytical、Opinion或Contextual。\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"你是一位问题分类专家。\n",
    "    请将给定的问题精确地归入以下类别之一：\n",
    "    - 事实型 (Factual)：寻求具体、可验证信息的问题。\n",
    "    - 分析型 (Analytical)：需要全面分析或解释的问题。\n",
    "    - 观点型 (Opinion)：关于主观事务或寻求不同观点的问题。\n",
    "    - 情景型 (Contextual)：依赖于用户特定情景的问题。\n",
    "\n",
    "    请只返回类别名称，不要包含任何解释或其他文字。\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    user_prompt = f\"Classify this query: {query}\"\n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "\n",
    "    category = response.choices[0].message.content.strip()\n",
    "    \n",
    "\n",
    "    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
    "    \n",
    "\n",
    "    for valid in valid_categories:\n",
    "        if valid in category:\n",
    "            return valid\n",
    "    \n",
    "\n",
    "    return \"Factual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专门的检索策略\n",
    "\n",
    "Factual策略 - 注重精准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "\n",
    "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    system_prompt = \"\"\"你是优化搜索查询的专家。\n",
    "        你的任务是重新构造给定的事实性查询，使其在信息检索时更精确、更具体。\n",
    "        请重点关注关键实体及其之间的关系。\n",
    "        仅提供优化后的查询，不作任何解释。\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"优化下面的请求: {query}\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    enhanced_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Enhanced query: {enhanced_query}\")\n",
    "    \n",
    "    query_embedding = create_embeddings(enhanced_query)\n",
    "\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "\n",
    "    ranked_results = []\n",
    "    \n",
    "    for doc in initial_results:\n",
    "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    \n",
    "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytical - 全面覆盖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \n",
    "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    你是分解复杂问题的专家。\n",
    "    生成子问题，以探究主要分析性查询的不同方面。\n",
    "    这些子问题应涵盖主题的广度，并有助于检索全面的信息。\n",
    "    返回一个包含恰好 3 个子问题的列表，每行一个。\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"为下面分析型query生成子查询: {query}\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    sub_queries = response.choices[0].message.content.strip().split('\\n')\n",
    "    sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
    "    print(f\"Generated sub-queries: {sub_queries}\")\n",
    "    \n",
    "    all_results = []\n",
    "    for sub_query in sub_queries:\n",
    "\n",
    "        sub_query_embedding = create_embeddings(sub_query)\n",
    "\n",
    "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    unique_texts = set()\n",
    "    diverse_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result[\"text\"] not in unique_texts:\n",
    "            unique_texts.add(result[\"text\"])\n",
    "            diverse_results.append(result)\n",
    "\n",
    "    if len(diverse_results) < k:\n",
    "\n",
    "        main_query_embedding = create_embeddings(query)\n",
    "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
    "        \n",
    "        for result in main_results:\n",
    "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
    "                unique_texts.add(result[\"text\"])\n",
    "                diverse_results.append(result)\n",
    "\n",
    "    return diverse_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opinion - 多元化视角"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "\n",
    "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    system_prompt = \"\"\"你是识别某个主题不同观点的专家。\n",
    "        对于给定的关于意见或观点的查询，请识别人们可能对该主题持有的不同观点。\n",
    "        返回一个包含恰好 3 个不同观点角度的列表，每行一个。\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"提出关于下面查询的不同角度观点: {query}\"\n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "\n",
    "    viewpoints = response.choices[0].message.content.strip().split('\\n')\n",
    "    viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
    "    print(f\"Identified viewpoints: {viewpoints}\")\n",
    "    \n",
    "    all_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "\n",
    "        combined_query = f\"{query} {viewpoint}\"\n",
    "\n",
    "        viewpoint_embedding = create_embeddings(combined_query)\n",
    "\n",
    "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
    "        \n",
    "        for result in results:\n",
    "            result[\"viewpoint\"] = viewpoint\n",
    "        \n",
    "        all_results.extend(results)\n",
    "    \n",
    "    selected_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "\n",
    "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
    "        if viewpoint_docs:\n",
    "            selected_results.append(viewpoint_docs[0])\n",
    "    \n",
    "    remaining_slots = k - len(selected_results)\n",
    "    if remaining_slots > 0:\n",
    "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
    "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        selected_results.extend(remaining_docs[:remaining_slots])\n",
    "\n",
    "    return selected_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual - 用户上下文集成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
    "    \n",
    "    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n",
    "\n",
    "    if not user_context:\n",
    "        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\n",
    "For the given query, infer what contextual information might be relevant or implied \n",
    "but not explicitly stated. Focus on what background would help answering this query.\n",
    "\n",
    "Return a brief description of the implied context.\"\"\"\n",
    "\n",
    "        user_prompt = f\"Infer the implied context in this query: {query}\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "\n",
    "        user_context = response.choices[0].message.content.strip()\n",
    "        print(f\"Inferred context: {user_context}\")\n",
    "    \n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    你是一位理解问题中隐含背景信息的专家。\n",
    "    对于给定的查询，请推断哪些背景信息可能是相关的或隐含的，但没有明确说明。\n",
    "    重点关注哪些背景信息有助于回答此查询。\n",
    "    返回隐含背景信息的简要描述。\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {user_context}\n",
    "\n",
    "    重新格式化查询以合并此上下文:\"\"\"\n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    contextualized_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Contextualized query: {contextualized_query}\")\n",
    "\n",
    "    query_embedding = create_embeddings(contextualized_query)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "\n",
    "    ranked_results = []\n",
    "    \n",
    "    for doc in initial_results:\n",
    " \n",
    "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"context_relevance\": context_relevance\n",
    "        })\n",
    "    \n",
    "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document_context_relevance(query, context, document, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    给文本打分，打分越高，文本越相关\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the model on how to rate relevance considering context\n",
    "    system_prompt = \"\"\"Y\n",
    "    你是一位在考虑上下文的情况下评估文档相关性的专家。\n",
    "    根据文档在多大程度上解决了查询（当考虑到所提供的上下文时），对其进行评分，评分范围为 0 到 10，其中：\n",
    "    0 = 完全不相关\n",
    "    10 = 在给定上下文中完美地解决了查询\n",
    "    仅返回一个 0 到 10 之间的数字分数，不要返回其他任何内容。\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {context}\n",
    "\n",
    "    Document: {doc_preview}\n",
    "\n",
    "    Relevance score considering context (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "  \n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    " \n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  \n",
    "    else:\n",
    "\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    选择不同策略执行daptive retrieval\n",
    "    \"\"\"\n",
    "\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "\n",
    "    if query_type == \"Factual\":\n",
    "\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Analytical\":\n",
    "\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Opinion\":\n",
    "\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Contextual\":\n",
    "\n",
    "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
    "    else:\n",
    "\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    \n",
    "    return results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "响应生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results, query_type, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    基于请求生成响应\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
    "    \n",
    "    if query_type == \"Factual\":\n",
    "        system_prompt = \"\"\"你是一位提供事实信息的乐于助人的助手。\n",
    "        根据提供的上下文回答问题。注重准确性和精确性。\n",
    "        如果上下文中不包含所需信息，请承认其局限性。\"\"\"\n",
    "        \n",
    "    elif query_type == \"Analytical\":\n",
    "        system_prompt = \"\"\"你是一位提供分析见解的乐于助人的助手。\n",
    "        根据提供的上下文，对该主题进行全面分析。\n",
    "        在你的解释中涵盖不同的方面和观点。\n",
    "        如果上下文存在不足，请在提供尽可能最佳分析的同时承认这些不足。\"\"\"\n",
    "        \n",
    "    elif query_type == \"Opinion\":\n",
    "        system_prompt = \"\"\"你是一位乐于助人的助手，能够讨论包含多种观点的主题。\n",
    "        根据提供的上下文，针对该主题提出不同的观点。\n",
    "        确保公正地呈现各种意见，不带偏见。\n",
    "        承认上下文中观点有限的地方。\"\"\"\n",
    "        \n",
    "    elif query_type == \"Contextual\":\n",
    "        system_prompt = \"\"\"你是一位乐于助人的助手，提供与上下文相关的信息。\n",
    "        同时考虑查询及其上下文来回答问题。\n",
    "        将查询上下文与所提供文档中的信息联系起来。\n",
    "        如果上下文未能完全解决具体情况，请承认其局限性。\"\"\"\n",
    "        \n",
    "    else:\n",
    "        system_prompt = \"\"\"你是一位乐于助人的助手。请根据提供的上下文回答问题。如果你无法从上下文中找到答案，请承认其局限性。\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    请根据上下文提供有用的答复。\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现Adaptive Retrieval 的 RAG流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
    "    \n",
    "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "\n",
    "    response = generate_response(query, retrieved_docs, query_type)\n",
    "\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare adaptive retrieval with standard retrieval on a set of test queries.\n",
    "    \n",
    "    This function processes a document, runs both standard and adaptive retrieval methods\n",
    "    on each test query, and compares their performance. If reference answers are provided,\n",
    "    it also evaluates the quality of responses against these references.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document to be processed as the knowledge source\n",
    "        test_queries (List[str]): List of test queries to evaluate both retrieval methods\n",
    "        reference_answers (List[str], optional): Reference answers for evaluation metrics\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results containing individual query results and overall comparison\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\")\n",
    "    \n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # --- Standard retrieval approach ---\n",
    "        print(\"\\n--- Standard Retrieval ---\")\n",
    "        # Create embedding for the query\n",
    "        query_embedding = create_embeddings(query)\n",
    "        # Retrieve documents using simple vector similarity\n",
    "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
    "        # Generate response using a generic approach\n",
    "        standard_response = generate_response(query, standard_docs, \"General\")\n",
    "        \n",
    "        # --- Adaptive retrieval approach ---\n",
    "        print(\"\\n--- Adaptive Retrieval ---\")\n",
    "        # Classify the query to determine its type (Factual, Analytical, Opinion, Contextual)\n",
    "        query_type = classify_query(query)\n",
    "        # Retrieve documents using the strategy appropriate for this query type\n",
    "        adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n",
    "        # Generate a response tailored to the query type\n",
    "        adaptive_response = generate_response(query, adaptive_docs, query_type)\n",
    "        \n",
    "        # Store complete results for this query\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": query_type,\n",
    "            \"standard_retrieval\": {\n",
    "                \"documents\": standard_docs,\n",
    "                \"response\": standard_response\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"documents\": adaptive_docs,\n",
    "                \"response\": adaptive_response\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add reference answer if available for this query\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            result[\"reference_answer\"] = reference_answers[i]\n",
    "            \n",
    "        results.append(result)\n",
    "        \n",
    "        # Display preview of both responses for quick comparison\n",
    "        print(\"\\n--- Responses ---\")\n",
    "        print(f\"Standard: {standard_response[:200]}...\")\n",
    "        print(f\"Adaptive: {adaptive_response[:200]}...\")\n",
    "    \n",
    "    # Calculate comparative metrics if reference answers are available\n",
    "    if reference_answers:\n",
    "        comparison = compare_responses(results)\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(comparison)\n",
    "    \n",
    "    # Return the complete evaluation results\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(results):\n",
    "    \"\"\"\n",
    "    Compare standard and adaptive responses against reference answers.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results containing both types of responses\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI in comparing responses\n",
    "    comparison_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n",
    "    Compare the standard retrieval and adaptive retrieval responses for each query.\n",
    "    Consider factors like accuracy, relevance, comprehensiveness, and alignment with the reference answer.\n",
    "    Provide a detailed analysis of the strengths and weaknesses of each approach.\"\"\"\n",
    "    \n",
    "    # Initialize the comparison text with a header\n",
    "    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n",
    "    \n",
    "    # Iterate through each result to compare responses\n",
    "    for i, result in enumerate(results):\n",
    "        # Skip if there is no reference answer for the query\n",
    "        if \"reference_answer\" not in result:\n",
    "            continue\n",
    "            \n",
    "        # Add query details to the comparison text\n",
    "        comparison_text += f\"## Query {i+1}: {result['query']}\\n\"\n",
    "        comparison_text += f\"*Query Type: {result['query_type']}*\\n\\n\"\n",
    "        comparison_text += f\"**Reference Answer:**\\n{result['reference_answer']}\\n\\n\"\n",
    "        \n",
    "        # Add standard retrieval response to the comparison text\n",
    "        comparison_text += f\"**Standard Retrieval Response:**\\n{result['standard_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # Add adaptive retrieval response to the comparison text\n",
    "        comparison_text += f\"**Adaptive Retrieval Response:**\\n{result['adaptive_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # Create the user prompt for the AI to compare the responses\n",
    "        user_prompt = f\"\"\"\n",
    "        Reference Answer: {result['reference_answer']}\n",
    "        \n",
    "        Standard Retrieval Response: {result['standard_retrieval']['response']}\n",
    "        \n",
    "        Adaptive Retrieval Response: {result['adaptive_retrieval']['response']}\n",
    "        \n",
    "        Provide a detailed comparison of the two responses.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the comparison analysis using the OpenAI client\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": comparison_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        # Add the AI's comparison analysis to the comparison text\n",
    "        comparison_text += f\"**Comparison Analysis:**\\n{response.choices[0].message.content}\\n\\n\"\n",
    "    \n",
    "    return comparison_text  # Return the complete comparison analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "test_queries = [\n",
    "    \"What is Explainable AI (XAI)?\",                                              # Factual query - seeking definition/specific information\n",
    "    # \"How do AI ethics and governance frameworks address potential societal impacts?\",  # Analytical query - requiring comprehensive analysis\n",
    "    # \"Is AI development moving too fast for proper regulation?\",                   # Opinion query - seeking diverse perspectives\n",
    "    # \"How might explainable AI help in healthcare decisions?\",                     # Contextual query - benefits from context-awareness\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "    \"Explainable AI (XAI) aims to make AI systems transparent and understandable by providing clear explanations of how decisions are made. This helps users trust and effectively manage AI technologies.\",\n",
    "    # \"AI ethics and governance frameworks address potential societal impacts by establishing guidelines and principles to ensure AI systems are developed and used responsibly. These frameworks focus on fairness, accountability, transparency, and the protection of human rights to mitigate risks and promote beneficial output.5.\",\n",
    "    # \"Opinions on whether AI development is moving too fast for proper regulation vary. Some argue that rapid advancements outpace regulatory efforts, leading to potential risks and ethical concerns. Others believe that innovation should continue at its current pace, with regulations evolving alongside to address emerging challenges.\",\n",
    "    # \"Explainable AI can significantly aid healthcare decisions by providing transparent and understandable insights into AI-driven recommendations. This transparency helps healthcare professionals trust AI systems, make informed decisions, and improve patient output by understanding the rationale behind AI suggestions.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_adaptive_vs_standard(\n",
    "    pdf_path=pdf_path,                  # Source document for knowledge extraction\n",
    "    test_queries=test_queries,          # List of test queries to evaluate\n",
    "    reference_answers=reference_answers  # Optional ground truth for comparison\n",
    ")\n",
    "\n",
    "# The results will show a detailed comparison between standard retrieval and \n",
    "# adaptive retrieval performance across different query types, highlighting\n",
    "# where adaptive strategies provide improved outcomes\n",
    "print(evaluation_results[\"comparison\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
