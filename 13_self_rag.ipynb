{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-RAG：一种动态的 RAG 方法\n",
    "\n",
    "在本笔记中，我实现了 Self-RAG，这是一个先进的 RAG 系统，它可以动态决定何时以及如何使用检索到的信息。与传统的 RAG 方法不同，Self-RAG 在整个检索和生成过程中引入了反思点，从而产生更高质量和更可靠的响应。\n",
    "\n",
    "## Self-RAG 的关键组成部分\n",
    "\n",
    "1.  **检索决策**：确定对于给定的查询是否需要进行检索。\n",
    "2.  **文档检索**：在需要时获取可能相关的文档。\n",
    "3.  **相关性评估**：评估每个检索到的文档的相关程度。\n",
    "4.  **响应生成**：基于相关上下文创建响应。\n",
    "5.  **支持度评估**：评估响应是否恰当地基于上下文。\n",
    "6.  **实用性评估**：对生成的响应的整体有用性进行评分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    提取PDF文件中的文本并打印前`num_chars`个字符。\n",
    "\n",
    "    参数：\n",
    "    pdf_path (str): PDF文件的路径。\n",
    "\n",
    "    返回：\n",
    "    str: 从PDF中提取的文本。\n",
    "\n",
    "    \"\"\"\n",
    "    # 打开PDF文件\n",
    "    mypdf = pymupdf.open(pdf_path)\n",
    "    all_text = \"\"  # 初始化一个空字符串来存储提取的文本\n",
    "\n",
    "    # 迭代PDF中的每个页面\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 获取页面\n",
    "        text = page.get_text(\"text\")  # 从页面中提取文本\n",
    "        all_text += text  # 将提取的文本附加到all_text字符串\n",
    "\n",
    "    return all_text  # 返回提取的文本\n",
    "\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    将文本分割为多个块，每个块的大小为n，重叠部分为overlap。\n",
    "    参数：\n",
    "    text: 输入的文本\n",
    "    n: 每个块的大小\n",
    "    overlap: 相邻块之间的重叠部分大小\n",
    "\n",
    "    返回：\n",
    "    文本块列表\n",
    "    \"\"\"\n",
    "    chunks = []  \n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        \n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如果你没有配置环境变量，请在此处用你的API Key进行替换\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 百炼服务的base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简易向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    简易的向量存储库。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        添加一个新的项到存储库。\n",
    "\n",
    "        参数:\n",
    "        text (str): 文本内容。\n",
    "        embedding (List[float]): 文本的嵌入向量。\n",
    "        metadata (Dict, optional): 与文本相关的元数据。\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入向量最相似的文本。\n",
    "\n",
    "        参数:\n",
    "        query_embedding (List[float]): 查询的嵌入向量。\n",
    "        k (int, optional): 返回最相似的k个结果。\n",
    "\n",
    "        返回:\n",
    "        List[Dict]: 最相似的文本及其相关信息。\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_in_batches(text_chunks, model=\"text-embedding-v3\", batch_size_limit=10): # 我改成了官方模型名，你可以换回 \"text-embedding-v3\"\n",
    "    \"\"\"\n",
    "    调用 OpenAI 的 Embedding API 来创建文本列表的嵌入向量，处理批处理大小限制。\n",
    "\n",
    "    参数:\n",
    "    text_chunks (List[str]): 需要创建嵌入的文本字符串列表。\n",
    "    model (str): 使用的嵌入模型。\n",
    "    batch_size_limit (int): API 允许的最大批处理大小。根据错误信息，这里是10。\n",
    "\n",
    "    返回:\n",
    "    List[List[float]]: 所有文本的嵌入向量列表。\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    if not text_chunks:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(text_chunks, list): # 确保输入是列表\n",
    "        text_chunks = [text_chunks]\n",
    "\n",
    "    for i in range(0, len(text_chunks), batch_size_limit):\n",
    "        batch = text_chunks[i:i + batch_size_limit]\n",
    "        try:\n",
    "            #print(f\"Processing batch {i//batch_size_limit + 1}, size: {len(batch)}\")\n",
    "            response = client.embeddings.create(\n",
    "                input=batch,\n",
    "                model=model,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "            # 从响应中提取该批次的嵌入向量\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting with chunk: '{batch[0][:50]}...'\")\n",
    "            print(f\"API Error: {e}\")\n",
    "\n",
    "            raise e \n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "def create_embeddings(text, model=\"text-embedding-v3\"):\n",
    "    \"\"\"\n",
    "    字符串向量化\n",
    "    参数:\n",
    "    text (str): 需要创建嵌入的文本字符串。\n",
    "    model (str): 使用的嵌入模型。\n",
    "\n",
    "    返回:\n",
    "    List[float]: 文本的嵌入向量。\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    处理带有反馈循环的RAG（检索增强生成）文档。\n",
    "    此函数处理完整的文档处理管道：\n",
    "    1、从PDF中提取文本\n",
    "    2、重叠文本分块\n",
    "    3、嵌入区块创建\n",
    "    4、矢量数据库元数据存储\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings_in_batches(chunks)\n",
    "    \n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"index\": i,                \n",
    "                \"source\": pdf_path,     \n",
    "                \"relevance_score\": 1.0,   \n",
    "                \"feedback_count\": 0        \n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-Rag 组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_if_retrieval_needed(query):\n",
    "\n",
    "    system_prompt = \"\"\"您是一个AI助手，负责确定回答查询是否需要检索。\n",
    "        对于事实问题、特定信息请求或有关事件、人员或概念的问题，请回答“是”。\n",
    "        对于观点、假设情景或常识性的简单查询，回答“否”。\n",
    "        只回答“yes”或“no”。\"\"\"\n",
    "\n",
    "\n",
    "    user_prompt = f\"Query: {query}\\n\\nIs retrieval necessary to answer this query accurately?\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    return \"yes\" in answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(query, context):\n",
    "    \"\"\"\n",
    "    Evaluates the relevance of a context to the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context text\n",
    "        \n",
    "    Returns:\n",
    "        str: 'relevant' or 'irrelevant'\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to determine document relevance\n",
    "    system_prompt = \"\"\"You are an AI assistant that determines if a document is relevant to a query.\n",
    "    Consider whether the document contains information that would be helpful in answering the query.\n",
    "    Answer with ONLY \"Relevant\" or \"Irrelevant\".\"\"\"\n",
    "\n",
    "    # Truncate context if it is too long to avoid exceeding token limits\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [truncated]\"\n",
    "\n",
    "    # User prompt containing the query and the document content\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "    Document content:\n",
    "    {context}\n",
    "\n",
    "    Is this document relevant to the query? Answer with ONLY \"Relevant\" or \"Irrelevant\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the answer from the model's response and convert to lowercase\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    return answer  # Return the relevance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_support(response, context):\n",
    "    \"\"\"\n",
    "    Assesses how well a response is supported by the context.\n",
    "    \n",
    "    Args:\n",
    "        response (str): Generated response\n",
    "        context (str): Context text\n",
    "        \n",
    "    Returns:\n",
    "        str: 'fully supported', 'partially supported', or 'no support'\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to evaluate support\n",
    "    system_prompt = \"\"\"You are an AI assistant that determines if a response is supported by the given context.\n",
    "    Evaluate if the facts, claims, and information in the response are backed by the context.\n",
    "    Answer with ONLY one of these three options:\n",
    "    - \"Fully supported\": All information in the response is directly supported by the context.\n",
    "    - \"Partially supported\": Some information in the response is supported by the context, but some is not.\n",
    "    - \"No support\": The response contains significant information not found in or contradicting the context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate context if it is too long to avoid exceeding token limits\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [truncated]\"\n",
    "\n",
    "    # User prompt containing the context and the response to be evaluated\n",
    "    user_prompt = f\"\"\"Context:\n",
    "    {context}\n",
    "\n",
    "    Response:\n",
    "    {response}\n",
    "\n",
    "    How well is this response supported by the context? Answer with ONLY \"Fully supported\", \"Partially supported\", or \"No support\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the answer from the model's response and convert to lowercase\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    return answer  # Return the support assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_utility(query, response):\n",
    "    \"\"\"\n",
    "    Rates the utility of a response for the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        response (str): Generated response\n",
    "        \n",
    "    Returns:\n",
    "        int: Utility rating from 1 to 5\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to rate the utility of the response\n",
    "    system_prompt = \"\"\"You are an AI assistant that rates the utility of a response to a query.\n",
    "    Consider how well the response answers the query, its completeness, correctness, and helpfulness.\n",
    "    Rate the utility on a scale from 1 to 5, where:\n",
    "    - 1: Not useful at all\n",
    "    - 2: Slightly useful\n",
    "    - 3: Moderately useful\n",
    "    - 4: Very useful\n",
    "    - 5: Exceptionally useful\n",
    "    Answer with ONLY a single number from 1 to 5.\"\"\"\n",
    "\n",
    "    # User prompt containing the query and the response to be rated\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "    Response:\n",
    "    {response}\n",
    "\n",
    "    Rate the utility of this response on a scale from 1 to 5:\"\"\"\n",
    "    \n",
    "    # Generate the utility rating using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the rating from the model's response\n",
    "    rating = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract just the number from the rating\n",
    "    rating_match = re.search(r'[1-5]', rating)\n",
    "    if rating_match:\n",
    "        return int(rating_match.group())  # Return the extracted rating as an integer\n",
    "    \n",
    "    return 3  # Default to middle rating if parsing fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context=None):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and optional context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str, optional): Context text\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to generate a helpful response\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Provide a clear, accurate, and informative response to the query.\"\"\"\n",
    "    \n",
    "    # Create the user prompt based on whether context is provided\n",
    "    if context:\n",
    "        user_prompt = f\"\"\"Context:\n",
    "        {context}\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Please answer the query based on the provided context.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        user_prompt = f\"\"\"Query: {query}\n",
    "        \n",
    "        Please answer the query to the best of your ability.\"\"\"\n",
    "    \n",
    "    # Generate the response using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # Return the generated response text\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    Implements the complete Self-RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store containing document chunks\n",
    "        top_k (int): Number of documents to retrieve initially\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results including query, response, and metrics from the Self-RAG process\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Starting Self-RAG for query: {query} ===\\n\")\n",
    "    \n",
    "    # Step 1: Determine if retrieval is necessary\n",
    "    print(\"Step 1: Determining if retrieval is necessary...\")\n",
    "    retrieval_needed = determine_if_retrieval_needed(query)\n",
    "    print(f\"Retrieval needed: {retrieval_needed}\")\n",
    "    \n",
    "    # Initialize metrics to track the Self-RAG process\n",
    "    metrics = {\n",
    "        \"retrieval_needed\": retrieval_needed,\n",
    "        \"documents_retrieved\": 0,\n",
    "        \"relevant_documents\": 0,\n",
    "        \"response_support_ratings\": [],\n",
    "        \"utility_ratings\": []\n",
    "    }\n",
    "    \n",
    "    best_response = None\n",
    "    best_score = -1\n",
    "    \n",
    "    if retrieval_needed:\n",
    "        # Step 2: Retrieve documents\n",
    "        print(\"\\nStep 2: Retrieving relevant documents...\")\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        metrics[\"documents_retrieved\"] = len(results)\n",
    "        print(f\"Retrieved {len(results)} documents\")\n",
    "        \n",
    "        # Step 3: Evaluate relevance of each document\n",
    "        print(\"\\nStep 3: Evaluating document relevance...\")\n",
    "        relevant_contexts = []\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            context = result[\"text\"]\n",
    "            relevance = evaluate_relevance(query, context)\n",
    "            print(f\"Document {i+1} relevance: {relevance}\")\n",
    "            \n",
    "            if relevance == \"relevant\":\n",
    "                relevant_contexts.append(context)\n",
    "        \n",
    "        metrics[\"relevant_documents\"] = len(relevant_contexts)\n",
    "        print(f\"Found {len(relevant_contexts)} relevant documents\")\n",
    "        \n",
    "        if relevant_contexts:\n",
    "            # Step 4: Process each relevant context\n",
    "            print(\"\\nStep 4: Processing relevant contexts...\")\n",
    "            for i, context in enumerate(relevant_contexts):\n",
    "                print(f\"\\nProcessing context {i+1}/{len(relevant_contexts)}...\")\n",
    "                \n",
    "                # Generate response based on the context\n",
    "                print(\"Generating response...\")\n",
    "                response = generate_response(query, context)\n",
    "                \n",
    "                # Assess how well the response is supported by the context\n",
    "                print(\"Assessing support...\")\n",
    "                support_rating = assess_support(response, context)\n",
    "                print(f\"Support rating: {support_rating}\")\n",
    "                metrics[\"response_support_ratings\"].append(support_rating)\n",
    "                \n",
    "                # Rate the utility of the response\n",
    "                print(\"Rating utility...\")\n",
    "                utility_rating = rate_utility(query, response)\n",
    "                print(f\"Utility rating: {utility_rating}/5\")\n",
    "                metrics[\"utility_ratings\"].append(utility_rating)\n",
    "                \n",
    "                # Calculate overall score (higher for better support and utility)\n",
    "                support_score = {\n",
    "                    \"fully supported\": 3, \n",
    "                    \"partially supported\": 1, \n",
    "                    \"no support\": 0\n",
    "                }.get(support_rating, 0)\n",
    "                \n",
    "                overall_score = support_score * 5 + utility_rating\n",
    "                print(f\"Overall score: {overall_score}\")\n",
    "                \n",
    "                # Keep track of the best response\n",
    "                if overall_score > best_score:\n",
    "                    best_response = response\n",
    "                    best_score = overall_score\n",
    "                    print(\"New best response found!\")\n",
    "        \n",
    "        # If no relevant contexts were found or all responses scored poorly\n",
    "        if not relevant_contexts or best_score <= 0:\n",
    "            print(\"\\nNo suitable context found or poor responses, generating without retrieval...\")\n",
    "            best_response = generate_response(query)\n",
    "    else:\n",
    "        # No retrieval needed, generate directly\n",
    "        print(\"\\nNo retrieval needed, generating response directly...\")\n",
    "        best_response = generate_response(query)\n",
    "    \n",
    "    # Final metrics\n",
    "    metrics[\"best_score\"] = best_score\n",
    "    metrics[\"used_retrieval\"] = retrieval_needed and best_score > 0\n",
    "    \n",
    "    print(\"\\n=== Self-RAG Completed ===\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": best_response,\n",
    "        \"metrics\": metrics\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
