{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 多模态 RAG 与图像描述 (Multi-Modal RAG with Image Captioning)\n",
    "\n",
    "在这个笔记本中，我实现了一个多模态 RAG 系统，该系统从文档中提取文本和图像，为图像生成描述，并使用两种内容类型来响应查询。这种方法通过将视觉信息纳入知识库来增强传统的 RAG。\n",
    "\n",
    "传统的 RAG 系统只处理文本，但许多文档在图像、图表和表格中包含关键信息。通过为这些视觉元素生成描述并将它们纳入我们的检索系统，我们可以：\n",
    "\n",
    "- 访问锁定在图形和图表中的信息\n",
    "- 理解补充文本的表格和图表\n",
    "- 创建更全面的知识库\n",
    "- 回答依赖于视觉数据的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置环境\n",
    "我们首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import re\n",
    "import tempfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置 OpenAI API 客户端\n",
    "我们初始化 OpenAI 客户端来生成嵌入向量和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用基础 URL 和 API 密钥初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 从环境变量中获取 API 密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    从 PDF 文件中提取文本和图像。\n",
    "    \n",
    "    参数:\n",
    "        pdf_path (str): PDF 文件的路径\n",
    "        output_dir (str, optional): 保存提取图像的目录\n",
    "        \n",
    "    返回:\n",
    "        Tuple[List[Dict], List[Dict]]: 文本数据和图像数据\n",
    "    \"\"\"\n",
    "    # 如果未提供，为图像创建临时目录\n",
    "    temp_dir = None\n",
    "    if output_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        output_dir = temp_dir\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    text_data = []  # 存储提取的文本数据的列表\n",
    "    image_paths = []  # 存储提取的图像路径的列表\n",
    "    \n",
    "    print(f\"从 {pdf_path} 提取内容...\")\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # 遍历 PDF 中的每一页\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "                \n",
    "                # 从页面提取文本\n",
    "                text = page.get_text().strip()\n",
    "                if text:\n",
    "                    text_data.append({\n",
    "                        \"content\": text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": page_number + 1,\n",
    "                            \"type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "                # 从页面提取图像\n",
    "                image_list = page.get_images(full=True)\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    xref = img[0]  # 图像的 XREF\n",
    "                    base_image = pdf_file.extract_image(xref)\n",
    "                    \n",
    "                    if base_image:\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "                        \n",
    "                        # 将图像保存到输出目录\n",
    "                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "                        img_path = os.path.join(output_dir, img_filename)\n",
    "                        \n",
    "                        with open(img_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "                        \n",
    "                        image_paths.append({\n",
    "                            \"path\": img_path,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_path,\n",
    "                                \"page\": page_number + 1,\n",
    "                                \"image_index\": img_index + 1,\n",
    "                                \"type\": \"image\"\n",
    "                            }\n",
    "                        })\n",
    "        \n",
    "        print(f\"提取了 {len(text_data)} 个文本段落和 {len(image_paths)} 张图像\")\n",
    "        return text_data, image_paths\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"提取内容时出错：{e}\")\n",
    "        if temp_dir and os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本内容分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text_data, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    将文本数据分割成重叠的块。\n",
    "    \n",
    "    参数:\n",
    "        text_data (List[Dict]): 从 PDF 提取的文本数据\n",
    "        chunk_size (int): 每个块的字符数大小\n",
    "        overlap (int): 块之间的重叠字符数\n",
    "        \n",
    "    返回:\n",
    "        List[Dict]: 分块的文本数据\n",
    "    \"\"\"\n",
    "    chunked_data = []  # 初始化一个空列表来存储分块数据\n",
    "    \n",
    "    for item in text_data:\n",
    "        text = item[\"content\"]  # 提取文本内容\n",
    "        metadata = item[\"metadata\"]  # 提取元数据\n",
    "        \n",
    "        # 如果文本太短则跳过\n",
    "        if len(text) < chunk_size / 2:\n",
    "            chunked_data.append({\n",
    "                \"content\": text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # 创建带重叠的块\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]  # 提取指定大小的块\n",
    "            if chunk:  # 确保我们不添加空块\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        # 添加每个块并更新元数据\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = metadata.copy()  # 复制原始元数据\n",
    "            chunk_metadata[\"chunk_index\"] = i  # 向元数据添加块索引\n",
    "            chunk_metadata[\"chunk_count\"] = len(chunks)  # 向元数据添加总块数\n",
    "            \n",
    "            chunked_data.append({\n",
    "                \"content\": chunk,  # 块文本\n",
    "                \"metadata\": chunk_metadata  # 更新的元数据\n",
    "            })\n",
    "    \n",
    "    print(f\"创建了 {len(chunked_data)} 个文本块\")  # 打印创建的块数量\n",
    "    return chunked_data  # 返回分块数据列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 OpenAI Vision 进行图像描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    将图像文件编码为 base64。\n",
    "    \n",
    "    参数:\n",
    "        image_path (str): 图像文件的路径\n",
    "        \n",
    "    返回:\n",
    "        str: Base64 编码的图像\n",
    "    \"\"\"\n",
    "    # 以二进制读取模式打开图像文件\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # 读取图像文件并编码为 base64\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        # 将 base64 字节解码为字符串并返回\n",
    "        return encoded_image.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的视觉能力为图像生成描述。\n",
    "    \n",
    "    参数:\n",
    "        image_path (str): 图像文件的路径\n",
    "        \n",
    "    返回:\n",
    "        str: 生成的描述\n",
    "    \"\"\"\n",
    "    # 检查文件是否存在且是图像\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"错误：未找到图像文件\"\n",
    "    \n",
    "    try:\n",
    "        # 打开并验证图像\n",
    "        Image.open(image_path)\n",
    "        \n",
    "        # 将图像编码为 base64\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        # 创建 API 请求来生成描述\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llava-hf/llava-1.5-7b-hf\", # 使用 llava-1.5-7b 模型\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"你是一个专门描述学术论文中图像的助手。\"\n",
    "                    \"为图像提供详细的描述，捕捉关键信息。\"\n",
    "                    \"如果图像包含图表、表格或图形，请清楚地描述它们的内容和目的。\"\n",
    "                    \"你的描述应该针对未来检索进行优化，当人们询问此内容时。\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"详细描述这张图像，重点关注其学术内容：\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # 从响应中提取描述\n",
    "        caption = response.choices[0].message.content\n",
    "        return caption\n",
    "    \n",
    "    except Exception as e:\n",
    "        # 如果发生异常，返回错误消息\n",
    "        return f\"生成描述时出错：{str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_paths):\n",
    "    \"\"\"\n",
    "    处理所有图像并生成描述。\n",
    "    \n",
    "    参数:\n",
    "        image_paths (List[Dict]): 提取的图像路径\n",
    "        \n",
    "    返回:\n",
    "        List[Dict]: 带有描述的图像数据\n",
    "    \"\"\"\n",
    "    image_data = []  # 初始化一个空列表来存储带描述的图像数据\n",
    "    \n",
    "    print(f\"为 {len(image_paths)} 张图像生成描述...\")  # 打印要处理的图像数量\n",
    "    for i, img_item in enumerate(image_paths):\n",
    "        print(f\"处理图像 {i+1}/{len(image_paths)}...\")  # 打印当前正在处理的图像\n",
    "        img_path = img_item[\"path\"]  # 获取图像路径\n",
    "        metadata = img_item[\"metadata\"]  # 获取图像元数据\n",
    "        \n",
    "        # 为图像生成描述\n",
    "        caption = generate_image_caption(img_path)\n",
    "        \n",
    "        # 将带描述的图像数据添加到列表中\n",
    "        image_data.append({\n",
    "            \"content\": caption,  # 生成的描述\n",
    "            \"metadata\": metadata,  # 图像元数据\n",
    "            \"image_path\": img_path  # 图像路径\n",
    "        })\n",
    "    \n",
    "    return image_data  # 返回带描述的图像数据列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单向量存储实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalVectorStore:\n",
    "    \"\"\"\n",
    "    多模态内容的简单向量存储实现。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初始化列表来存储向量、内容和元数据\n",
    "        self.vectors = []\n",
    "        self.contents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, content, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        向向量存储中添加一个项目。\n",
    "        \n",
    "        参数:\n",
    "            content (str): 内容（文本或图像描述）\n",
    "            embedding (List[float]): 嵌入向量\n",
    "            metadata (Dict, optional): 附加元数据\n",
    "        \"\"\"\n",
    "        # 将嵌入向量、内容和元数据追加到各自的列表中\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.contents.append(content)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        向向量存储中添加多个项目。\n",
    "        \n",
    "        参数:\n",
    "            items (List[Dict]): 内容项目列表\n",
    "            embeddings (List[List[float]]): 嵌入向量列表\n",
    "        \"\"\"\n",
    "        # 遍历项目和嵌入向量，将每个添加到向量存储中\n",
    "        for item, embedding in zip(items, embeddings):\n",
    "            self.add_item(\n",
    "                content=item[\"content\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入向量最相似的项目。\n",
    "        \n",
    "        参数:\n",
    "            query_embedding (List[float]): 查询嵌入向量\n",
    "            k (int): 返回结果的数量\n",
    "            \n",
    "        返回:\n",
    "            List[Dict]: 前 k 个最相似的项目\n",
    "        \"\"\"\n",
    "        # 如果存储中没有向量，返回空列表\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 将查询嵌入向量转换为 numpy 数组\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 使用余弦相似度计算相似性\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 按相似度降序排序\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 返回前 k 个结果\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"content\": self.contents[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)  # 转换为 float 以便 JSON 序列化\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    为给定的文本创建嵌入向量。\n",
    "    \n",
    "    参数:\n",
    "        texts (List[str]): 输入文本\n",
    "        model (str): 嵌入模型名称\n",
    "        \n",
    "    返回:\n",
    "        List[List[float]]: 嵌入向量\n",
    "    \"\"\"\n",
    "    # 处理空输入\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # 如果需要，分批处理（OpenAI API 限制）\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # 分批遍历输入文本\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # 获取当前批次的文本\n",
    "        \n",
    "        # 为当前批次创建嵌入向量\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # 从响应中提取嵌入向量\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # 将批次嵌入向量添加到列表中\n",
    "    \n",
    "    return all_embeddings  # 返回所有嵌入向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的处理流水线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    为多模态 RAG 处理文档。\n",
    "    \n",
    "    参数:\n",
    "        pdf_path (str): PDF 文件的路径\n",
    "        chunk_size (int): 每个块的字符数大小\n",
    "        chunk_overlap (int): 块之间的重叠字符数\n",
    "        \n",
    "    返回:\n",
    "        Tuple[MultiModalVectorStore, Dict]: 向量存储和文档信息\n",
    "    \"\"\"\n",
    "    # 为提取的图像创建目录\n",
    "    image_dir = \"extracted_images\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    \n",
    "    # 从 PDF 提取文本和图像\n",
    "    text_data, image_paths = extract_content_from_pdf(pdf_path, image_dir)\n",
    "    \n",
    "    # 对提取的文本进行分块\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # 处理提取的图像以生成描述\n",
    "    image_data = process_images(image_paths)\n",
    "    \n",
    "    # 合并所有内容项目（文本块和图像描述）\n",
    "    all_items = chunked_text + image_data\n",
    "    \n",
    "    # 提取内容用于嵌入\n",
    "    contents = [item[\"content\"] for item in all_items]\n",
    "    \n",
    "    # 为所有内容创建嵌入向量\n",
    "    print(\"为所有内容创建嵌入向量...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # 构建向量存储并添加项目及其嵌入向量\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(all_items, embeddings)\n",
    "    \n",
    "    # 准备包含文本块和图像描述计数的文档信息\n",
    "    doc_info = {\n",
    "        \"text_count\": len(chunked_text),\n",
    "        \"image_count\": len(image_data),\n",
    "        \"total_items\": len(all_items),\n",
    "    }\n",
    "    \n",
    "    # 打印添加项目的摘要\n",
    "    print(f\"向向量存储添加了 {len(all_items)} 个项目（{len(chunked_text)} 个文本块，{len(image_data)} 个图像描述）\")\n",
    "    \n",
    "    # 返回向量存储和文档信息\n",
    "    return vector_store, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询处理和响应生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multimodal_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    查询多模态 RAG 系统。\n",
    "    \n",
    "    参数:\n",
    "        query (str): 用户查询\n",
    "        vector_store (MultiModalVectorStore): 包含文档内容的向量存储\n",
    "        k (int): 要检索的结果数量\n",
    "        \n",
    "    返回:\n",
    "        Dict: 查询结果和生成的响应\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 处理查询：{query} ===\\n\")\n",
    "    \n",
    "    # 为查询生成嵌入向量\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # 从向量存储中检索相关内容\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # 分离文本和图像结果\n",
    "    text_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"text\"]\n",
    "    image_results = [r for r in results if r[\"metadata\"].get(\"type\") == \"image\"]\n",
    "    \n",
    "    print(f\"检索到 {len(results)} 个相关项目（{len(text_results)} 个文本，{len(image_results)} 个图像描述）\")\n",
    "    \n",
    "    # 使用检索到的内容生成响应\n",
    "    response = generate_response(query, results)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"response\": response,\n",
    "        \"text_results_count\": len(text_results),\n",
    "        \"image_results_count\": len(image_results)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(query, results):\n",
    "    \"\"\"\n",
    "    基于查询和检索结果生成响应。\n",
    "    \n",
    "    参数:\n",
    "        query (str): 用户查询\n",
    "        results (List[Dict]): 检索到的内容\n",
    "        \n",
    "    返回:\n",
    "        str: 生成的响应\n",
    "    \"\"\"\n",
    "    # 从检索结果格式化上下文\n",
    "    context = \"\"\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        # 确定内容类型（文本或图像描述）\n",
    "        content_type = \"文本\" if result[\"metadata\"].get(\"type\") == \"text\" else \"图像描述\"\n",
    "        # 从元数据获取页码\n",
    "        page_num = result[\"metadata\"].get(\"page\", \"未知\")\n",
    "        \n",
    "        # 将内容类型和页码追加到上下文中\n",
    "        context += f\"[来自第 {page_num} 页的{content_type}]\\n\"\n",
    "        # 将实际内容追加到上下文中\n",
    "        context += result[\"content\"]\n",
    "        context += \"\\n\\n\"\n",
    "    \n",
    "    # 指导 AI 助手的系统消息\n",
    "    system_message = \"\"\"你是一个专门回答包含文本和图像的文档问题的 AI 助手。\n",
    "    你已经获得了文档中的相关文本段落和图像描述。\n",
    "    使用这些信息为查询提供全面、准确的响应。\n",
    "    如果信息来自图像或图表，请在答案中提及这一点。\n",
    "    如果检索到的信息无法完全回答查询，请承认这些限制。\"\"\"\n",
    "\n",
    "    # 包含查询和格式化上下文的用户消息\n",
    "    user_message = f\"\"\"查询：{query}\n",
    "\n",
    "    检索到的内容：\n",
    "    {context}\n",
    "\n",
    "    请基于检索到的内容回答查询。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 使用 OpenAI API 生成响应\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # 返回生成的响应\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与纯文本 RAG 的评估对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    构建纯文本向量存储用于比较。\n",
    "    \n",
    "    参数:\n",
    "        pdf_path (str): PDF 文件的路径\n",
    "        chunk_size (int): 每个块的字符数大小\n",
    "        chunk_overlap (int): 块之间的重叠字符数\n",
    "        \n",
    "    返回:\n",
    "        MultiModalVectorStore: 纯文本向量存储\n",
    "    \"\"\"\n",
    "    # 从 PDF 提取文本（重用函数但忽略图像）\n",
    "    text_data, _ = extract_content_from_pdf(pdf_path, None)\n",
    "    \n",
    "    # 分块文本\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # 提取内容用于嵌入\n",
    "    contents = [item[\"content\"] for item in chunked_text]\n",
    "    \n",
    "    # 创建嵌入向量\n",
    "    print(\"为纯文本内容创建嵌入向量...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # 构建向量存储\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(chunked_text, embeddings)\n",
    "    \n",
    "    print(f\"向纯文本向量存储添加了 {len(chunked_text)} 个文本项目\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multimodal_vs_textonly(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    比较多模态 RAG 与纯文本 RAG。\n",
    "    \n",
    "    参数:\n",
    "        pdf_path (str): PDF 文件的路径\n",
    "        test_queries (List[str]): 测试查询\n",
    "        reference_answers (List[str], optional): 参考答案\n",
    "        \n",
    "    返回:\n",
    "        Dict: 评估结果\n",
    "    \"\"\"\n",
    "    print(\"=== 评估多模态 RAG 与纯文本 RAG ===\\n\")\n",
    "    \n",
    "    # 为多模态 RAG 处理文档\n",
    "    print(\"\\n为多模态 RAG 处理文档...\")\n",
    "    mm_vector_store, mm_doc_info = process_document(pdf_path)\n",
    "    \n",
    "    # 构建纯文本存储\n",
    "    print(\"\\n为纯文本 RAG 处理文档...\")\n",
    "    text_vector_store = build_text_only_store(pdf_path)\n",
    "    \n",
    "    # 为每个查询运行评估\n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== 评估查询 {i+1}：{query} ===\")\n",
    "        \n",
    "        # 如果可用，获取参考答案\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # 运行多模态 RAG\n",
    "        print(\"\\n运行多模态 RAG...\")\n",
    "        mm_result = query_multimodal_rag(query, mm_vector_store)\n",
    "        \n",
    "        # 运行纯文本 RAG\n",
    "        print(\"\\n运行纯文本 RAG...\")\n",
    "        text_result = query_multimodal_rag(query, text_vector_store)\n",
    "        \n",
    "        # 比较响应\n",
    "        comparison = compare_responses(query, mm_result[\"response\"], text_result[\"response\"], reference)\n",
    "        \n",
    "        # 添加到结果中\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"multimodal_response\": mm_result[\"response\"],\n",
    "            \"textonly_response\": text_result[\"response\"],\n",
    "            \"multimodal_results\": {\n",
    "                \"text_count\": mm_result[\"text_results_count\"],\n",
    "                \"image_count\": mm_result[\"image_results_count\"]\n",
    "            },\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison\n",
    "        })\n",
    "    \n",
    "    # 生成总体分析\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis,\n",
    "        \"multimodal_doc_info\": mm_doc_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, mm_response, text_response, reference=None):\n",
    "    \"\"\"\n",
    "    比较多模态和纯文本响应。\n",
    "    \n",
    "    参数:\n",
    "        query (str): 用户查询\n",
    "        mm_response (str): 多模态响应\n",
    "        text_response (str): 纯文本响应\n",
    "        reference (str, optional): 参考答案\n",
    "        \n",
    "    返回:\n",
    "        str: 比较分析\n",
    "    \"\"\"\n",
    "    # 评估员的系统提示词\n",
    "    system_prompt = \"\"\"你是比较两个 RAG 系统的专家评估员：\n",
    "    1. 多模态 RAG：从文本和图像描述中检索\n",
    "    2. 纯文本 RAG：仅从文本中检索\n",
    "\n",
    "    基于以下标准评估哪个响应更好地回答了查询：\n",
    "    - 准确性和正确性\n",
    "    - 信息的完整性\n",
    "    - 与查询的相关性\n",
    "    - 来自视觉元素的独特信息（对于多模态）\"\"\"\n",
    "\n",
    "    # 包含查询和响应的用户提示词\n",
    "    user_prompt = f\"\"\"查询：{query}\n",
    "\n",
    "    多模态 RAG 响应：\n",
    "    {mm_response}\n",
    "\n",
    "    纯文本 RAG 响应：\n",
    "    {text_response}\n",
    "    \"\"\"\n",
    "\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "    参考答案：\n",
    "    {reference}\n",
    "    \"\"\"\n",
    "\n",
    "        user_prompt += \"\"\"\n",
    "    比较这些响应并解释哪一个更好地回答了查询以及原因。\n",
    "    注意多模态响应中来自图像的任何特定信息。\n",
    "    \"\"\"\n",
    "\n",
    "    # 使用 meta-llama/Llama-3.2-3B-Instruct 生成比较\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    生成多模态与纯文本 RAG 的总体分析。\n",
    "    \n",
    "    参数:\n",
    "        results (List[Dict]): 每个查询的评估结果\n",
    "        \n",
    "    返回:\n",
    "        str: 总体分析\n",
    "    \"\"\"\n",
    "    # 评估员的系统提示词\n",
    "    system_prompt = \"\"\"你是 RAG 系统的专家评估员。基于多个测试查询，\n",
    "    提供比较多模态 RAG（文本 + 图像）与纯文本 RAG 的总体分析。\n",
    "\n",
    "    重点关注：\n",
    "    1. 多模态 RAG 优于纯文本的查询类型\n",
    "    2. 纳入图像信息的具体优势\n",
    "    3. 多模态方法的任何缺点或限制\n",
    "    4. 何时使用每种方法的总体建议\"\"\"\n",
    "\n",
    "    # 创建评估摘要\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"查询 {i+1}：{result['query']}\\n\"\n",
    "        evaluations_summary += f\"多模态检索到 {result['multimodal_results']['text_count']} 个文本块和 {result['multimodal_results']['image_count']} 个图像描述\\n\"\n",
    "        evaluations_summary += f\"比较摘要：{result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # 包含评估摘要的用户提示词\n",
    "    user_prompt = f\"\"\"基于以下对 {len(results)} 个查询的多模态与纯文本 RAG 评估，\n",
    "    提供比较这两种方法的总体分析：\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    请提供多模态 RAG 相对于纯文本 RAG 的相对优缺点的全面分析，\n",
    "    特别关注图像信息如何贡献（或未贡献）响应质量。\"\"\"\n",
    "\n",
    "    # 使用 meta-llama/Llama-3.2-3B-Instruct 生成总体分析\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多模态 RAG 与纯文本 RAG 的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 评估多模态 RAG 与纯文本 RAG ===\n",
      "\n",
      "\n",
      "为多模态 RAG 处理文档...\n",
      "从 data/attention_is_all_you_need.pdf 提取内容...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取了 15 个文本段落和 3 张图像\n",
      "创建了 59 个文本块\n",
      "为 3 张图像生成描述...\n",
      "处理图像 1/3...\n",
      "处理图像 2/3...\n",
      "处理图像 3/3...\n",
      "为所有内容创建嵌入向量...\n",
      "向向量存储添加了 62 个项目（59 个文本块，3 个图像描述）\n",
      "\n",
      "为纯文本 RAG 处理文档...\n",
      "从 data/attention_is_all_you_need.pdf 提取内容...\n",
      "提取了 15 个文本段落和 3 张图像\n",
      "创建了 59 个文本块\n",
      "为纯文本内容创建嵌入向量...\n",
      "向纯文本向量存储添加了 59 个文本项目\n",
      "\n",
      "\n",
      "=== 评估查询 1：Transformer（基础模型）的 BLEU 分数是多少？ ===\n",
      "\n",
      "运行多模态 RAG...\n",
      "\n",
      "=== 处理查询：Transformer（基础模型）的 BLEU 分数是多少？ ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faree\\AppData\\Local\\Temp\\ipykernel_14692\\2117883450.py:75: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # Convert to float for JSON serialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到 5 个相关项目（5 个文本，0 个图像描述）\n",
      "\n",
      "运行纯文本 RAG...\n",
      "\n",
      "=== 处理查询：Transformer（基础模型）的 BLEU 分数是多少？ ===\n",
      "\n",
      "检索到 5 个相关项目（5 个文本，0 个图像描述）\n",
      "\n",
      "=== 总体分析 ===\n",
      "\n",
      "**多模态 RAG 与纯文本 RAG 的总体分析**\n",
      "\n",
      "基于对单个查询的评估，我们可以得出以下关于多模态 RAG 相对于纯文本 RAG 的分析：\n",
      "\n",
      "**多模态 RAG 优于纯文本的查询类型：**\n",
      "\n",
      "1. **视觉数据查询**：当查询涉及图表、图形、表格或其他视觉元素中包含的信息时，多模态 RAG 具有明显优势。在这种情况下，图像描述可以提供纯文本中可能缺失的关键信息。\n",
      "\n",
      "2. **复合信息查询**：需要结合文本和视觉信息来提供完整答案的查询，多模态方法可以提供更全面的响应。\n",
      "\n",
      "**纳入图像信息的具体优势：**\n",
      "\n",
      "1. **信息完整性**：图像描述可以补充文本信息，提供更完整的知识库。\n",
      "\n",
      "2. **数据验证**：图像中的信息可以作为文本信息的验证或补充，提高答案的可靠性。\n",
      "\n",
      "3. **上下文丰富性**：视觉元素可以提供额外的上下文，帮助更好地理解复杂概念。\n",
      "\n",
      "**多模态方法的缺点或限制：**\n",
      "\n",
      "1. **处理复杂性**：多模态系统需要额外的图像处理和描述生成步骤，增加了系统复杂性和计算成本。\n",
      "\n",
      "2. **图像描述质量依赖**：系统的性能很大程度上依赖于图像描述的质量。如果图像描述不准确或不完整，可能会影响整体性能。\n",
      "\n",
      "3. **检索效率**：在当前评估的查询中，多模态 RAG 检索到的图像描述为 0，这表明对于某些类型的查询，额外的图像处理可能不会带来实际益处。\n",
      "\n",
      "**当前评估的具体观察：**\n",
      "\n",
      "在评估的查询\"Transformer（基础模型）的 BLEU 分数是多少？\"中，多模态 RAG 检索到 5 个文本块和 0 个图像描述。这表明：\n",
      "\n",
      "- 对于这个特定查询，相关信息主要存在于文本中\n",
      "- 图像内容与查询的相关性较低\n",
      "- 在这种情况下，多模态和纯文本方法可能产生相似的结果\n",
      "\n",
      "**何时使用每种方法的建议：**\n",
      "\n",
      "1. **使用多模态 RAG 的情况：**\n",
      "   - 文档包含大量图表、表格、图形等视觉元素\n",
      "   - 查询可能涉及视觉信息\n",
      "   - 需要最全面的信息检索\n",
      "   - 对准确性要求高于效率要求\n",
      "\n",
      "2. **使用纯文本 RAG 的情况：**\n",
      "   - 文档主要是文本内容\n",
      "   - 查询主要涉及文本信息\n",
      "   - 对处理速度和效率有较高要求\n",
      "   - 系统资源有限\n",
      "\n",
      "**结论：**\n",
      "\n",
      "多模态 RAG 在处理包含丰富视觉信息的文档时具有明显优势，但其效果很大程度上取决于查询类型和文档内容。对于主要基于文本的查询，纯文本 RAG 可能已经足够。选择哪种方法应该基于具体的应用场景、文档类型和性能要求。\n"
     ]
    }
   ],
   "source": [
    "# 你的 PDF 文档路径\n",
    "pdf_path = \"data/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# 定义针对文本和视觉内容的测试查询\n",
    "test_queries = [\n",
    "    \"Transformer（基础模型）的 BLEU 分数是多少？\",\n",
    "]\n",
    "\n",
    "# 用于评估的可选参考答案\n",
    "reference_answers = [\n",
    "    \"Transformer（基础模型）在 WMT 2014 英德翻译任务上达到了 27.3 的 BLEU 分数，在 WMT 2014 英法翻译任务上达到了 38.1 的 BLEU 分数。\",\n",
    "]\n",
    "\n",
    "# 运行评估\n",
    "evaluation_results = evaluate_multimodal_vs_textonly(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# 打印总体分析\n",
    "print(\"\\n=== 总体分析 ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}