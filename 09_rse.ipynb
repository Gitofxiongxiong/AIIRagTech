{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用于增强 RAG 的相关片段提取 (RSE)\n",
    "\n",
    "相关片段提取 (RSE)技术，用以提升我们 RAG 系统中的上下文质量，不再是简单地检索一堆孤立的文本块，而是识别并重构出连续的文本片段，从而为我们的语言模型提供更优质的上下文。\n",
    "\n",
    "## 核心概念\n",
    "\n",
    "在文档中，相关的文本块往往会聚集在一起。通过识别这些集群并保持其连续性，我们可以为大语言模型（LLM）的工作提供更连贯的上下文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从pdf提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    提取PDF文件中的文本并打印前`num_chars`个字符。\n",
    "\n",
    "    参数：\n",
    "    pdf_path (str): PDF文件的路径。\n",
    "\n",
    "    返回：\n",
    "    str: 从PDF中提取的文本。\n",
    "\n",
    "    \"\"\"\n",
    "    # 打开PDF文件\n",
    "    mypdf = pymupdf.open(pdf_path)\n",
    "    all_text = \"\"  # 初始化一个空字符串来存储提取的文本\n",
    "\n",
    "    # 迭代PDF中的每个页面\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 获取页面\n",
    "        text = page.get_text(\"text\")  # 从页面中提取文本\n",
    "        all_text += text  # 将提取的文本附加到all_text字符串\n",
    "\n",
    "    return all_text  # 返回提取的文本\n",
    "\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    将文本分割为多个块，每个块的大小为n，重叠部分为overlap。\n",
    "    参数：\n",
    "    text: 输入的文本\n",
    "    n: 每个块的大小\n",
    "    overlap: 相邻块之间的重叠部分大小\n",
    "\n",
    "    返回：\n",
    "    文本块列表\n",
    "    \"\"\"\n",
    "    chunks = []  \n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        \n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 百炼服务的base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简易的向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    简易的向量存储库。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "        self.documents = []\n",
    "    def add_documents(self, documents, vectors=None, metadata=None):\n",
    "        \"\"\"\n",
    "        Add documents to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            documents (List[str]): List of document chunks\n",
    "            vectors (List[List[float]], optional): List of embedding vectors\n",
    "            metadata (List[Dict], optional): List of metadata dictionaries\n",
    "        \"\"\"\n",
    "        if vectors is None:\n",
    "            vectors = [None] * len(documents)\n",
    "        \n",
    "        if metadata is None:\n",
    "            metadata = [{} for _ in range(len(documents))]\n",
    "        \n",
    "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
    "            self.documents.append(doc)\n",
    "            self.vectors.append(vec)\n",
    "            self.metadata.append(meta)\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        添加一个新的项到存储库。\n",
    "\n",
    "        参数:\n",
    "        text (str): 文本内容。\n",
    "        embedding (List[float]): 文本的嵌入向量。\n",
    "        metadata (Dict, optional): 与文本相关的元数据。\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def search(self, query_vector, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for most similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query_vector (List[float]): Query embedding vector\n",
    "            top_k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of results with documents, scores, and metadata\n",
    "        \"\"\"\n",
    "        if not self.vectors or not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # Convert query vector to numpy array\n",
    "        query_array = np.array(query_vector)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if vector is not None:\n",
    "\n",
    "                similarity = np.dot(query_array, vector) / (\n",
    "                    np.linalg.norm(query_array) * np.linalg.norm(vector)\n",
    "                )\n",
    "                similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top-k results\n",
    "        results = []\n",
    "        for i, score in similarities[:top_k]:\n",
    "            results.append({\n",
    "                \"document\": self.documents[i],\n",
    "                \"score\": float(score),\n",
    "                \"metadata\": self.metadata[i]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入向量最相似的文本。\n",
    "\n",
    "        参数:\n",
    "        query_embedding (List[float]): 查询的嵌入向量。\n",
    "        k (int, optional): 返回最相似的k个结果。\n",
    "\n",
    "        返回:\n",
    "        List[Dict]: 最相似的文本及其相关信息。\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "\n",
    "        similarities = []\n",
    "\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_in_batches(text_chunks, model=\"text-embedding-v3\", batch_size_limit=10): # 我改成了官方模型名，你可以换回 \"text-embedding-v3\"\n",
    "    \"\"\"\n",
    "    调用 OpenAI 的 Embedding API 来创建文本列表的嵌入向量，处理批处理大小限制。\n",
    "\n",
    "    参数:\n",
    "    text_chunks (List[str]): 需要创建嵌入的文本字符串列表。\n",
    "    model (str): 使用的嵌入模型。\n",
    "    batch_size_limit (int): API 允许的最大批处理大小。根据错误信息，这里是10。\n",
    "\n",
    "    返回:\n",
    "    List[List[float]]: 所有文本的嵌入向量列表。\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    if not text_chunks:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(text_chunks, list): # 确保输入是列表\n",
    "        text_chunks = [text_chunks]\n",
    "\n",
    "    for i in range(0, len(text_chunks), batch_size_limit):\n",
    "        batch = text_chunks[i:i + batch_size_limit]\n",
    "        try:\n",
    "            #print(f\"Processing batch {i//batch_size_limit + 1}, size: {len(batch)}\")\n",
    "            response = client.embeddings.create(\n",
    "                input=batch,\n",
    "                model=model,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "            # 从响应中提取该批次的嵌入向量\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting with chunk: '{batch[0][:50]}...'\")\n",
    "            print(f\"API Error: {e}\")\n",
    "\n",
    "            raise e \n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "def create_embeddings(text, model=\"text-embedding-v3\"):\n",
    "    \"\"\"\n",
    "    字符串向量化\n",
    "    参数:\n",
    "    text (str): 需要创建嵌入的文本字符串。\n",
    "    model (str): 使用的嵌入模型。\n",
    "\n",
    "    返回:\n",
    "    List[float]: 文本的嵌入向量。\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=800):\n",
    "    \n",
    "    print(\"Extracting text from document...\")\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text into non-overlapping segments...\")\n",
    "\n",
    "    chunks = chunk_text(text, chunk_size, 0)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    print(\"Generating embeddings for chunks...\")\n",
    "\n",
    "    chunk_embeddings = create_embeddings_in_batches(chunks)\n",
    "    \n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    metadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n",
    "    vector_store.add_documents(chunks, chunk_embeddings, metadata)\n",
    "    \n",
    "\n",
    "    doc_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"source\": pdf_path,\n",
    "    }\n",
    "    \n",
    "    return chunks, vector_store, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSE核心方法：计算分块值和查找最佳段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    通过相关性和位置来计算块值。\n",
    "    参数：\n",
    "        query (str): 查询文本\n",
    "        chunks (List[str]): 文档块列表\n",
    "        vector_store (SimpleVectorStore): 包含块的向量存储\n",
    "        irrelevant_chunk_penalty (float): 无关块的惩罚\n",
    "    返回：\n",
    "        List[float]: 块值列表\n",
    "    \"\"\"\n",
    "    query_embedding = create_embeddings(query)\n",
    "    num_chunks = len(chunks)\n",
    "    results = vector_store.search(query_embedding, num_chunks)\n",
    "    \n",
    "    relevance_scores = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n",
    "    \n",
    "    chunk_values = []\n",
    "    for i in range(num_chunks):\n",
    "        score = relevance_scores.get(i, 0.0)\n",
    "        value = score - irrelevant_chunk_penalty\n",
    "        chunk_values.append(value)\n",
    "    \n",
    "    return chunk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n",
    "    \"\"\"\n",
    "    使用最大子数组和的变体算法，找到最优的连续文本段,贪心算法。\n",
    "\n",
    "    参数：\n",
    "        chunk_values (List[float]): 每个块的值的列表\n",
    "        max_segment_length (int): 单个段的最大长度\n",
    "        total_max_length (int): 所有段的最大总长度\n",
    "        min_segment_value (float): 段的最小取值，以考虑\n",
    "\n",
    "    返回：\n",
    "        List[Tuple[int, int]]: 最优段的 (start, end) 索引列表\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal continuous text segments...\")\n",
    "    \n",
    "    best_segments = []\n",
    "    segment_scores = []\n",
    "    total_included_chunks = 0\n",
    "    \n",
    "    while total_included_chunks < total_max_length:\n",
    "        best_score = min_segment_value  \n",
    "        best_segment = None\n",
    "\n",
    "        for start in range(len(chunk_values)):\n",
    "\n",
    "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
    "                continue\n",
    "                \n",
    "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
    "                end = start + length\n",
    "                \n",
    "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
    "                    continue\n",
    "                \n",
    "                segment_value = sum(chunk_values[start:end])\n",
    "                \n",
    "                if segment_value > best_score:\n",
    "                    best_score = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        if best_segment:\n",
    "            best_segments.append(best_segment)\n",
    "            segment_scores.append(best_score)\n",
    "            total_included_chunks += best_segment[1] - best_segment[0]\n",
    "            print(f\"Found segment {best_segment} with score {best_score:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
    "    \n",
    "    return best_segments, segment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重建段落并使用段落来做RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_segments(chunks, best_segments):\n",
    "    \"\"\"\n",
    "    重新构造文本段，基于块索引。\n",
    "    参数：\n",
    "        chunks (List[str]): 所有文档块的列表\n",
    "        best_segments (List[Tuple[int, int]]): 段的 (start, end) 索引列表\n",
    "    返回：\n",
    "        List[str]: 重新构造的文本段列表\n",
    "    \"\"\"\n",
    "    reconstructed_segments = []  \n",
    "    for start, end in best_segments:\n",
    "        \n",
    "        segment_text = \" \".join(chunks[start:end])\n",
    "        \n",
    "        reconstructed_segments.append({\n",
    "            \"text\": segment_text,\n",
    "            \"segment_range\": (start, end),\n",
    "        })\n",
    "    \n",
    "    return reconstructed_segments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_segments_for_context(segments):\n",
    "    \"\"\"\n",
    "    格式化segments为LLM的上下文字符串。\n",
    "\n",
    "    参数：\n",
    "    segments (List[Dict]): 包含segment字典的列表\n",
    "\n",
    "    返回：\n",
    "    str: 格式化的上下文文本   \n",
    "    \"\"\"\n",
    "    context = []  \n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        \n",
    "        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
    "        context.append(segment_header)  \n",
    "        context.append(segment['text'])  \n",
    "        context.append(\"-\" * 80)  \n",
    "    \n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"qwen3-4b\"):\n",
    "    print(\"Generating response using relevant segments as context...\")\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "    The context consists of document segments that have been retrieved as relevant to the user's query.\n",
    "    Use the information from these segments to provide a comprehensive and accurate answer.\n",
    "    If the context doesn't contain relevant information to answer the question, say so clearly.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:{context}\n",
    "        Question: {query}\n",
    "        Please provide a helpful answer based on the context provided.\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \n",
    "        extra_body={\n",
    "            \"enable_thinking\": False,\n",
    "            \"temperature\": 0\n",
    "            }\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整RSE流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    完整的 RAG 流水线，包括相关片段提取。\n",
    "    参数：\n",
    "        pdf_path (str): 文档的路径\n",
    "        query (str): 用户查询\n",
    "        chunk_size (int): 分块的大小\n",
    "        irrelevant_chunk_penalty (float): 与无关分块相关的惩罚\n",
    "    返回：\n",
    "        Dict: 包含查询、片段和响应的结果\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    print(\"\\nCalculating relevance scores and chunk values...\")\n",
    "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
    "    \n",
    "    best_segments, scores = find_best_segments(\n",
    "        chunk_values, \n",
    "        max_segment_length=20, \n",
    "        total_max_length=30, \n",
    "        min_segment_value=0.2\n",
    "    )\n",
    "    \n",
    "    print(\"\\nReconstructing text segments from chunks...\")\n",
    "    segments = reconstruct_segments(chunks, best_segments)\n",
    "    \n",
    "    context = format_segments_for_context(segments)\n",
    "    \n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"segments\": segments,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比标准的检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_top_k_retrieval(pdf_path, query, k=10, chunk_size=800):\n",
    "\n",
    "    print(\"\\n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "\n",
    "    print(\"Creating query embedding and retrieving chunks...\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "\n",
    "    results = vector_store.search(query_embedding, top_k=k)\n",
    "    retrieved_chunks = [result[\"document\"] for result in results]\n",
    "\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"CHUNK {i+1}:\\n{chunk}\" \n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ])\n",
    "\n",
    "    response = generate_response(query, context)\n",
    "\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_methods(pdf_path, query, reference_answer=None):\n",
    "\n",
    "    print(\"\\n========= EVALUATION =========\\n\")\n",
    "    \n",
    "    rse_result = rag_with_rse(pdf_path, query)\n",
    "\n",
    "    standard_result = standard_top_k_retrieval(pdf_path, query)\n",
    "\n",
    "    if reference_answer:\n",
    "        print(\"\\n=== COMPARING RESULTS ===\")\n",
    "\n",
    "        evaluation_prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "\n",
    "            Reference Answer:\n",
    "            {reference_answer}\n",
    "\n",
    "            Response from Standard Retrieval:\n",
    "            {standard_result[\"response\"]}\n",
    "\n",
    "            Response from Relevant Segment Extraction:\n",
    "            {rse_result[\"response\"]}\n",
    "\n",
    "            Compare these two responses against the reference answer. Which one is:\n",
    "            1. More accurate and comprehensive\n",
    "            2. Better at addressing the user's query\n",
    "            3. Less likely to include irrelevant information\n",
    "\n",
    "            Explain your reasoning for each point.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Evaluating responses against reference answer...\")\n",
    "\n",
    "        evaluation = client.chat.completions.create(\n",
    "            model=\"qwen-plus\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an objective evaluator of RAG system responses.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(evaluation.choices[0].message.content)\n",
    "\n",
    "    return {\n",
    "        \"rse_result\": rse_result,\n",
    "        \"standard_result\": standard_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= EVALUATION =========\n",
      "\n",
      "\n",
      "=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\n",
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Extracting text from document...\n",
      "Chunking text into non-overlapping segments...\n",
      "Created 42 chunks\n",
      "Generating embeddings for chunks...\n",
      "\n",
      "Calculating relevance scores and chunk values...\n",
      "Finding optimal continuous text segments...\n",
      "Found segment (22, 42) with score 6.7414\n",
      "Found segment (0, 20) with score 6.3919\n",
      "\n",
      "Reconstructing text segments from chunks...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "=== FINAL RESPONSE ===\n",
      "Explainable AI (XAI) refers to the development of AI systems that are more transparent and understandable. The goal of XAI is to provide insights into how AI models make decisions, which enhances trust and accountability. This is important because many AI systems, particularly deep learning models, are often considered \"black boxes,\" meaning it is difficult to understand how they arrive at their decisions. \n",
      "\n",
      "By making AI systems more explainable, stakeholders can better understand the reasoning behind AI decisions, which is crucial for building trust in AI technologies. This transparency is essential for ensuring that AI systems are fair, ethical, and aligned with human values. It also helps in identifying and mitigating potential biases or errors in AI decision-making processes. Therefore, Explainable AI is considered important as it addresses the need for transparency and accountability in AI systems, which are critical for their responsible development and deployment.\n",
      "\n",
      "=== STARTING STANDARD TOP-K RETRIEVAL ===\n",
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Extracting text from document...\n",
      "Chunking text into non-overlapping segments...\n",
      "Created 42 chunks\n",
      "Generating embeddings for chunks...\n",
      "Creating query embedding and retrieving chunks...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "=== FINAL RESPONSE ===\n",
      "Explainable AI (XAI) is a field of study aimed at making AI systems more transparent and understandable. The goal of XAI is to develop methods that allow users to explain how AI systems arrive at their decisions, which helps in building trust and accountability. This is important because many AI systems, particularly deep learning models, are often \"black boxes,\" meaning it is difficult to understand how they make decisions. By enhancing transparency and explainability, XAI helps users assess the reliability and fairness of AI systems, which is crucial for ensuring ethical behavior and responsible use of AI. Additionally, XAI contributes to addressing concerns about bias, privacy, and the potential for unintended consequences by providing insights into the decision-making processes of AI systems.\n",
      "\n",
      "=== COMPARING RESULTS ===\n",
      "Evaluating responses against reference answer...\n",
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "Let's analyze the two responses against the reference answer and evaluate them based on the three criteria provided:\n",
      "\n",
      "### 1. **More accurate and comprehensive**\n",
      "\n",
      "- **Standard Retrieval Response**: This response is highly accurate and comprehensive. It closely mirrors the reference answer, providing a clear definition of Explainable AI (XAI) and emphasizing its importance in terms of trust, accountability, and fairness. Additionally, it expands on the \"black box\" nature of deep learning models, which is an important concept in understanding why XAI is necessary. The mention of ethical behavior, bias, privacy, and unintended consequences adds depth to the explanation without deviating from the core topic.\n",
      "\n",
      "- **Relevant Segment Extraction Response**: This response is also accurate but slightly less comprehensive compared to the Standard Retrieval Response. While it covers the key points about transparency, trust, accountability, and fairness, it lacks some of the additional context provided by the Standard Retrieval Response, such as the discussion of bias, privacy, and unintended consequences. However, it still provides a solid explanation of XAI and its importance.\n",
      "\n",
      "**Conclusion for #1**: The **Standard Retrieval Response** is more accurate and comprehensive because it includes more detailed information about the challenges of AI systems (e.g., bias, privacy) and the broader implications of XAI.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Better at addressing the user's query**\n",
      "\n",
      "- **Standard Retrieval Response**: This response directly addresses the user's query by defining Explainable AI and explaining why it is important. It goes further by elaborating on the challenges posed by \"black box\" models and how XAI helps mitigate these issues. The additional details about bias, privacy, and unintended consequences are relevant and enhance the user's understanding of the importance of XAI.\n",
      "\n",
      "- **Relevant Segment Extraction Response**: This response also effectively addresses the user's query by defining XAI and discussing its importance in terms of trust, accountability, and fairness. However, it does not go as far as the Standard Retrieval Response in providing deeper insights into the challenges of AI systems and the broader implications of XAI.\n",
      "\n",
      "**Conclusion for #2**: The **Standard Retrieval Response** is better at addressing the user's query because it provides a more detailed and nuanced explanation of why XAI is considered important, including additional relevant aspects like bias and privacy.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Less likely to include irrelevant information**\n",
      "\n",
      "- **Standard Retrieval Response**: While this response is thorough, it introduces concepts like bias, privacy, and unintended consequences, which might be seen as tangential to the core question. However, these concepts are directly related to the importance of XAI, so they can be considered relevant rather than irrelevant. The inclusion of these points enhances the explanation without detracting from the main topic.\n",
      "\n",
      "- **Relevant Segment Extraction Response**: This response focuses strictly on the core aspects of XAI—transparency, trust, accountability, and fairness—without introducing additional topics. As a result, it is less likely to include information that could be perceived as irrelevant.\n",
      "\n",
      "**Conclusion for #3**: The **Relevant Segment Extraction Response** is less likely to include irrelevant information because it stays closer to the core definition and importance of XAI without expanding into related but potentially tangential topics.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Evaluation\n",
      "\n",
      "- **Most accurate and comprehensive**: **Standard Retrieval Response**\n",
      "- **Better at addressing the user's query**: **Standard Retrieval Response**\n",
      "- **Less likely to include irrelevant information**: **Relevant Segment Extraction Response**\n",
      "\n",
      "The **Standard Retrieval Response** excels in accuracy, comprehensiveness, and addressing the user's query, while the **Relevant Segment Extraction Response** is more concise and focused on avoiding irrelevant information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "query = data[0]['question']\n",
    "\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "results = evaluate_methods(pdf_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
