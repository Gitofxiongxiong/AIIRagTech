{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 中的上下文丰富检索（Context-Enriched Retrieval）\n",
    "\n",
    "检索增强生成 (RAG) 通过从外部来源检索相关知识来增强 AI 的回复。传统的检索方法返回的是孤立的文本块，这可能导致答案不够完整。\n",
    "\n",
    "为了解决这个问题，出现了**上下文丰富检索**方法，该方法确保检索到的信息包含相邻的文本块，以获得更好的连贯性。\n",
    "\n",
    "**具体步骤：**\n",
    "\n",
    "*   **数据提取 (Data Ingestion)**：从 PDF 文件中提取文本。\n",
    "*   **带重叠上下文的分块 (Chunking with Overlapping Context)**：将文本分割成相互重叠的文本块，以保留上下文信息。\n",
    "*   **嵌入向量创建 (Embedding Creation)**：将文本块转换为数值表示形式。\n",
    "*   **上下文感知检索 (Context-Aware Retrieval)**：检索相关文本块及其相邻的文本块，以提升信息的完整性。\n",
    "*   **回复生成 (Response Generation)**：使用语言模型根据检索到的上下文生成回复。\n",
    "*   **评估 (Evaluation)**：评估模型回复的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从pdf提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    提取PDF文件中的文本并打印前`num_chars`个字符。\n",
    "\n",
    "    参数：\n",
    "    pdf_path (str): PDF文件的路径。\n",
    "\n",
    "    返回：\n",
    "    str: 从PDF中提取的文本。\n",
    "\n",
    "    \"\"\"\n",
    "    # 打开PDF文件\n",
    "    mypdf = pymupdf.open(pdf_path)\n",
    "    all_text = \"\"  # 初始化一个空字符串来存储提取的文本\n",
    "\n",
    "    # 迭代PDF中的每个页面\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 获取页面\n",
    "        text = page.get_text(\"text\")  # 从页面中提取文本\n",
    "        all_text += text  # 将提取的文本附加到all_text字符串\n",
    "\n",
    "    return all_text  # 返回提取的文本\n",
    "\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    将文本分割成n个部分，每个部分之间有overlap个字符的重叠。\n",
    "    参数：\n",
    "    text (str): 要分割的文本\n",
    "    n (int): 每个部分的字符数\n",
    "    overlap (int): 每个部分之间的重叠字符数\n",
    "\n",
    "    返回：\n",
    "    list: 包含分割后的文本部分的列表\n",
    "    \"\"\"\n",
    "    chunks = []  \n",
    "    \n",
    "    \n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        \n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 42\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the development and deployment of AI. \n",
      "Historical Context \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
      "and symbolic methods. The 1980s saw a rise in exp\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 百炼服务的base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1, size: 10\n",
      "Processing batch 2, size: 10\n",
      "Processing batch 3, size: 10\n",
      "Processing batch 4, size: 10\n",
      "Processing batch 5, size: 2\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings_in_batches(text_chunks, model=\"text-embedding-v3\", batch_size_limit=10): # 我改成了官方模型名，你可以换回 \"text-embedding-v3\"\n",
    "    \"\"\"\n",
    "    调用 OpenAI 的 Embedding API 来创建文本列表的嵌入向量，处理批处理大小限制。\n",
    "\n",
    "    参数:\n",
    "    text_chunks (List[str]): 需要创建嵌入的文本字符串列表。\n",
    "    model (str): 使用的嵌入模型。\n",
    "    batch_size_limit (int): API 允许的最大批处理大小。根据错误信息，这里是10。\n",
    "\n",
    "    返回:\n",
    "    List[List[float]]: 所有文本的嵌入向量列表。\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    if not text_chunks:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(text_chunks, list): # 确保输入是列表\n",
    "        text_chunks = [text_chunks]\n",
    "\n",
    "    for i in range(0, len(text_chunks), batch_size_limit):\n",
    "        batch = text_chunks[i:i + batch_size_limit]\n",
    "        try:\n",
    "            print(f\"Processing batch {i//batch_size_limit + 1}, size: {len(batch)}\")\n",
    "            response = client.embeddings.create(\n",
    "                input=batch,\n",
    "                model=model,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "            # 从响应中提取该批次的嵌入向量\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting with chunk: '{batch[0][:50]}...'\")\n",
    "            print(f\"API Error: {e}\")\n",
    "\n",
    "            raise e \n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "response = create_embeddings_in_batches(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度。\n",
    "\n",
    "    参数:\n",
    "    vec1 (numpy.ndarray): 第一个向量。\n",
    "    vec2 (numpy.ndarray): 第二个向量。\n",
    "\n",
    "    返回:\n",
    "    float: 余弦相似度。\n",
    "\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上下文感知的检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_enriched_search(query, text_chunks, embeddings, k=1, context_size=1,model=\"text-embedding-v3\"):\n",
    "    \"\"\"\n",
    "    检索最相关的文本块，同时考虑上下文信息。\n",
    "    参数：\n",
    "        query (str): 用户的查询。\n",
    "        text_chunks (list): 文本块列表。\n",
    "        embeddings (list): 文本块的嵌入列表。\n",
    "        k (int): 要返回的最相关文本块的数量。\n",
    "        context_size (int): 要考虑的上下文大小。\n",
    "    返回：\n",
    "        list: 最相关的文本块列表。\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "                input=query,\n",
    "                model=model,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "    query_embedding = response.data[0].embedding\n",
    "    similarity_scores = []\n",
    "\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding))\n",
    "\n",
    "        similarity_scores.append((i, similarity_score))\n",
    "\n",
    "\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    top_index = similarity_scores[0][0]\n",
    "\n",
    "    # Define the range for context inclusion\n",
    "\n",
    "    start = max(0, top_index - context_size)\n",
    "    end = min(len(text_chunks), top_index + context_size + 1)\n",
    "\n",
    "\n",
    "    return [text_chunks[i] for i in range(start, end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检索一条请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Context 1:\n",
      "nt aligns with societal values. Education and awareness campaigns inform the public \n",
      "about AI, its impacts, and its potential. \n",
      "Chapter 19: AI and Ethics \n",
      "Principles of Ethical AI \n",
      "Ethical AI principles guide the development and deployment of AI systems to ensure they are fair, \n",
      "transparent, accountable, and beneficial to society. Key principles include respect for human \n",
      "rights, privacy, non-discrimination, and beneficence. \n",
      " \n",
      " \n",
      "Addressing Bias in AI \n",
      "AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair \n",
      "or discriminatory outcomes. Addressing bias requires careful data collection, algorithm design, \n",
      "and ongoing monitoring and evaluation. \n",
      "Transparency and Explainability \n",
      "Transparency and explainability are essential for building trust in AI systems. Explainable AI (XAI) \n",
      "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
      "fairness and accuracy. \n",
      "Privacy and Data Protection \n",
      "AI systems often rely on la\n",
      "=====================================\n",
      "Context 2:\n",
      "systems. Explainable AI (XAI) \n",
      "techniques aim to make AI decisions more understandable, enabling users to assess their \n",
      "fairness and accuracy. \n",
      "Privacy and Data Protection \n",
      "AI systems often rely on large amounts of data, raising concerns about privacy and data \n",
      "protection. Ensuring responsible data handling, implementing privacy-preserving techniques, \n",
      "and complying with data protection regulations are crucial. \n",
      "Accountability and Responsibility \n",
      "Establishing accountability and responsibility for AI systems is essential for addressing potential \n",
      "harms and ensuring ethical behavior. This includes defining roles and responsibilities for \n",
      "developers, deployers, and users of AI systems. \n",
      "Chapter 20: Building Trust in AI \n",
      "Transparency and Explainability \n",
      "Transparency and explainability are key to building trust in AI. Making AI systems understandable \n",
      "and providing insights into their decision-making processes helps users assess their reliability \n",
      "and fairness. \n",
      "Robustness and Reliability \n",
      "\n",
      "=====================================\n",
      "Context 3:\n",
      "to building trust in AI. Making AI systems understandable \n",
      "and providing insights into their decision-making processes helps users assess their reliability \n",
      "and fairness. \n",
      "Robustness and Reliability \n",
      "Ensuring that AI systems are robust and reliable is essential for building trust. This includes \n",
      "testing and validating AI models, monitoring their performance, and addressing potential \n",
      "vulnerabilities. \n",
      "User Control and Agency \n",
      "Empowering users with control over AI systems and providing them with agency in their \n",
      "interactions with AI enhances trust. This includes allowing users to customize AI settings, \n",
      "understand how their data is used, and opt out of AI-driven features. \n",
      "Ethical Design and Development \n",
      "Incorporating ethical considerations into the design and development of AI systems is crucial for \n",
      "building trust. This includes conducting ethical impact assessments, engaging stakeholders, and \n",
      "adhering to ethical guidelines and standards. \n",
      "Public Engagement and Education \n",
      "Engaging th\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "query = data[0]['question']\n",
    "\n",
    "top_chunks = context_enriched_search(query, text_chunks, response, k=1, context_size=1)\n",
    "\n",
    "\n",
    "print(\"Query:\", query)\n",
    "\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于检索生成回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainable AI (XAI) refers to techniques that aim to make AI decisions more understandable to users, enabling them to assess the fairness and accuracy of these decisions. It is considered important because transparency and explainability are key to building trust in AI systems. By making AI systems understandable and providing insights into their decision-making processes, users can better evaluate the reliability and fairness of AI, which is essential for its acceptance and ethical use.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_prompt, model=\"qwen3-4b\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        extra_body={\"enable_thinking\": False}\n",
    "    \n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)\n",
    "print(ai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the evaluation system\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
    "\n",
    "# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# Generate the evaluation response using the evaluation system prompt and evaluation prompt\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Print the evaluation response\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
