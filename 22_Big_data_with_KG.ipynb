{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-0",
   "metadata": {},
   "source": [
    "## 端到端流水线：大数据与知识图谱（基于书籍参考）\n",
    "\n",
    "### 目标：\n",
    "将关于科技公司收购的新闻文章转换为结构化的知识图谱，使用现代技术进行提取、精炼和推理——遵循概念性书籍中概述的基础原则。\n",
    "\n",
    "### 数据集：CNN/DailyMail\n",
    "\n",
    "### 方法概述：\n",
    "本笔记本演示了一个多阶段过程：\n",
    "1.  **数据获取与准备：** 获取和清理原始新闻文本。\n",
    "2.  **信息提取：** 识别关键实体（组织、人员、金钱、日期）以及它们之间的关系（例如，'收购'、'投资于'）。\n",
    "3.  **知识图谱构建：** 将提取的信息结构化为RDF三元组，形成我们知识图谱的节点和边。\n",
    "4.  **知识图谱精炼（概念性）：** 使用嵌入来表示知识图谱组件，并从概念上探索链接预测。\n",
    "5.  **持久化与利用：** 存储、查询（SPARQL）和可视化知识图谱。\n",
    "\n",
    "我们将利用大型语言模型（LLMs）处理复杂的自然语言处理任务，如细致的实体和关系提取，同时也使用传统库如spaCy进行初步探索，使用`rdflib`进行知识图谱管理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b29f8",
   "metadata": {},
   "source": [
    "# 目录\n",
    "\n",
    "- [端到端流水线：大数据与知识图谱（基于书籍参考）](#intro-0)\n",
    "  - [初始设置：导入和配置](#intro-setup)\n",
    "    - [初始化LLM客户端和spaCy模型](#llm-spacy-init-desc)\n",
    "    - [定义RDF命名空间](#namespace-init-desc)\n",
    "- [阶段1：数据获取和准备](#phase1)\n",
    "  - [步骤1.1：数据获取](#step1-1-desc)\n",
    "    - [执行数据获取](#data-acquisition-exec-desc)\n",
    "  - [步骤1.2：数据清理与预处理](#step1-2-desc)\n",
    "    - [执行数据清理](#data-cleaning-exec-desc)\n",
    "- [阶段2：信息提取](#phase2)\n",
    "  - [步骤2.1：实体提取（命名实体识别 - NER）](#step2-1-desc)\n",
    "    - [2.1.1：使用spaCy进行实体探索 - 函数定义](#step2-1-1-spacy-desc)\n",
    "    - [2.1.1：使用spaCy进行实体探索 - 绘图函数定义](#plot_entity_distribution_func_def_desc)\n",
    "    - [2.1.1：使用spaCy进行实体探索 - 执行](#spacy-ner-exec-desc)\n",
    "    - [通用LLM调用函数定义](#llm-call-func-def-desc)\n",
    "    - [2.1.2：使用LLM进行实体类型选择 - 执行](#step2-1-2-llm-type-selection-desc)\n",
    "    - [LLM JSON输出解析函数定义](#parse_llm_json_func_def_desc)\n",
    "    - [2.1.3：使用LLM进行目标实体提取 - 执行](#step2-1-3-llm-ner-exec-desc)\n",
    "  - [步骤2.2：关系提取](#step2-2-desc)\n",
    "- [阶段3：知识图谱构建](#phase3)\n",
    "  - [步骤3.1：实体消歧与链接（简化） - 标准化函数](#step3-1-normalize-entity-text-func-def-desc)\n",
    "    - [执行实体标准化和URI生成](#entity-normalization-exec-desc)\n",
    "  - [步骤3.2：模式/本体对齐 - RDF类型映射函数](#step3-2-rdf-type-func-def-desc)\n",
    "    - [模式/本体对齐 - RDF谓词映射函数](#step3-2-rdf-predicate-func-def-desc)\n",
    "    - [模式/本体对齐 - 示例](#schema-alignment-example-desc)\n",
    "  - [步骤3.3：三元组生成](#step3-3-triple-generation-exec-desc)\n",
    "- [阶段4：使用嵌入进行知识图谱精炼](#phase4)\n",
    "  - [步骤4.1：生成知识图谱嵌入 - 嵌入函数定义](#step4-1-embedding-func-def-desc)\n",
    "    - [生成知识图谱嵌入 - 执行](#kg-embedding-exec-desc)\n",
    "  - [步骤4.2：链接预测（知识发现 - 概念性） - 余弦相似度函数](#step4-2-cosine-sim-func-def-desc)\n",
    "    - [链接预测（概念性） - 相似度计算示例](#link-prediction-exec-desc)\n",
    "  - [步骤4.3：添加预测链接（可选且概念性） - 函数定义](#step4-3-add-inferred-func-def-desc)\n",
    "    - [添加预测链接（概念性） - 执行示例](#add-predicted-links-exec-desc)\n",
    "- [阶段5：持久化和利用](#phase5)\n",
    "  - [步骤5.1：知识图谱存储 - 保存函数定义](#step5-1-save-graph-func-def-desc)\n",
    "    - [知识图谱存储 - 执行](#kg-storage-exec-desc)\n",
    "  - [步骤5.2：查询和分析 - SPARQL执行函数](#step5-2-sparql-func-def-desc)\n",
    "    - [SPARQL查询和分析 - 执行示例](#sparql-querying-exec-desc)\n",
    "  - [步骤5.3：可视化（可选） - 可视化函数定义](#step5-3-viz-func-def-desc)\n",
    "    - [知识图谱可视化 - 执行](#visualization-exec-desc)\n",
    "- [结论和未来工作](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-setup",
   "metadata": {},
   "source": [
    "### 初始设置：导入和配置\n",
    "\n",
    "**理论：**\n",
    "在任何数据处理或分析开始之前，我们需要设置环境。这包括：\n",
    "*   **导入库：** 引入必要的Python包。这些包括用于数据加载的`datasets`、用于与LLMs交互的`openai`、用于基础NLP的`spacy`、用于知识图谱操作的`rdflib`、用于正则表达式文本处理的`re`、用于处理LLM输出的`json`、用于可视化的`matplotlib`和`pyvis`，以及标准库如`os`、`collections`和`tqdm`。\n",
    "*   **API配置：** 为外部服务设置凭据和端点，特别是Nebius LLM API。**安全注意：** 在生产环境中，API密钥绝不应硬编码。使用环境变量或安全的密钥管理系统。\n",
    "*   **模型初始化：** 加载预训练模型，如spaCy的`en_core_web_sm`用于基本NLP任务，并配置LLM客户端以使用部署在Nebius上的特定模型进行生成和嵌入。\n",
    "*   **命名空间定义：** 对于基于RDF的知识图谱，命名空间（如我们自定义术语的`EX`、schema.org的`SCHEMA`）对于为实体和属性创建唯一且可解析的URI至关重要。这符合链接数据原则。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "库已导入。\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# NLP和知识图谱库\n",
    "import spacy\n",
    "from rdflib import Graph, Literal, Namespace, URIRef\n",
    "from rdflib.namespace import RDF, RDFS, XSD, SKOS # 添加SKOS用于altLabel\n",
    "\n",
    "# 用于LLM的OpenAI客户端\n",
    "from openai import OpenAI\n",
    "\n",
    "# 可视化\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Hugging Face数据集库\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 用于嵌入相似度\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- API配置（重要：请替换为您的实际凭据和模型名称）---\n",
    "NEBIUS_API_KEY = os.getenv(\"NEBIUS_API_KEY\", \"your_nebius_api_key_here\") # 替换为您的实际API密钥\n",
    "NEBIUS_BASE_URL = \"https://api.studio.nebius.com/v1/\"\n",
    "\n",
    "# --- 模型名称（重要：请替换为您部署的模型名称）---\n",
    "TEXT_GEN_MODEL_NAME = \"deepseek-ai/DeepSeek-V3\" # 例如，phi-4、deepseek或任何其他模型\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-multilingual-gemma2\" # 例如，text-embedding-ada-002、BAAI/bge-multilingual-gemma2或任何其他模型\n",
    "\n",
    "print(\"库已导入。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-code-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "此代码块简单确认必要的库已成功导入，没有错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-spacy-init-desc",
   "metadata": {},
   "source": [
    "#### 初始化LLM客户端和spaCy模型\n",
    "\n",
    "**理论：**\n",
    "在这里，我们实例化主要NLP工具的客户端：\n",
    "*   **OpenAI客户端：** 配置为指向Nebius API。此客户端将用于向部署的LLM发送请求，执行实体提取、关系提取和生成嵌入等任务。执行基本检查以查看配置参数是否已设置。\n",
    "*   **spaCy模型：** 我们加载`en_core_web_sm`，这是spaCy的一个小型英语模型。该模型提供了分词、词性标注、词形还原和基本命名实体识别（NER）的高效功能。它对初始文本探索很有用，可以补充基于LLM的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "llm-spacy-init-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI客户端已初始化，base_url: https://api.studio.nebius.com/v1/ 使用模型: deepseek-ai/DeepSeek-V3\n",
      "spaCy模型 'en_core_web_sm' 已加载。\n"
     ]
    }
   ],
   "source": [
    "client = None # 将客户端初始化为None\n",
    "if NEBIUS_API_KEY != \"YOUR_NEBIUS_API_KEY\" and NEBIUS_BASE_URL != \"YOUR_NEBIUS_BASE_URL\" and TEXT_GEN_MODEL_NAME != \"YOUR_TEXT_GENERATION_MODEL_NAME\":\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            base_url=NEBIUS_BASE_URL,\n",
    "            api_key=NEBIUS_API_KEY \n",
    "        )\n",
    "        print(f\"OpenAI客户端已初始化，base_url: {NEBIUS_BASE_URL} 使用模型: {TEXT_GEN_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"初始化OpenAI客户端时出错: {e}\")\n",
    "        client = None # 如果初始化失败，确保客户端为None\n",
    "else:\n",
    "    print(\"警告：OpenAI客户端未完全配置。LLM功能将被禁用。请设置NEBIUS_API_KEY、NEBIUS_BASE_URL和TEXT_GEN_MODEL_NAME。\")\n",
    "\n",
    "nlp_spacy = None # 将nlp_spacy初始化为None\n",
    "try:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy模型 'en_core_web_sm' 已加载。\")\n",
    "except OSError:\n",
    "    print(\"未找到spaCy模型 'en_core_web_sm'。正在下载...（这可能需要一些时间）\")\n",
    "    try:\n",
    "        spacy.cli.download(\"en_core_web_sm\")\n",
    "        nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"spaCy模型 'en_core_web_sm' 下载并加载成功。\")\n",
    "    except Exception as e:\n",
    "        print(f\"下载或加载spaCy模型失败: {e}\")\n",
    "        print(\"请在终端中尝试: python -m spacy download en_core_web_sm 并重启内核。\")\n",
    "        nlp_spacy = None # 如果加载失败，确保nlp_spacy为None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-spacy-init-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "此代码块打印消息，指示OpenAI客户端和spaCy模型初始化的状态。如果配置缺失或模型无法加载，将显示警告。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "namespace-init-desc",
   "metadata": {},
   "source": [
    "#### 定义RDF命名空间\n",
    "\n",
    "**理论：**\n",
    "在RDF中，命名空间用于避免命名冲突并为术语（URI）提供上下文。\n",
    "*   `EX`：我们项目特定术语的自定义命名空间（例如，如果未映射到标准本体，则为我们的实体和关系）。\n",
    "*   `SCHEMA`：指向Schema.org，这是互联网上结构化数据的广泛使用词汇表。我们将尝试将一些提取的类型映射到Schema.org术语，以获得更好的互操作性。\n",
    "*   `RDFS`：RDF Schema，提供描述RDF词汇表的基本词汇（例如，`rdfs:label`、`rdfs:Class`）。\n",
    "*   `RDF`：核心RDF词汇表（例如，`rdf:type`）。\n",
    "*   `XSD`：XML Schema数据类型，用于指定字面数据类型（例如，`xsd:string`、`xsd:date`）。\n",
    "*   `SKOS`：简单知识组织系统，对词典、分类法和受控词汇表有用（例如，`skos:altLabel`用于替代名称）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "namespace-init-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自定义命名空间EX定义为: http://example.org/kg/\n",
      "Schema.org命名空间SCHEMA定义为: http://schema.org/\n"
     ]
    }
   ],
   "source": [
    "EX = Namespace(\"http://example.org/kg/\")\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "\n",
    "print(f\"自定义命名空间EX定义为: {EX}\")\n",
    "print(f\"Schema.org命名空间SCHEMA定义为: {SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "namespace-init-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "这确认了我们主要自定义命名空间（`EX`）和来自Schema.org的`SCHEMA`命名空间的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1",
   "metadata": {},
   "source": [
    "## 阶段1：数据获取和准备\n",
    "**(参考：第1章 – 大数据生态系统；第3章 – 大数据处理价值链)**\n",
    "\n",
    "**理论（阶段概述）：**\n",
    "这个初始阶段在任何数据驱动项目中都是至关重要的。它对应于大数据价值链的早期阶段：\\\"数据获取\\\"和\\\"数据准备/预处理\\\"的部分。目标是获取原始数据并将其转换为适合进一步处理和信息提取的状态。低质量的输入数据（\\\"垃圾进，垃圾出\\\"原则）将不可避免地导致低质量的知识图谱。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-1-desc",
   "metadata": {},
   "source": [
    "### 步骤1.1：数据获取\n",
    "**任务：** 收集新闻文章集合。\n",
    "\n",
    "**书籍概念：** （第1章，图1和图2；第3章 - 数据获取阶段）\n",
    "此步骤代表大数据生态系统的\\\"数据源\\\"和\\\"摄取\\\"组件。我们正在利用现有数据集（通过Hugging Face `datasets`的CNN/DailyMail），而不是抓取实时新闻，但原理是相同的：将外部数据引入我们的处理流水线。\"\n",
    "\n",
    "**方法论：**\n",
    "我们将定义一个函数`acquire_articles`来加载CNN/DailyMail数据集。为了管理此演示的处理时间和成本，并专注于潜在相关的文章，此函数将：\n",
    "1.  加载数据集的指定分割（例如，'train'）。\n",
    "2.  可选地基于关键词列表过滤文章。对于我们的科技公司收购目标，像\"acquire\"、\"merger\"、\"technology\"、\"startup\"这样的关键词是相关的。这是一个简单的启发式方法；在更大的数据集上，可以使用更高级的主题建模或分类来获得更好的过滤效果。\n",
    "3.  取（过滤后的）文章的小样本。\n",
    "\n",
    "**输出：** 原始文章数据结构列表（通常是包含'id'、'article'文本等的字典）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acquire_articles_func_def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 'acquire_articles' 已定义。\n"
     ]
    }
   ],
   "source": [
    "def acquire_articles(dataset_name=\"cnn_dailymail\", version=\"3.0.0\", split='train', sample_size=1000, keyword_filter=None):\n",
    "    \"\"\"从指定的Hugging Face数据集加载文章，可选地过滤它们，并取样本。\"\"\"\n",
    "    print(f\"尝试加载数据集: {dataset_name} (版本: {version}, 分割: '{split}')...\")\n",
    "    try:\n",
    "        full_dataset = load_dataset(dataset_name, version, split=split, streaming=False) # 对较小数据集使用streaming=False以便于切片\n",
    "        print(f\"成功加载数据集。分割中的总记录数: {len(full_dataset)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载数据集 {dataset_name} 时出错: {e}\")\n",
    "        print(\"请确保数据集可用或您有互联网连接。\")\n",
    "        return [] # 失败时返回空列表\n",
    "    \n",
    "    raw_articles_list = []\n",
    "    if keyword_filter:\n",
    "        print(f\"过滤包含任何关键词的文章: {keyword_filter}...\")\n",
    "        # 这是一个简单的关键词搜索。对于非常大的数据集，这可能很慢。\n",
    "        # 如果不是流式处理，考虑使用Hugging Face数据集的.filter()方法以提高效率。\n",
    "        count = 0\n",
    "        # 为了避免在数据集很大而我们只需要过滤后的小样本时遍历整个数据集：\n",
    "        # 我们将迭代到某个点或直到我们有足够的过滤文章。\n",
    "        # 这是在潜在大数据集上平衡过滤与性能的启发式方法。\n",
    "        iteration_limit = min(len(full_dataset), sample_size * 20) # 最多查看sample_size * 20篇文章\n",
    "        for i in tqdm(range(iteration_limit), desc=\"过滤文章\"):\n",
    "            record = full_dataset[i]\n",
    "            if any(keyword.lower() in record['article'].lower() for keyword in keyword_filter):\n",
    "                raw_articles_list.append(record)\n",
    "                count += 1\n",
    "            if count >= sample_size:\n",
    "                print(f\"在检查的{i+1}条记录中找到{sample_size}篇符合过滤条件的文章。\")\n",
    "                break\n",
    "        if not raw_articles_list:\n",
    "            print(f\"警告：在前{iteration_limit}条记录中未找到包含关键词{keyword_filter}的文章。返回空列表。\")\n",
    "            return []\n",
    "        # 如果我们找到了文章但少于sample_size，我们取找到的。\n",
    "        # 如果我们找到了更多，我们仍然只取sample_size。\n",
    "        raw_articles_list = raw_articles_list[:sample_size]\n",
    "    else:\n",
    "        print(f\"取前{sample_size}篇文章，不进行关键词过滤。\")\n",
    "        # 确保sample_size不超过数据集长度\n",
    "        actual_sample_size = min(sample_size, len(full_dataset))\n",
    "        raw_articles_list = list(full_dataset.select(range(actual_sample_size)))\n",
    "        \n",
    "    print(f\"获取了{len(raw_articles_list)}篇文章。\")\n",
    "    return raw_articles_list\n",
    "\n",
    "print(\"函数 'acquire_articles' 已定义。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquire_articles_func_def_output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "此单元格定义了`acquire_articles`函数。一旦函数在Python解释器的内存中定义，它将打印确认信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-acquisition-exec-desc",
   "metadata": {},
   "source": [
    "#### 执行数据获取\n",
    "\n",
    "**理论：**\n",
    "现在我们调用`acquire_articles`函数。我们定义与我们目标（科技公司收购）相关的关键词来指导过滤过程。设置`SAMPLE_SIZE`以保持数据量在此演示中可管理。较小的样本允许更快的迭代，特别是在使用可能有相关成本和延迟的LLMs时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "data-acquisition-exec-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尝试加载数据集: cnn_dailymail (版本: 3.0.0, 分割: 'train')...\n",
      "成功加载数据集。分割中的总记录数: 287113\n",
      "过滤包含任何关键词的文章: ['acquire', 'acquisition', 'merger', 'buyout', 'purchased by', 'acquired by', 'takeover']...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09b9f3b82bc4649af79f8216c3b5034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "过滤文章:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取了3篇文章。\n",
      "\n",
      "原始获取文章示例 (ID: 56d7d67bb0fc32ee71cc006b915244776d883661):\n",
      "圣地亚哥，加利福尼亚（CNN）-- 你必须知道真正推动移民辩论的是什么。这是文化，愚蠢。鲁本·纳瓦雷特Jr.：一些移民反对者，甚至是合法移民，担心当地文化的变化。移民限制主义者——我指的是那些想要限制所有移民，甚至是合法移民的人——喜欢假装他们如此高尚。然而他们无法控制自己。他们总是走低路，回到迎接早期移民浪潮的本土主义...\n",
      "\n",
      "记录中的字段数: 3\n",
      "字段: ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "# 定义与科技公司收购相关的关键词\n",
    "ACQUISITION_KEYWORDS = [\"acquire\", \"acquisition\", \"merger\", \"buyout\", \"purchased by\", \"acquired by\", \"takeover\"]\n",
    "TECH_KEYWORDS = [\"technology\", \"software\", \"startup\", \"app\", \"platform\", \"digital\", \"AI\", \"cloud\"]\n",
    "\n",
    "# 对于此演示，我们将主要按收购相关术语过滤。\n",
    "# '技术'方面将通过在实体/关系提取期间向LLM的提示来加强。\n",
    "FILTER_KEYWORDS = ACQUISITION_KEYWORDS\n",
    "\n",
    "SAMPLE_SIZE = 10 # 在此演示笔记本中保持非常小以便快速LLM处理\n",
    "\n",
    "# 将raw_data_sample初始化为空列表\n",
    "raw_data_sample = [] \n",
    "raw_data_sample = acquire_articles(sample_size=SAMPLE_SIZE, keyword_filter=FILTER_KEYWORDS)\n",
    "\n",
    "if raw_data_sample: # 检查列表是否不为空\n",
    "    print(f\"\\n原始获取文章示例 (ID: {raw_data_sample[0]['id']}):\")\n",
    "    print(raw_data_sample[0]['article'][:500] + \"...\")\n",
    "    print(f\"\\n记录中的字段数: {len(raw_data_sample[0].keys())}\")\n",
    "    print(f\"字段: {list(raw_data_sample[0].keys())}\")\n",
    "else:\n",
    "    print(\"未获取到文章。涉及文章处理的后续步骤可能会被跳过或不产生输出。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-acquisition-exec-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "此代码块执行数据获取。它将打印：\n",
    "*   关于数据加载和过滤过程的消息。\n",
    "*   获取的文章数量。\n",
    "*   第一篇获取文章的片段及其可用字段，以验证过程并了解数据结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-2-desc",
   "metadata": {},
   "source": [
    "### 步骤1.2：数据清理与预处理\n",
    "**任务：** 执行基本文本标准化。\n",
    "\n",
    "**书籍概念：** （第3章 - 大数据的多样性挑战）\n",
    "来自新闻文章等来源的原始文本数据通常是混乱的。它可能包含HTML标签、样板内容（如署名、版权声明）、特殊字符和不一致的格式。此步骤与解决大数据的\\\"多样性\\\"（在某种程度上还有\\\"准确性\\\"）挑战相对应。清洁、标准化的输入对于有效的下游NLP任务至关重要，因为噪声会显著降低实体识别器和关系提取器的性能。\\n",
    "\n",
    "**方法论：**\n",
    "我们将定义一个函数`clean_article_text`，使用正则表达式（`re`模块）来：\n",
    "*   删除常见的新闻样板（例如，\"(CNN) --\"，特定的署名模式）。\n",
    "*   删除HTML标签和URL。\n",
    "*   标准化空白（例如，用单个空格替换多个空格/换行符）。\n",
    "*   可选地，处理引号或其他特殊字符，如果不仔细处理可能会干扰LLM处理或JSON格式化。\n",
    "\n",
    "**输出：** 字典列表，其中每个字典包含文章ID及其清理后的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "clean_article_text_func_def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 'clean_article_text' 已定义。\n"
     ]
    }
   ],
   "source": [
    "def clean_article_text(raw_text):\n",
    "    \"\"\"使用正则表达式清理新闻文章的原始文本。\"\"\"\n",
    "    text = raw_text\n",
    "    \n",
    "    # 删除(CNN)样式前缀\n",
    "    text = re.sub(r'^\\(CNN\\)\\s*(--)?\\s*', '', text)\n",
    "    # 删除常见的署名和发布/更新行（模式可能需要根据特定数据集细节调整）\n",
    "    text = re.sub(r'By .*? for Dailymail\\.com.*?Published:.*?Updated:.*', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    text = re.sub(r'PUBLISHED:.*?BST,.*?UPDATED:.*?BST,.*', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    text = re.sub(r'Last updated at.*on.*', '', text, flags=re.IGNORECASE)\n",
    "    # 删除URL\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "    # 删除HTML标签\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 删除电子邮件地址\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
    "    # 标准化空白：用单个空格替换换行符、制表符，然后用单个空格替换多个空格\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # 可选：如果LLM有问题则转义引号，但通常对好的模型不需要\n",
    "    # text = text.replace('\"', \"\\\\\\\"\").replace(\"'\", \"\\\\'\") \n",
    "    return text\n",
    "\n",
    "print(\"函数 'clean_article_text' 已定义。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean_article_text_func_def_output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "确认将用于预处理文章内容的`clean_article_text`函数已定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-cleaning-exec-desc",
   "metadata": {},
   "source": [
    "#### 执行数据清理\n",
    "\n",
    "**理论：**\n",
    "此代码块遍历`raw_data_sample`（在前一步中获取）。对于每篇文章，它调用`clean_article_text`函数。清理后的文本，连同原始文章ID和可能的其他有用字段如'summary'（如果数据集中有'highlights'可用），存储在名为`cleaned_articles`的新列表中。这个新列表将是后续信息提取阶段的主要输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "data-cleaning-exec-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理3篇获取的文章...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aeade12fca241dfbf636992cddfdc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "清理文章:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理完成。清理后的文章总数: 3。\n",
      "\n",
      "清理后文章示例 (ID: 56d7d67bb0fc32ee71cc006b915244776d883661):\n",
      "圣地亚哥，加利福尼亚 你必须知道真正推动移民辩论的是什么。这是文化，愚蠢。鲁本·纳瓦雷特Jr.：一些移民反对者，甚至是合法移民，担心当地文化的变化。移民限制主义者——我指的是那些想要限制所有移民，甚至是合法移民的人——喜欢假装他们如此高尚。然而他们无法控制自己。他们总是走低路，回到迎接早期移民浪潮的本土主义...\n"
     ]
    }
   ],
   "source": [
    "cleaned_articles = [] # 初始化为空列表\n",
    "\n",
    "if raw_data_sample: # 仅在raw_data_sample不为空时继续\n",
    "    print(f\"清理{len(raw_data_sample)}篇获取的文章...\")\n",
    "    for record in tqdm(raw_data_sample, desc=\"清理文章\"):\n",
    "        cleaned_text_content = clean_article_text(record['article'])\n",
    "        cleaned_articles.append({\n",
    "            \"id\": record['id'],\n",
    "            \"original_text\": record['article'], # 保留原文以供参考\n",
    "            \"cleaned_text\": cleaned_text_content,\n",
    "            \"summary\": record.get('highlights', '') # CNN/DM有'highlights'，这些是摘要\n",
    "        })\n",
    "    print(f\"清理完成。清理后的文章总数: {len(cleaned_articles)}。\")\n",
    "    if cleaned_articles: # 处理后检查列表是否不为空\n",
    "        print(f\"\\n清理后文章示例 (ID: {cleaned_articles[0]['id']}):\")\n",
    "        print(cleaned_articles[0]['cleaned_text'][:500] + \"...\")\n",
    "else:\n",
    "    print(\"在前一步中未获取到原始文章，因此跳过清理。\")\n",
    "\n",
    "# 这确保cleaned_articles始终被定义，即使为空。\n",
    "if 'cleaned_articles' not in globals():\n",
    "    cleaned_articles = []\n",
    "    print(\"将'cleaned_articles'初始化为空列表，因为它之前未被创建。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-cleaning-exec-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "此代码块将：\n",
    "*   指示清理过程的开始和结束。\n",
    "*   显示清理的文章数量。\n",
    "*   显示第一篇清理后文章文本的片段，允许对清理效果进行视觉检查。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2",
   "metadata": {},
   "source": [
    "## 阶段2：信息提取\n",
    "**(参考：第2章 – 知识图谱基础；第4章 – 从结构化数据创建知识图谱)**\n",
    "\n",
    "**理论（阶段概述）：**\n",
    "信息提取（IE）是从非结构化或半结构化来源（如我们的新闻文章）自动提取结构化信息的过程。在知识图谱创建的背景下，信息提取至关重要，因为它识别基本构建块：实体（节点）和连接它们的关系（边）。此阶段直接解决如何将原始文本转换为更结构化的格式，这是知识图谱物化（第4章）之前的关键步骤。它涉及命名实体识别（NER）和关系提取（RE）等任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-1-desc",
   "metadata": {},
   "source": [
    "### 步骤2.1：实体提取（命名实体识别 - NER）\n",
    "**任务：** 识别命名实体，如组织、人员、产品、货币数字和日期。\n",
    "\n",
    "**书籍概念：** （第2章 - 实体作为节点）\n",
    "命名实体是现实世界的对象，如人员、地点、组织、产品等，可以用专有名称表示。在知识图谱中，这些实体成为*节点*。准确的命名实体识别是构建有意义图谱的基础。\n",
    "\n",
    "**方法论：**\n",
    "我们将采用双重方法：\n",
    "1.  **使用spaCy进行探索性NER：** 使用spaCy的预训练模型快速概览我们清理后文章中存在的常见实体类型。这有助于理解实体的一般情况。\n",
    "2.  **LLM驱动的实体类型选择：** 基于spaCy的输出和我们的特定目标（技术收购），我们将提示LLM建议一组最相关的重点实体类型。\n",
    "3.  **使用LLM进行目标NER：** 使用LLM和精炼的实体类型列表对文章执行NER，旨在为我们的特定领域获得更高的准确性和相关性。由于LLMs的上下文理解能力，特别是在精心制作的提示指导下，它们在这里可能很强大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-1-1-spacy-desc",
   "metadata": {},
   "source": [
    "#### 2.1.1：使用spaCy进行实体探索 - 函数定义\n",
    "\n",
    "**理论：**\n",
    "此函数`get_spacy_entity_counts`接受文章列表，使用spaCy的NER功能处理其文本样本，并返回一个计数器对象，统计不同实体标签（例如，`PERSON`、`ORG`、`GPE`）的频率。这为我们在使用更耗费资源的LLM之前理解数据集中普遍存在的实体类型提供了经验基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "get_spacy_entity_counts_func_def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 'get_spacy_entity_counts' 已定义。\n"
     ]
    }
   ],
   "source": [
    "def get_spacy_entity_counts(articles_data, text_field='cleaned_text', sample_size_spacy=50):\n",
    "    \"\"\"使用spaCy处理文章样本并计算实体标签。\"\"\"\n",
    "    if not nlp_spacy:\n",
    "        print(\"spaCy模型未加载。跳过spaCy实体计数。\")\n",
    "        return Counter()\n",
    "    if not articles_data:\n",
    "        print(\"未向spaCy提供文章数据进行实体计数。跳过。\")\n",
    "        return Counter()\n",
    "    \n",
    "    label_counter = Counter()\n",
    "    # 处理较小样本以进行快速spaCy分析\n",
    "    sample_to_process = articles_data[:min(len(articles_data), sample_size_spacy)]\n",
    "    \n",
    "    print(f\"使用spaCy处理{len(sample_to_process)}篇文章进行实体计数...\")\n",
    "    for article in tqdm(sample_to_process, desc=\"spaCy NER计数\"):\n",
    "        doc = nlp_spacy(article[text_field])\n",
    "        for ent in doc.ents:\n",
    "            label_counter[ent.label_] += 1\n",
    "    return label_counter\n",
    "\n",
    "print(\"函数 'get_spacy_entity_counts' 已定义。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "get_spacy_entity_counts_func_def_output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "确认`get_spacy_entity_counts`函数的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_entity_distribution_func_def_desc",
   "metadata": {},
   "source": [
    "#### 2.1.1：使用spaCy进行实体探索 - 绘图函数定义\n",
    "\n",
    "**理论：**\n",
    "`plot_entity_distribution`函数接受实体计数（来自`get_spacy_entity_counts`）并使用`matplotlib`生成条形图。可视化这种分布有助于快速识别最频繁的实体类型，这可以为后续关于优先考虑哪些类型用于知识图谱的决策提供信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "plot_entity_distribution_func_def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 'plot_entity_distribution' 已定义。\n"
     ]
    }
   ],
   "source": [
    "def plot_entity_distribution(label_counter_to_plot):\n",
    "    \"\"\"从Counter对象绘制实体标签的分布。\"\"\"\n",
    "    if not label_counter_to_plot:\n",
    "        print(\"没有实体计数可绘制。\")\n",
    "        return\n",
    "    \n",
    "    # 获取最常见的15个，如果少于15个则全部获取\n",
    "    top_items = label_counter_to_plot.most_common(min(15, len(label_counter_to_plot)))\n",
    "    if not top_items: # 处理计数器不为空但most_common(0)或类似边缘情况\n",
    "        print(\"实体计数中没有项目可绘制。\")\n",
    "        return\n",
    "        \n",
    "    labels, counts = zip(*top_items)\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.bar(labels, counts, color='skyblue')\n",
    "    plt.title(\"顶级实体类型分布（通过spaCy）\")\n",
    "    plt.ylabel(\"频率\")\n",
    "    plt.xlabel(\"实体标签\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout() # 调整布局以确保所有内容都适合\n",
    "    plt.show()\n",
    "\n",
    "print(\"函数 'plot_entity_distribution' 已定义。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_entity_distribution_func_def_output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "确认`plot_entity_distribution`函数的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spacy-ner-exec-desc",
   "metadata": {},
   "source": [
    "#### 2.1.1：使用spaCy进行实体探索 - 执行\n",
    "\n",
    "**理论：**\n",
    "此代码块执行基于spaCy的实体探索。它在`cleaned_articles`上调用`get_spacy_entity_counts`。然后打印结果计数并传递给`plot_entity_distribution`以可视化发现。如果没有可用的清理文章或spaCy模型加载失败，则跳过此步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spacy-ner-exec-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在3篇清理文章样本上运行spaCy NER...\n",
      "使用spaCy处理3篇文章进行实体计数...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0697c026abbd4932bca897921dae59fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spaCy NER计数:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy实体计数（来自样本）:\n",
      "  ORG: 57\n",
      "  GPE: 47\n",
      "  PERSON: 39\n",
      "  NORP: 28\n",
      "  CARDINAL: 27\n",
      "  DATE: 22\n",
      "  LOC: 7\n",
      "  LANGUAGE: 5\n",
      "  PRODUCT: 4\n",
      "  ORDINAL: 4\n",
      "  FAC: 3\n",
      "  TIME: 3\n",
      "  WORK_OF_ART: 2\n",
      "  LAW: 1\n",
      "  MONEY: 1\n",
      "  QUANTITY: 1\n"
     ]
    }
   ],
   "source": [
    "# 初始化entity_counts为空Counter\n",
    "entity_counts = Counter()\n",
    "\n",
    "if cleaned_articles and nlp_spacy: # 检查是否有清理的文章和spaCy模型\n",
    "    print(f\"在{len(cleaned_articles)}篇清理文章样本上运行spaCy NER...\")\n",
    "    entity_counts = get_spacy_entity_counts(cleaned_articles)\n",
    "    \n",
    "    if entity_counts:\n",
    "        print(\"\\nspaCy实体计数（来自样本）:\")\n",
    "        for label, count in entity_counts.most_common():\n",
    "            print(f\"  {label}: {count}\")\n",
    "        \n",
    "        # 绘制分布\n",
    "        plot_entity_distribution(entity_counts)\n",
    "    else:\n",
    "        print(\"spaCy未找到实体或处理失败。\")\n",
    "else:\n",
    "    if not cleaned_articles:\n",
    "        print(\"没有清理的文章可用于spaCy NER。跳过此步骤。\")\n",
    "    if not nlp_spacy:\n",
    "        print(\"spaCy模型不可用。跳过spaCy NER。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spacy-ner-exec-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "此代码块将打印：\n",
    "*   样本中spaCy找到的不同实体类型的频率。\n",
    "*   可视化此分布的条形图。\n",
    "如果不满足先决条件，它将打印一条消息，说明为什么跳过此步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-call-func-def-desc",
   "metadata": {},
   "source": [
    "#### 通用LLM调用函数定义\n",
    "\n",
    "**理论：**\n",
    "为了与LLM交互执行各种任务（实体类型选择、NER、关系提取），我们定义一个可重用的辅助函数`call_llm_for_response`。此函数封装了以下逻辑：\n",
    "*   接受系统提示（LLM的指令）和用户提示（特定输入/查询）。\n",
    "*   向配置的LLM端点进行API调用。\n",
    "*   从LLM的响应中提取文本内容。\n",
    "*   如果LLM客户端未初始化或API调用失败，进行基本错误处理。\n",
    "使用辅助函数促进代码重用性并使主逻辑更清洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "llm-call-func-def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 'call_llm_for_response' 已定义。\n"
     ]
    }
   ],
   "source": [
    "def call_llm_for_response(system_prompt, user_prompt, model_to_use=TEXT_GEN_MODEL_NAME, temperature=0.2):\n",
    "    \"\"\"调用LLM并获取响应的通用函数，具有基本错误处理。\"\"\"\n",
    "    if not client:\n",
    "        print(\"LLM客户端未初始化。跳过LLM调用。\")\n",
    "        return \"LLM_CLIENT_NOT_INITIALIZED\"\n",
    "    try:\n",
    "        print(f\"\\n调用LLM (模型: {model_to_use}, 温度: {temperature})...\")\n",
    "        # 用于调试，取消注释以查看提示（可能很长）\n",
    "        # print(f\"系统提示（前200个字符）: {system_prompt[:200]}...\")\n",
    "        # print(f\"用户提示（前200个字符）: {user_prompt[:200]}...\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model_to_use,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=temperature # 较低的温度用于更集中/确定性的输出\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(\"已收到LLM响应。\")\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"调用LLM时出错: {e}\")\n",
    "        return f\"LLM_ERROR: {str(e)}\"\n",
    "\n",
    "print(\"函数 'call_llm_for_response' 已定义。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-call-func-def-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "确认`call_llm_for_response`辅助函数的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-1-2-llm-type-selection-desc",
   "metadata": {},
   "source": [
    "#### 2.1.2：使用LLM进行实体类型选择 - 执行\n",
    "\n",
    "**理论：**\n",
    "虽然spaCy提供了一般的实体类型集合，但并非所有类型都与我们构建关于科技公司收购的知识图谱的特定目标相关。例如，`WORK_OF_ART`可能不如`ORG`（组织）或`MONEY`重要。\n",
    "在此步骤中，我们利用LLM的理解来精炼这个列表。\n",
    "1.  **系统提示：** 我们制作详细的系统提示，指示LLM充当科技新闻知识图谱构建专家。要求它从spaCy派生的列表中选择*最相关*的实体标签，专注于我们的领域，并为每个选择的类型提供解释。\n",
    "2.  **用户提示：** 用户提示包含从spaCy获得的实际实体标签列表及其频率。\n",
    "3.  **LLM调用：** 我们使用`call_llm_for_response`函数。\n",
    "LLM的输出应该是选择的实体类型及其描述的逗号分隔字符串（例如，`ORG (参与收购的组织，例如Google、Microsoft)`）。这个精选列表为我们后续基于LLM的NER形成了更有针对性的模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "llm-entity-type-selection-exec-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "调用LLM (模型: deepseek-ai/DeepSeek-V3, 温度: 0.2)...\n",
      "已收到LLM响应。\n",
      "\n",
      "LLM为科技收购知识图谱建议的实体类型：\n",
      "\n",
      "用于NER的最终实体类型列表: ORG (参与收购的公司，例如Google、Microsoft), PERSON (关键人物如CEO或创始人，例如Satya Nadella), DATE (收购公告或完成日期，例如2023年7月26日), MONEY (交易价值或投资，例如10亿美元), PRODUCT (涉及的技术产品或服务，例如云计算平台), GPE (与收购相关的地缘政治实体，例如美国、加利福尼亚), CARDINAL (与交易相关的数值，例如转移的员工数量)\n"
     ]
    }
   ],
   "source": [
    "ENTITY_TYPE_SELECTION_SYSTEM_PROMPT = (\n",
    "    \"你是专门从事科技新闻分析知识图谱构建的专家助手。\"\n",
    "    \"你将获得一个命名实体标签及其频率的列表，这些标签来自新闻文章。\"\n",
    "    \"你的任务是选择并返回一个逗号分隔的列表，包含构建专注于**科技公司收购**的知识图谱最相关的实体标签。\"\n",
    "    \"优先考虑组织（收购方、被收购方）、金融金额（交易价值）、日期（公告/完成）、关键人物（CEO、创始人）以及相关技术产品/服务或行业等标签。\"\n",
    "    \"对于你在输出列表中包含的每个实体标签，请提供简洁的括号解释或清晰的说明性示例。\"\n",
    "    \"示例：ORG (参与收购的公司，例如Google、Microsoft), MONEY (交易价值或投资，例如10亿美元), DATE (收购公告或完成日期，例如2023年7月26日)。\"\n",
    "    \"输出必须仅为标签及其括号解释的逗号分隔列表。\"\n",
    "    \"不要包含任何介绍性短语、问候语、摘要或此格式化列表之外的任何其他文本。\"\n",
    ")\n",
    "\n",
    "llm_selected_entity_types_str = \"\" # 初始化\n",
    "DEFAULT_ENTITY_TYPES_STR = \"ORG (收购或被收购公司，例如TechCorp), PERSON (关键高管，例如CEO), MONEY (收购价格，例如5亿美元), DATE (收购公告日期), PRODUCT (涉及的关键产品/服务), GPE (公司位置，例如硅谷)\"\n",
    "\n",
    "if entity_counts and client: # 如果我们有spaCy计数且LLM客户端可用则继续\n",
    "    # 从spaCy实体计数为提示创建字符串\n",
    "    spacy_labels_for_prompt = \", \".join([f\"{label} (频率: {count})\" for label, count in entity_counts.most_common()])\n",
    "    user_prompt_for_types = f\"从以下在新闻文章中找到的实体标签及其频率: [{spacy_labels_for_prompt}]。请根据指令选择并格式化关于科技公司收购知识图谱最相关的实体类型。\"\n",
    "    \n",
    "    llm_selected_entity_types_str = call_llm_for_response(ENTITY_TYPE_SELECTION_SYSTEM_PROMPT, user_prompt_for_types)\n",
    "    \n",
    "    if \"LLM_CLIENT_NOT_INITIALIZED\" in llm_selected_entity_types_str or \"LLM_ERROR\" in llm_selected_entity_types_str or not llm_selected_entity_types_str.strip():\n",
    "        print(\"\\nLLM实体类型选择失败或返回空。使用默认实体类型。\")\n",
    "        llm_selected_entity_types_str = DEFAULT_ENTITY_TYPES_STR\n",
    "    else:\n",
    "        print(\"\\nLLM为科技收购知识图谱建议的实体类型：\")\n",
    "        # 后处理以确保它是一个清洁的列表，如果LLM尽管有指令仍添加额外的措辞\n",
    "        # 这是一个简单的启发式方法，对于不太合规的LLM可能需要更强大的解析\n",
    "        if not re.match(r\"^([A-Z_]+ \\(.*?\\))(, [A-Z_]+ \\(.*?\\))*$\", llm_selected_entity_types_str.strip()):\n",
    "             print(f\"警告：LLM实体类型输出可能不符合预期的严格格式。原始: '{llm_selected_entity_types_str}'\")\n",
    "             # 尝试简单清理：取看起来像术语列表的最长行\n",
    "             lines = llm_selected_entity_types_str.strip().split('\\n')\n",
    "             best_line = \"\"\n",
    "             for line in lines:\n",
    "                 if '(' in line and ')' in line and len(line) > len(best_line):\n",
    "                     best_line = line\n",
    "             if best_line:\n",
    "                 llm_selected_entity_types_str = best_line\n",
    "                 print(f\"尝试清理: '{llm_selected_entity_types_str}'\")\n",
    "             else:\n",
    "                 print(\"清理失败，回退到默认实体类型。\")\n",
    "                 llm_selected_entity_types_str = DEFAULT_ENTITY_TYPES_STR\n",
    "else:\n",
    "    print(\"\\n跳过LLM实体类型选择（spaCy计数不可用或LLM客户端未初始化）。使用默认实体类型。\")\n",
    "    llm_selected_entity_types_str = DEFAULT_ENTITY_TYPES_STR\n",
    "\n",
    "print(f\"\\n用于NER的最终实体类型列表: {llm_selected_entity_types_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-entity-type-selection-exec-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "此代码块将打印：\n",
    "*   LLM建议的实体类型及其描述的逗号分隔列表（如果LLM调用失败/跳过则为默认列表）。\n",
    "*   此列表将指导下一步：目标命名实体识别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parse_llm_json_func_def_desc",
   "metadata": {},
   "source": [
    "#### LLM JSON输出解析函数定义\n",
    "\n",
    "**理论：**\n",
    "LLMs即使在提示特定格式如JSON时，有时也会产生包含额外文本、markdown格式（如` ```json ... ``` `）或与完美JSON略有偏差的输出。`parse_llm_json_output`函数是一个实用工具，用于稳健地将LLM的字符串输出解析为Python字典列表（表示实体或关系）。\n",
    "它尝试：\n",
    "1.  处理常见的markdown代码块语法。\n",
    "2.  使用`json.loads()`进行解析。\n",
    "3.  包含`JSONDecodeError`的错误处理，如果简单解析失败，提供基于正则表达式的提取等回退机制。\n",
    "此函数对于可靠地将LLM响应转换为可用的结构化数据至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parse_llm_json_func_def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 'parse_llm_json_output' 已定义。\n"
     ]
    }
   ],
   "source": [
    "def parse_llm_json_output(llm_output_str):\n",
    "    \"\"\"解析LLM的JSON输出，处理潜在的markdown代码块和常见问题。\"\"\"\n",
    "    if not llm_output_str or \"LLM_CLIENT_NOT_INITIALIZED\" in llm_output_str or \"LLM_ERROR\" in llm_output_str:\n",
    "        print(\"无法解析LLM输出：LLM未运行、出错或输出为空。\")\n",
    "        return [] # 返回空列表\n",
    "\n",
    "    # 尝试从markdown代码块中提取JSON\n",
    "    match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', llm_output_str, re.IGNORECASE)\n",
    "    if match:\n",
    "        json_str = match.group(1).strip()\n",
    "    else:\n",
    "        # 如果没有markdown块，假设整个字符串是JSON（或需要清理）\n",
    "        # LLMs有时在JSON列表之前添加介绍性文本。尝试找到列表的开始。\n",
    "        list_start_index = llm_output_str.find('[')\n",
    "        list_end_index = llm_output_str.rfind(']')\n",
    "        if list_start_index != -1 and list_end_index != -1 and list_start_index < list_end_index:\n",
    "            json_str = llm_output_str[list_start_index : list_end_index+1].strip()\n",
    "        else:\n",
    "            json_str = llm_output_str.strip() # 回退到整个字符串\n",
    "\n",
    "    try:\n",
    "        parsed_data = json.loads(json_str)\n",
    "        if isinstance(parsed_data, list):\n",
    "            return parsed_data\n",
    "        else:\n",
    "            print(f\"警告：LLM输出是有效JSON但不是列表（类型：{type(parsed_data)}）。返回空列表。\")\n",
    "            print(f\"有问题的JSON字符串（或其部分）：{json_str[:200]}...\")\n",
    "            return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"解码LLM输出的JSON时出错：{e}\")\n",
    "        print(f\"有问题的JSON字符串（或其部分）：{json_str[:500]}...\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"LLM JSON输出解析期间发生意外错误：{e}\")\n",
    "        return []\n",
    "\n",
    "print(\"函数 'parse_llm_json_output' 已定义。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parse_llm_json_func_def_output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "确认`parse_llm_json_output`实用函数的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-1-3-llm-ner-exec-desc",
   "metadata": {},
   "source": [
    "#### 2.1.3：使用LLM进行目标实体提取 - 执行\n",
    "\n",
    "**理论：**\n",
    "现在，配备了我们精选的实体类型列表（`llm_selected_entity_types_str`），我们指示LLM对每篇（清理后的）文章执行NER。\n",
    "1.  **系统提示：** NER的系统提示经过精心构建。它告诉LLM：\n",
    "    *   其角色（科技收购的专家NER系统）。\n",
    "    *   要关注的特定实体类型（来自`llm_selected_entity_types_str`）。\n",
    "    *   所需的输出格式：JSON对象列表，其中每个对象具有`\"text\"`（确切提取的实体跨度）和`\"type\"`（指定实体类型之一）。\n",
    "    *   所需JSON输出的示例。\n",
    "    *   如果未找到相关实体，则输出空JSON列表`[]`。\n",
    "2.  **用户提示：** 对于每篇文章，用户提示简单地是其`cleaned_text`。\n",
    "3.  **处理循环：** 我们遍历`cleaned_articles`的小样本（由`MAX_ARTICLES_FOR_LLM_NER`定义以管理时间/成本）。对于每篇：\n",
    "    *   文章文本（如果对LLM上下文窗口太长则可选截断）。\n",
    "    *   调用`call_llm_for_response`。\n",
    "    *   `parse_llm_json_output`处理LLM的响应。\n",
    "    *   提取的实体与文章数据一起存储在新列表`articles_with_entities`中。\n",
    "在API调用之间添加小延迟（`time.sleep`）以对API端点礼貌并避免潜在的速率限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "llm-ner-exec-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对3篇文章进行LLM NER...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d4cca6263341858b79d2b43219c17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM NER处理:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理文章ID: 56d7d67bb0fc32ee71cc006b915244776d883661 进行LLM NER (1/3)...\n",
      "\n",
      "调用LLM (模型: deepseek-ai/DeepSeek-V3, 温度: 0.2)...\n",
      "已收到LLM响应。\n",
      "  为文章ID 56d7d67bb0fc32ee71cc006b915244776d883661 提取了0个实体。\n",
      "\n",
      "处理文章ID: 4cf51ce9372dff8ff7f44f098eab1c1d7569af7a 进行LLM NER (2/3)...\n",
      "\n",
      "调用LLM (模型: deepseek-ai/DeepSeek-V3, 温度: 0.2)...\n",
      "已收到LLM响应。\n",
      "  为文章ID 4cf51ce9372dff8ff7f44f098eab1c1d7569af7a 提取了23个实体。\n",
      "  实体样本: [\n",
      "  {\n",
      "    \"text\": \"联合国\",\n",
      "    \"type\": \"ORG\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"阿尔及尔\",\n",
      "    \"type\": \"GPE\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"CNN\",\n",
      "    \"type\": \"ORG\"\n",
      "  }\n",
      "]\n",
      "\n",
      "处理文章ID: 82a0e1f034174079179821b052f33df76c781b47 进行LLM NER (3/3)...\n",
      "\n",
      "调用LLM (模型: deepseek-ai/DeepSeek-V3, 温度: 0.2)...\n",
      "已收到LLM响应。\n",
      "  为文章ID 82a0e1f034174079179821b052f33df76c781b47 提取了0个实体。\n",
      "\n",
      "LLM NER完成。处理了3篇文章并存储了实体。\n"
     ]
    }
   ],
   "source": [
    "LLM_NER_SYSTEM_PROMPT_TEMPLATE = (\n",
    "    \"你是专门识别**科技公司收购**信息的专家命名实体识别系统。\"\n",
    "    \"从提供的新闻文章文本中，识别并提取实体。\"\n",
    "    \"要关注的实体类型是：{entity_types_list_str}。\"\n",
    "    \"确保每个实体的提取'text'是文章中的确切跨度。\"\n",
    "    \"仅输出有效的JSON对象列表，其中每个对象具有'text'（确切提取的实体字符串）和'type'（实体类型主标签之一，例如来自您列表的ORG、PERSON、MONEY）键。\"\n",
    "    \"示例：[{{'text': '联合国', 'type': 'ORG'}}, {{'text': '巴拉克·奥巴马', 'type': 'PERSON'}}, {{'text': 'iPhone 15', 'type': 'PRODUCT'}}]\"\n",
    ")\n",
    "\n",
    "articles_with_entities = [] # 初始化\n",
    "MAX_ARTICLES_FOR_LLM_NER = 3 # 为此演示处理非常少的数量\n",
    "\n",
    "if cleaned_articles and client and llm_selected_entity_types_str and \"LLM_\" not in llm_selected_entity_types_str:\n",
    "    # 使用动态选择的实体类型准备系统提示\n",
    "    ner_system_prompt = LLM_NER_SYSTEM_PROMPT_TEMPLATE.format(entity_types_list_str=llm_selected_entity_types_str)\n",
    "    \n",
    "    # 确定要处理NER的文章数量\n",
    "    num_articles_to_process_ner = min(len(cleaned_articles), MAX_ARTICLES_FOR_LLM_NER)\n",
    "    print(f\"开始对{num_articles_to_process_ner}篇文章进行LLM NER...\")\n",
    "    \n",
    "    for i, article_dict in enumerate(tqdm(cleaned_articles[:num_articles_to_process_ner], desc=\"LLM NER处理\")):\n",
    "        print(f\"\\n处理文章ID: {article_dict['id']} 进行LLM NER ({i+1}/{num_articles_to_process_ner})...\")\n",
    "        \n",
    "        # 如果文章文本太长则截断（例如，对某些模型>3000词）\n",
    "        # 字符计数是令牌计数的粗略代理。根据模型限制需要调整。\n",
    "        max_text_chars = 12000 # 大约3000词。对许多模型应该是安全的。\n",
    "        article_text_for_llm = article_dict['cleaned_text'][:max_text_chars]\n",
    "        if len(article_dict['cleaned_text']) > max_text_chars:\n",
    "            print(f\"  警告：文章文本从{len(article_dict['cleaned_text'])}字符截断到{max_text_chars}字符用于LLM NER。\")\n",
    "\n",
    "        llm_ner_raw_output = call_llm_for_response(ner_system_prompt, article_text_for_llm)\n",
    "        extracted_entities_list = parse_llm_json_output(llm_ner_raw_output)\n",
    "        \n",
    "        # 存储结果\n",
    "        current_article_data = article_dict.copy() # 复制以避免直接修改原始列表项\n",
    "        current_article_data['llm_entities'] = extracted_entities_list\n",
    "        articles_with_entities.append(current_article_data)\n",
    "        \n",
    "        print(f\"  为文章ID {article_dict['id']} 提取了{len(extracted_entities_list)}个实体。\")\n",
    "        if extracted_entities_list:\n",
    "            # 打印实体样本，最多3个\n",
    "            print(f\"  实体样本: {json.dumps(extracted_entities_list[:min(3, len(extracted_entities_list))], indent=2, ensure_ascii=False)}\")\n",
    "        \n",
    "        if i < num_articles_to_process_ner - 1: # 避免在最后一篇文章后睡眠\n",
    "            time.sleep(1) # 小延迟以对API礼貌\n",
    "            \n",
    "    if articles_with_entities:\n",
    "        print(f\"\\nLLM NER完成。处理了{len(articles_with_entities)}篇文章并存储了实体。\")\n",
    "else:\n",
    "    print(\"跳过LLM NER：先决条件（清理的文章、LLM客户端或有效实体类型字符串）未满足。\")\n",
    "    # 如果跳过NER，确保articles_with_entities填充空实体列表用于后续步骤\n",
    "    if cleaned_articles: # 仅当我们首先有清理的文章时\n",
    "        num_articles_to_fallback = min(len(cleaned_articles), MAX_ARTICLES_FOR_LLM_NER)\n",
    "        for article_dict_fallback in cleaned_articles[:num_articles_to_fallback]:\n",
    "            fallback_data = article_dict_fallback.copy()\n",
    "            fallback_data['llm_entities'] = []\n",
    "            articles_with_entities.append(fallback_data)\n",
    "        print(f\"用具有空'llm_entities'列表的{len(articles_with_entities)}个条目填充'articles_with_entities'。\")\n",
    "\n",
    "# 确保articles_with_entities已定义\n",
    "if 'articles_with_entities' not in globals():\n",
    "    articles_with_entities = []\n",
    "    print(\"将'articles_with_entities'初始化为空列表。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-ner-exec-output",
   "metadata": {},
   "source": [
    "**输出说明：**\n",
    "此代码块将显示基于LLM的NER的进度：\n",
    "*   对于每篇处理的文章：其ID、关于截断的消息（如果有）、提取的实体数量，以及JSON格式的提取实体样本。\n",
    "*   指示完成或为什么跳过步骤的最终消息。\n",
    "`articles_with_entities`列表现在包含原始文章数据加上新键`llm_entities`，其中包含LLM为该文章提取的实体列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-2-desc",
   "metadata": {},
   "source": [
    "### 步骤2.2：关系提取\n",
    "**任务：** 识别提取实体之间的语义关系，如收购事件或隶属关系。\n",
    "\n",
    "**书籍概念：** （第2章 - 关系作为边）\n",
    "关系定义实体如何连接，形成我们知识图谱中的*边*。提取这些关系对于捕获实际知识至关重要（例如，\"公司A *收购了* 公司B\"，\"收购 *价格为* X百万美元\"）。\n",
    "\n",
    "**方法论：**\n",
    "与NER类似，我们将使用LLM进行关系提取（RE）。对于每篇文章：\n",
    "1.  **系统提示：** 我们设计一个系统提示，指示LLM充当科技收购的关系提取专家。它指定：\n",
    "    *   所需的关系类型（谓词），如`ACQUIRED`、`HAS_PRICE`、`ANNOUNCED_ON`、`ACQUIRER_IS`、`ACQUIRED_COMPANY_IS`、`INVOLVES_PRODUCT`。这些被选择来捕获收购事件的关键方面。\n",
    "    *   关系的主语和宾语必须是为该文章提供的实体列表中的确切文本跨度的要求。\n",
    "    *   所需的输出格式：JSON对象列表，每个对象具有`subject_text`、`subject_type`、`predicate`、`object_text`和`object_type`。\n",
    "    *   输出格式的示例。\n",
    "2.  **用户提示：** 每篇文章的用户提示将包含：\n",
    "    *   文章的`cleaned_text`。\n",
    "    *   在前一步中为该特定文章提取的`llm_entities`列表（在提示中序列化为JSON字符串）。\n",
    "3.  **处理循环：** 遍历`articles_with_entities`。如果文章有实体：\n",
    "    *   构建用户提示。\n",
    "    *   调用LLM。\n",
    "    *   解析JSON输出。\n",
    "    *   可选地，验证提取关系中的主语/宾语文本确实来自提供的实体列表以保持一致性。\n",
    "    *   将提取的关系存储在新列表`articles_with_relations`中（每个项目将是增强了`llm_relations`的文章数据）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 结论和未来工作\n",
    "\n",
    "**总结：**\n",
    "本笔记本演示了一个完整的端到端流水线，用于从新闻文章构建关于科技公司收购的知识图谱。我们涵盖了：\n",
    "\n",
    "1. **数据获取与准备：** 从CNN/DailyMail数据集获取相关文章并进行清理。\n",
    "2. **信息提取：** 使用spaCy进行初步探索，然后使用LLM进行精确的实体和关系提取。\n",
    "3. **知识图谱构建：** 将提取的信息结构化为RDF三元组。\n",
    "4. **知识图谱精炼：** 使用嵌入技术进行链接预测和知识发现。\n",
    "5. **持久化与利用：** 存储、查询和可视化构建的知识图谱。\n",
    "\n",
    "**关键见解：**\n",
    "*   **混合方法的价值：** 结合传统NLP工具（spaCy）和现代LLM提供了灵活性和准确性。\n",
    "*   **提示工程的重要性：** 精心制作的系统提示对于从LLM获得高质量、结构化输出至关重要。\n",
    "*   **数据质量的影响：** 清理和预处理步骤显著影响下游任务的质量。\n",
    "*   **可扩展性考虑：** 虽然此演示使用小样本，但该方法可以扩展到更大的数据集。\n",
    "\n",
    "**未来工作方向：**\n",
    "1. **扩展实体类型：** 包括更多特定领域的实体，如技术栈、投资轮次、监管机构。\n",
    "2. **改进关系提取：** 开发更复杂的关系类型和时间关系。\n",
    "3. **实体链接：** 实现与外部知识库（如Wikidata、DBpedia）的链接。\n",
    "4. **多语言支持：** 扩展到处理多种语言的新闻来源。\n",
    "5. **实时更新：** 开发增量更新机制以处理新的新闻文章。\n",
    "6. **评估框架：** 建立全面的评估指标来衡量知识图谱质量。\n",
    "7. **应用开发：** 构建基于知识图谱的应用，如趋势分析、投资建议系统。\n",
    "\n",
    "**技术改进：**\n",
    "*   **错误处理：** 实现更强大的错误恢复机制。\n",
    "*   **性能优化：** 优化LLM调用和处理流水线以提高效率。\n",
    "*   **质量保证：** 添加验证步骤以确保提取信息的准确性。\n",
    "*   **可视化增强：** 开发更丰富的交互式可视化工具。\n",
    "\n",
    "这个流水线为在各种领域构建知识图谱提供了坚实的基础，展示了现代NLP技术在结构化知识提取和表示中的强大功能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}