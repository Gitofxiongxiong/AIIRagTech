{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 基于强化学习的简单RAG系统\n",
    "\n",
    "[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/release/python-370/) [![Nebius AI](https://img.shields.io/badge/Nebius%20AI-LLM-brightgreen)](https://cloud.nebius.ai/services/llm-embedding) [![OpenAI](https://img.shields.io/badge/OpenAI-API-lightgrey)](https://openai.com/) [![Medium](https://img.shields.io/badge/Medium-Blog-black?logo=medium)](https://medium.com/@fareedkhandev/maximizing-simple-rag-performance-using-rl-rewards-in-python-d4c14cbadf59)\n",
    "\n",
    "简单的RAG（检索增强生成）系统通过三个简单步骤工作：\n",
    "\n",
    "1. **索引（Indexing）**: 将文档分解为块并转换为向量嵌入。\n",
    "\n",
    "2. **检索（Retrieval）**: 当提出问题时，找到最相关的文档块。\n",
    "\n",
    "3. **生成（Generation）**: 将问题与检索到的文档块结合，让AI使用这些信息生成答案。\n",
    "\n",
    "实际问题是使用提供的文档为给定问题生成答案。简单的RAG系统往往由于检索到的文档块缺乏上下文而无法生成准确的答案。在本笔记本中，我们将使用`RL RAG`（强化学习RAG）方法来使用提供的文档为给定问题生成答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "\n",
    "- [环境设置](#环境设置)\n",
    "- [数据预处理](#数据预处理)\n",
    "- [文档嵌入生成](#文档嵌入生成)\n",
    "- [向量存储实现](#向量存储实现)\n",
    "- [简单检索实现](#简单检索实现)\n",
    "  - [余弦相似度](#余弦相似度)\n",
    "  - [相似度搜索](#相似度搜索)\n",
    "  - [LLM响应生成](#llm响应生成)\n",
    "  - [基础RAG管道](#基础rag管道)\n",
    "  - [基础RAG评估](#评估基础rag管道)\n",
    "- [RAG的强化学习](#rag的强化学习)\n",
    "  - [状态、动作空间和奖励方法](#状态动作空间和奖励方法)\n",
    "  - [策略网络](#策略网络)\n",
    "  - [单个RL步骤](#单个rl步骤)\n",
    "  - [训练参数和策略更新](#训练参数和策略更新)\n",
    "  - [训练循环](#训练循环)\n",
    "  - [性能比较逻辑](#性能比较逻辑)\n",
    "- [评估框架](#评估框架)\n",
    "- [评估RL与简单RAG](#评估rl与简单rag)\n",
    "- [保存比较结果](#保存比较结果)\n",
    "- [结论](#我们能得出什么结论)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置\n",
    "\n",
    "首先，我们需要导入必要的库并设置环境。我们将使用托管在**Nebius**平台下的HuggingFace模型。显然，只要与OpenAI的API兼容，您可以使用自己的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入os模块用于与操作系统交互\n",
    "import os\n",
    "\n",
    "# 导入OpenAI模块用于使用OpenAI的API\n",
    "from openai import OpenAI\n",
    "\n",
    "# 导入numpy用于数值运算\n",
    "import numpy as np\n",
    "\n",
    "# 导入json用于处理JSON数据\n",
    "import json\n",
    "\n",
    "# 导入typing模块用于类型提示\n",
    "from typing import Dict, List, Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要初始化负责响应和嵌入生成的客户端。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用OpenAI客户端设置API连接\n",
    "# 请将base_url和api_key替换为您自己的值\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",  # 基础URL（例如ollama api，其他llm api提供商）\n",
    "    api_key= os.environ[\"OPENAI_API_KEY\"]  # 用于身份验证的API密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "现在我们进入数据预处理阶段，我们需要加载数据并对其进行预处理。让我们创建一个函数，从目录中加载所有`.txt`文件并返回文档列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从目录加载文档的函数\n",
    "def load_documents(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    从指定目录加载所有文本文档。\n",
    "\n",
    "    参数:\n",
    "        directory_path (str): 包含文本文件的目录路径。\n",
    "\n",
    "    返回:\n",
    "        List[str]: 字符串列表，其中每个字符串是文本文件的内容。\n",
    "    \"\"\"\n",
    "    documents = []  # 初始化空列表来存储文档内容\n",
    "    for filename in os.listdir(directory_path):  # 遍历目录中的所有文件\n",
    "        if filename.endswith(\".txt\"):  # 检查文件是否具有.txt扩展名\n",
    "            # 以UTF-8编码的读取模式打开文件，并将其内容添加到列表中\n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents  # 返回文档内容列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要创建一个函数，在文档加载后对其进行分块。我们使用`chunk_size`为`100`个字符，但您可以根据需要进行调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文档分割为块的函数\n",
    "def split_into_chunks(documents: List[str], chunk_size: int = 30) -> List[str]:\n",
    "    \"\"\"\n",
    "    将文档分割为指定大小的较小块。\n",
    "\n",
    "    参数:\n",
    "        documents (List[str]): 要分割为块的文档字符串列表。\n",
    "        chunk_size (int): 每个块中的最大单词数。默认为100。\n",
    "\n",
    "    返回:\n",
    "        List[str]: 块列表，其中每个块是包含最多`chunk_size`个单词的字符串。\n",
    "    \"\"\"\n",
    "    chunks = []  # 初始化空列表来存储块\n",
    "    for doc in documents:  # 遍历每个文档\n",
    "        words = doc.split()  # 将文档分割为单词\n",
    "        # 创建指定大小的块\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i:i + chunk_size])  # 连接单词形成块\n",
    "            chunks.append(chunk)  # 将块添加到列表中\n",
    "    return chunks  # 返回块列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步是**可选的**，我们通过删除特殊字符、转换为小写等方式预处理每个块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理文本的函数（例如，小写化，删除特殊字符）\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    通过转换为小写并删除特殊字符来预处理输入文本。\n",
    "\n",
    "    参数:\n",
    "        text (str): 要预处理的输入文本。\n",
    "\n",
    "    返回:\n",
    "        str: 仅包含字母数字字符和空格的预处理文本。\n",
    "    \"\"\"\n",
    "    # 将文本转换为小写\n",
    "    text = text.lower()\n",
    "    # 删除特殊字符，仅保留字母数字字符和空格\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，如果您使用前面的预处理步骤，您可以简单地创建一个函数来预处理整个文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理所有块的函数\n",
    "def preprocess_chunks(chunks: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    对所有文本块应用预处理。\n",
    "\n",
    "    参数:\n",
    "        chunks (List[str]): 要预处理的文本块列表。\n",
    "\n",
    "    返回:\n",
    "        List[str]: 预处理后的文本块列表。\n",
    "    \"\"\"\n",
    "    # 对列表中的每个块应用preprocess_text函数\n",
    "    return [preprocess_text(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经实现了数据预处理的所有函数，我们可以从目录加载文档，将其分割为块，并预处理这些块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定包含文本文件的目录路径\n",
    "directory_path = \"data\"\n",
    "\n",
    "# 从指定目录加载所有文本文档\n",
    "documents = load_documents(directory_path)\n",
    "\n",
    "# 将加载的文档分割为较小的文本块\n",
    "chunks = split_into_chunks(documents)\n",
    "\n",
    "# 预处理块（例如，小写化，删除特殊字符）\n",
    "preprocessed_chunks = preprocess_chunks(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印前两个块的前200个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 1: quantum computing principles progress and possibil ... \n",
      "--------------------------------------------------\n",
      "块 2: process information in binary digits bits quantum  ... \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 打印前2个预处理块，仅显示每个块的前200个字符\n",
    "for i in range(2):\n",
    "    # 使用切片将输出限制为前200个字符\n",
    "    print(f\"块 {i+1}: {preprocessed_chunks[i][:50]} ... \")\n",
    "    print(\"-\" * 50)  # 打印分隔线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档嵌入生成\n",
    "\n",
    "在前面的步骤中，我们对文档进行了分块。现在是时候为块数据集生成嵌入了。在使用RAG时，我们的知识库通常相当大。因此，我们需要批量执行嵌入生成。让我们创建一个核心函数来批量生成块的嵌入。\n",
    "\n",
    "我们使用的嵌入模型是`BAAI/bge-en-icl`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为单个文本块批次生成嵌入的函数\n",
    "def generate_embeddings_batch(chunks_batch: List[str], model: str = \"BAAI/bge-en-icl\") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    使用OpenAI客户端为一批文本块生成嵌入。\n",
    "\n",
    "    参数:\n",
    "        chunks_batch (List[str]): 要生成嵌入的文本块批次。\n",
    "        model (str): 用于嵌入生成的模型。默认为\"BAAI/bge-en-icl\"。\n",
    "\n",
    "    返回:\n",
    "        List[List[float]]: 嵌入列表，其中每个嵌入是浮点数列表。\n",
    "    \"\"\"\n",
    "    # 使用OpenAI客户端为输入批次创建嵌入\n",
    "    response = client.embeddings.create(\n",
    "        model=model,  # 指定用于嵌入生成的模型\n",
    "        input=chunks_batch  # 提供文本块批次作为输入\n",
    "    )\n",
    "    # 从响应中提取嵌入并返回\n",
    "    embeddings = [item.embedding for item in response.data]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将定义一个函数来批量生成所有文本块的嵌入。该函数将接受文本块列表作为输入，并使用OpenAI客户端为每批块生成嵌入。该函数将返回对应于所有文本块的嵌入列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量生成所有块嵌入的函数\n",
    "def generate_embeddings(chunks: List[str], batch_size: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    批量生成所有文本块的嵌入。\n",
    "\n",
    "    参数:\n",
    "        chunks (List[str]): 要生成嵌入的文本块列表。\n",
    "        batch_size (int): 每批处理的块数。默认为10。\n",
    "\n",
    "    返回:\n",
    "        np.ndarray: 包含所有块嵌入的NumPy数组。\n",
    "    \"\"\"\n",
    "    all_embeddings = []  # 初始化空列表来存储所有嵌入\n",
    "\n",
    "    # 批量遍历块\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        # 提取当前批次的块\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        # 为当前批次生成嵌入\n",
    "        embeddings = generate_embeddings_batch(batch)\n",
    "        # 将当前批次的嵌入扩展到所有嵌入列表中\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    # 将嵌入列表转换为NumPy数组并返回\n",
    "    return np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们创建另一个函数来将嵌入保存为JSON格式的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将嵌入保存到文件的函数\n",
    "def save_embeddings(embeddings: np.ndarray, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    将嵌入保存到JSON文件。\n",
    "\n",
    "    参数:\n",
    "        embeddings (np.ndarray): 包含要保存的嵌入的NumPy数组。\n",
    "        output_file (str): 将保存嵌入的输出JSON文件路径。\n",
    "\n",
    "    返回:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 以UTF-8编码的写入模式打开指定文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        # 将NumPy数组转换为列表并保存为JSON\n",
    "        json.dump(embeddings.tolist(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经实现了嵌入生成的所有函数，我们可以继续为预处理的文本块生成嵌入并将其保存到JSON文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保在生成嵌入之前对块进行预处理\n",
    "preprocessed_chunks = preprocess_chunks(chunks)\n",
    "\n",
    "# 为预处理的块生成嵌入\n",
    "embeddings = generate_embeddings(preprocessed_chunks)\n",
    "\n",
    "# 将生成的嵌入保存到名为\"embeddings.json\"的JSON文件中\n",
    "save_embeddings(embeddings, \"embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量存储实现\n",
    "由于我们没有使用任何Python库进行向量存储，我们将使用字典实现一个简单的向量存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将内存中的向量存储初始化为字典\n",
    "# 键将是唯一标识符（整数），值将是包含嵌入和相应文本块的字典\n",
    "vector_store: dict[int, dict[str, object]] = {}\n",
    "\n",
    "# 将嵌入和相应的文本块添加到向量存储的函数\n",
    "def add_to_vector_store(embeddings: np.ndarray, chunks: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    将嵌入及其相应的文本块添加到向量存储中。\n",
    "\n",
    "    参数:\n",
    "        embeddings (np.ndarray): 包含要添加的嵌入的NumPy数组。\n",
    "        chunks (List[str]): 与嵌入对应的文本块列表。\n",
    "\n",
    "    返回:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 同时遍历嵌入和块\n",
    "    for embedding, chunk in zip(embeddings, chunks):\n",
    "        # 将每个嵌入及其相应的块添加到向量存储中\n",
    "        # 使用向量存储的当前长度作为唯一键\n",
    "        vector_store[len(vector_store)] = {\"embedding\": embedding, \"chunk\": chunk}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单检索实现\n",
    "\n",
    "我们知道，为了检索与给定查询最相似的文本块，我们可以使用查询嵌入与所有文本块嵌入之间的余弦相似度。余弦相似度越高，文本块越相似。然后我们可以根据相似度分数对块进行排序，并返回前k个最相似的块。\n",
    "    \n",
    "所以，让我们实现一个基于余弦相似度的简单检索函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "两个向量$A$和$B$之间的余弦相似度计算如下：\n",
    "\n",
    "$$\\text{余弦相似度} = \\frac{A \\cdot B}{||A|| \\times ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n",
    "其中：\n",
    "- $A \\cdot B$ 是向量$A$和$B$的点积\n",
    "- $||A||$ 和 $||B||$ 是向量的欧几里得范数（幅度）\n",
    "- $n$ 是向量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两个向量之间余弦相似度的函数\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度。\n",
    "\n",
    "    参数:\n",
    "        vec1 (np.ndarray): 第一个向量。\n",
    "        vec2 (np.ndarray): 第二个向量。\n",
    "\n",
    "    返回:\n",
    "        float: 两个向量之间的余弦相似度，范围从-1到1。\n",
    "    \"\"\"\n",
    "    # 计算两个向量的点积\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    # 计算第一个向量的幅度（范数）\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    # 计算第二个向量的幅度（范数）\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    # 返回余弦相似度作为点积与范数乘积的比值\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们计算查询与所有块之间的余弦相似度时，我们可以执行相似度搜索。基于`top_k`参数，我们检索前k个最相似的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在向量存储中执行相似度搜索的函数\n",
    "def similarity_search(query_embedding: np.ndarray, top_k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    在向量存储中执行相似度搜索并返回top_k个最相似的块。\n",
    "\n",
    "    参数:\n",
    "        query_embedding (np.ndarray): 查询的嵌入向量。\n",
    "        top_k (int): 要检索的最相似块的数量。默认为5。\n",
    "\n",
    "    返回:\n",
    "        List[str]: top_k个最相似文本块的列表。\n",
    "    \"\"\"\n",
    "    similarities = []  # 初始化列表来存储相似度分数和相应的键\n",
    "\n",
    "    # 遍历向量存储中的所有项目\n",
    "    for key, value in vector_store.items():\n",
    "        # 计算查询嵌入与存储嵌入之间的余弦相似度\n",
    "        similarity = cosine_similarity(query_embedding, value[\"embedding\"])\n",
    "        # 将键和相似度分数作为元组添加到列表中\n",
    "        similarities.append((key, similarity))\n",
    "\n",
    "    # 根据相似度分数按降序对相似度列表进行排序\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 根据键检索top_k个最相似的块\n",
    "    return [vector_store[key][\"chunk\"] for key, _ in similarities[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦我们准备好相似度搜索函数，我们可以简单地在其基础上编写一个检索函数，该函数将根据查询提供相关的块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为查询检索相关文档块的函数\n",
    "def retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    为给定查询文本检索最相关的文档块。\n",
    "\n",
    "    参数:\n",
    "        query_text (str): 要检索相关块的查询文本。\n",
    "        top_k (int): 要检索的最相关块的数量。默认为5。\n",
    "\n",
    "    返回:\n",
    "        List[str]: top_k个最相关文本块的列表。\n",
    "    \"\"\"\n",
    "    # 使用嵌入模型为查询文本生成嵌入\n",
    "    query_embedding = generate_embeddings([query_text])[0]\n",
    "    \n",
    "    # 执行相似度搜索以找到最相关的块\n",
    "    relevant_chunks = similarity_search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    # 返回相关块列表\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经实现了检索的所有函数，我们可以继续使用示例查询测试检索系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 1: quantum computing principles progress and possibil ... \n",
      "--------------------------------------------------\n",
      "块 2: through distinct stages 1 nisq era current 2 error ... \n",
      "--------------------------------------------------\n",
      "块 3: quantum advantage and practical applications quant ... \n",
      "--------------------------------------------------\n",
      "块 4: process information in binary digits bits quantum  ... \n",
      "--------------------------------------------------\n",
      "块 5: measuring the correct answer quantum gates and cir ... \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 将生成的嵌入及其相应的预处理块添加到向量存储中\n",
    "add_to_vector_store(embeddings, preprocessed_chunks)\n",
    "\n",
    "# 定义要检索相关文档块的查询文本\n",
    "query_text = \"什么是量子计算？\"\n",
    "\n",
    "# 根据查询文本从向量存储中检索最相关的块\n",
    "relevant_chunks = retrieve_relevant_chunks(query_text)\n",
    "\n",
    "# 打印每个检索到的相关块的前50个字符\n",
    "for idx, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"块 {idx + 1}: {chunk[:50]} ... \")\n",
    "    print(\"-\" * 50)  # 打印分隔线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM响应生成\n",
    "\n",
    "当我们有查询和一组相关文档块时，我们可以使用大型语言模型（LLM）基于查询和检索到的信息生成响应。在本节中，我们将使用OpenAI API通过向LLM提供查询文本和相关文档块作为上下文来生成查询响应。\n",
    "\n",
    "首先，我们需要一个函数来构建LLM的输入提示，其中包括查询文本和相关文档块作为上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建带有上下文的提示的函数\n",
    "def construct_prompt(query: str, context_chunks: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    通过将查询与检索到的上下文块结合来构建提示。\n",
    "\n",
    "    参数:\n",
    "        query (str): 正在构建提示的查询文本。\n",
    "        context_chunks (List[str]): 要包含在提示中的相关上下文块列表。\n",
    "\n",
    "    返回:\n",
    "        str: 构建的提示，用作LLM的输入。\n",
    "    \"\"\"\n",
    "    # 将所有上下文块合并为单个字符串，用换行符分隔\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    \n",
    "    # 定义系统消息来指导LLM的行为\n",
    "    system_message = (\n",
    "        \"你是一个有用的助手。仅使用提供的上下文来回答问题。\"\n",
    "        \"如果上下文不包含所需的信息，请说'我没有足够的信息来回答这个问题。'\"\n",
    "    )\n",
    "    \n",
    "    # 通过组合系统消息、上下文和查询来构建最终提示\n",
    "    prompt = f\"系统: {system_message}\\n\\n上下文:\\n{context}\\n\\n问题:\\n{query}\\n\\n答案:\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要生成LLM响应，我们需要实现一个函数，该函数接受构建的输入提示并将其发送到OpenAI API进行响应生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用OpenAI聊天模型生成响应的函数\n",
    "def generate_response(\n",
    "    prompt: str,\n",
    "    model: str = \"google/gemma-2-2b-it\",\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 1,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    基于构建的提示从OpenAI聊天模型生成响应。\n",
    "\n",
    "    参数:\n",
    "        prompt (str): 提供给聊天模型的输入提示。\n",
    "        model (str): 用于生成响应的模型。默认为\"google/gemma-2-2b-it\"。\n",
    "        max_tokens (int): 响应中的最大令牌数。默认为512。\n",
    "        temperature (float): 响应多样性的采样温度。默认为0.5。\n",
    "        top_p (float): 核采样的概率质量。默认为0.9。\n",
    "        top_k (int): 要考虑的最高概率令牌数。默认为50。\n",
    "\n",
    "    返回:\n",
    "        str: 聊天模型生成的响应。\n",
    "    \"\"\"\n",
    "    # 使用OpenAI客户端创建聊天完成\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,  # 指定用于生成响应的模型\n",
    "        max_tokens=max_tokens,  # 响应中的最大令牌数\n",
    "        temperature=temperature,  # 响应多样性的采样温度\n",
    "        top_p=top_p,  # 核采样的概率质量\n",
    "        extra_body={  # 请求的附加参数\n",
    "            \"top_k\": top_k  # 要考虑的最高概率令牌数\n",
    "        },\n",
    "        messages=[  # 为聊天模型提供上下文的消息列表\n",
    "            {\n",
    "                \"role\": \"user\",  # 消息发送者的角色（在这种情况下是用户）\n",
    "                \"content\": [  # 消息的内容\n",
    "                    {\n",
    "                        \"type\": \"text\",  # 内容类型（在这种情况下是文本）\n",
    "                        \"text\": prompt  # 实际的提示文本\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # 返回响应中第一个选择的内容\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础RAG管道\n",
    "\n",
    "我们不能重复运行小段代码。因此，我们需要创建一个简单的RAG管道，该管道只接受一个参数（我们的查询），并返回LLM响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现基础检索增强生成（RAG）管道的函数\n",
    "def basic_rag_pipeline(query: str) -> str:\n",
    "    \"\"\"\n",
    "    实现基础检索增强生成（RAG）管道：\n",
    "    检索相关块，构建提示，并生成响应。\n",
    "\n",
    "    参数:\n",
    "        query (str): 要生成响应的输入查询。\n",
    "\n",
    "    返回:\n",
    "        str: 基于查询和检索上下文的LLM生成响应。\n",
    "    \"\"\"\n",
    "    # 步骤1：为给定查询检索最相关的块\n",
    "    relevant_chunks: List[str] = retrieve_relevant_chunks(query)\n",
    "    \n",
    "    # 步骤2：使用查询和检索到的块构建提示\n",
    "    prompt: str = construct_prompt(query, relevant_chunks)\n",
    "    \n",
    "    # 步骤3：使用构建的提示从LLM生成响应\n",
    "    response: str = generate_response(prompt)\n",
    "    \n",
    "    # 返回生成的响应\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估基础RAG管道\n",
    "\n",
    "现在我们已经编写了基础RAG管道，我们可以用它进行评估。我们的评估查询包含不同的目标段，如`factual_queries`和`complex_nature`。我们将测试RAG管道的事实知识。\n",
    "\n",
    "让我们加载评估查询及其预期答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "示例查询: 量子比特在叠加态中的数学表示是什么？\n",
      "\n",
      "预期答案: |ψ⟩ = α|0⟩ + β|1⟩，其中α和β是满足|α|² + |β|² = 1的复数，分别表示在状态|0⟩或|1⟩中测量量子比特的概率幅度。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 以读取模式打开验证数据文件并将其内容加载为字典\n",
    "with open('data/val_rl.json', 'r') as file:\n",
    "    validation_data = json.load(file)\n",
    "\n",
    "# 使用示例查询测试基础RAG管道\n",
    "sample_query = validation_data['basic_factual_questions'][0]['question']  # 提取查询文本\n",
    "expected_answer = validation_data['basic_factual_questions'][0]['answer']  # 提取真实答案\n",
    "\n",
    "# 打印示例查询和预期答案\n",
    "print(f\"示例查询: {sample_query}\\n\")\n",
    "print(f\"预期答案: {expected_answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们用这个评估查询测试基础RAG管道，看看它的表现如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 运行检索增强生成（RAG）管道...\n",
      "📥 查询: 量子比特在叠加态中的数学表示是什么？\n",
      "\n",
      "🤖 AI响应:\n",
      "--------------------------------------------------\n",
      "ψ  α0  β1\n",
      "--------------------------------------------------\n",
      "✅ 真实答案:\n",
      "--------------------------------------------------\n",
      "|ψ⟩ = α|0⟩ + β|1⟩，其中α和β是满足|α|² + |β|² = 1的复数，分别表示在状态|0⟩或|1⟩中测量量子比特的概率幅度。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 打印消息以指示RAG管道的开始\n",
    "print(\"🔍 运行检索增强生成（RAG）管道...\")\n",
    "print(f\"📥 查询: {sample_query}\\n\")\n",
    "\n",
    "# 运行RAG管道并获取响应\n",
    "response = basic_rag_pipeline(sample_query)\n",
    "\n",
    "# 以更好的格式打印响应\n",
    "print(\"🤖 AI响应:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.strip())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 打印真实答案进行比较\n",
    "print(\"✅ 真实答案:\")\n",
    "print(\"-\" * 50)\n",
    "print(expected_answer)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的RAG管道在当前状态下似乎表现不佳。生成的响应不仅与真实答案无关，而且还缺少关键信息。\n",
    "\n",
    "但别担心！在接下来的步骤中，我们将实现基于强化学习的RAG管道来解决这些不足。这将帮助我们改进检索和生成过程，使响应更准确和上下文相关。\n",
    "\n",
    "敬请期待，我们将把RAG管道提升到新的水平！🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "## RAG的强化学习\n",
    "\n",
    "强化学习（Reinforcement Learning, RL）是一种机器学习类型，其中智能体通过在环境中采取行动来学习做决策，以最大化某种累积奖励的概念。与监督学习不同，智能体不会被明确告知要采取哪些行动，而是必须通过试错来发现哪些行动产生最多的奖励。\n",
    "\n",
    "以下是强化学习系统的主要组件：\n",
    "\n",
    "1. **智能体（Agent）**: 学习者或决策者\n",
    "2. **环境（Environment）**: 智能体与之交互的世界\n",
    "3. **状态（State, S）**: 智能体在环境中的当前情况\n",
    "4. **动作（Action, A）**: 智能体可以做出的一组可能移动\n",
    "5. **奖励（Reward, R）**: 每次行动后来自环境的反馈\n",
    "6. **策略（Policy, π）**: 智能体遵循的策略，用于确定下一个行动\n",
    "\n",
    "强化学习的目标是学习一个策略π，使期望累积奖励最大化：\n",
    "\n",
    "$$\\pi^* = \\arg\\max_\\pi \\mathbb{E}\\left[ \\sum_{t=0}^{T} \\gamma^t R_t \\right]$$\n",
    "\n",
    "其中：\n",
    "- $\\pi^*$ 是最优策略\n",
    "- $\\gamma$ 是折扣因子（0 ≤ γ ≤ 1）\n",
    "- $R_t$ 是时间步t的奖励\n",
    "- $T$ 是最终时间步\n",
    "\n",
    "在RAG系统的上下文中，强化学习可以用于：\n",
    "- 通过学习哪些文档最有帮助来改进检索\n",
    "- 基于用户反馈优化提示构建\n",
    "- 通过从成功响应中学习来优化生成过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 状态、动作空间和奖励方法\n",
    "\n",
    "编写RL算法时的第一步是定义三个要素：\n",
    "\n",
    "- **状态（State）**: 环境的当前情况。在我们的情况下，初始状态是我们的简单RAG管道（查询、上下文、响应）。\n",
    "- **动作空间（Action Space）**: 智能体基于状态做出的决策。在我们的情况下，动作可以包括更改模型、修改上下文、改变查询等。\n",
    "- **奖励（Reward）**: 智能体在采取行动后收到的反馈。在我们的情况下，奖励可以是生成响应与真实答案之间的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的状态将在训练过程中不断变化。为此，我们需要在每个`训练回合`后保存状态，以便我们的RL智能体可以从中学习并避免再次犯同样的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为强化学习定义状态表示的函数\n",
    "def define_state(\n",
    "    query: str, \n",
    "    context_chunks: List[str], \n",
    "    rewritten_query: str = None, \n",
    "    previous_responses: List[str] = None, \n",
    "    previous_rewards: List[float] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    为强化学习智能体定义状态表示。\n",
    "    \n",
    "    参数:\n",
    "        query (str): 原始用户查询。\n",
    "        context_chunks (List[str]): 从知识库检索的上下文块。\n",
    "        rewritten_query (str, optional): 原始查询的重新表述版本。\n",
    "        previous_responses (List[str], optional): 先前生成的响应列表。\n",
    "        previous_rewards (List[float], optional): 先前行动收到的奖励列表。\n",
    "    \n",
    "    返回:\n",
    "        dict: 表示当前状态及所有相关信息的字典。\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"original_query\": query,                                    # 来自用户的初始查询\n",
    "        \"current_query\": rewritten_query if rewritten_query else query,  # 查询的当前版本（可能已重写）\n",
    "        \"context\": context_chunks,                                 # 从知识库检索的上下文块\n",
    "        \"previous_responses\": previous_responses if previous_responses else [],  # 生成响应的历史\n",
    "        \"previous_rewards\": previous_rewards if previous_rewards else []         # 收到奖励的历史\n",
    "    }\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经为RL智能体定义了状态表示，包括用户查询、检索的上下文块、重写的查询（如果有）以及响应和奖励的历史。这个状态将指导智能体生成更好的响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要为强化学习智能体定义动作空间。动作空间包括智能体在每个步骤可以采取的可能动作集合。在这种情况下，我们定义四个动作：\n",
    "- `rewrite_query`: 重新表述原始查询以改进检索\n",
    "- `expand_context`: 检索额外的上下文块\n",
    "- `filter_context`: 删除不相关的上下文块\n",
    "- `generate_response`: 基于当前查询和上下文生成响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为强化学习定义动作空间的函数\n",
    "def define_action_space() -> List[str]:\n",
    "    \"\"\"\n",
    "    定义强化学习智能体可以采取的可能动作集合。\n",
    "    \n",
    "    动作包括：\n",
    "    - rewrite_query: 重新表述原始查询以改进检索\n",
    "    - expand_context: 检索额外的上下文块\n",
    "    - filter_context: 删除不相关的上下文块\n",
    "    - generate_response: 基于当前查询和上下文生成响应\n",
    "    \n",
    "    返回:\n",
    "        List[str]: 可用动作列表。\n",
    "    \"\"\"\n",
    "\n",
    "    # 定义智能体可以采取的动作集合\n",
    "    actions = [\"rewrite_query\", \"expand_context\", \"filter_context\", \"generate_response\"]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，当我们的RL智能体采取行动时，它将基于当前状态和动作空间。它将根据RAG管道生成的响应质量获得奖励。奖励函数将基于生成响应与真实答案之间的余弦相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于响应质量计算奖励的函数\n",
    "def calculate_reward(response: str, ground_truth: str) -> float:\n",
    "    \"\"\"\n",
    "    通过将生成的响应与真实答案进行比较来计算奖励值。\n",
    "    \n",
    "    使用响应和真实答案嵌入之间的余弦相似度\n",
    "    来确定响应与预期答案的接近程度。\n",
    "    \n",
    "    参数:\n",
    "        response (str): RAG管道生成的响应。\n",
    "        ground_truth (str): 预期的正确答案。\n",
    "    \n",
    "    返回:\n",
    "        float: -1到1之间的奖励值，其中较高的值表示\n",
    "               与真实答案的相似度更高。\n",
    "    \"\"\"\n",
    "    # 为响应和真实答案生成嵌入\n",
    "    response_embedding = generate_embeddings([response])[0]\n",
    "    ground_truth_embedding = generate_embeddings([ground_truth])[0]\n",
    "    \n",
    "    # 计算嵌入之间的余弦相似度作为奖励\n",
    "    similarity = cosine_similarity(response_embedding, ground_truth_embedding)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的目标是通过生成与真实答案相似的响应来最大化奖励。较高的奖励值表明生成的响应与预期答案更加一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动作函数逻辑\n",
    "\n",
    "现在我们已经定义了动作空间，我们需要实现每个动作的逻辑。这个逻辑将确定如何根据RL智能体采取的动作来修改RAG管道。\n",
    "\n",
    "回顾一下，四个动作是：\n",
    "- `rewrite_query`: 重新表述原始查询以改进检索\n",
    "- `expand_context`: 检索额外的上下文块\n",
    "- `filter_context`: 删除不相关的上下文块\n",
    "- `generate_response`: 基于当前查询和上下文生成响应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们为智能体创建第一个动作逻辑。我们将实现的第一个动作是`rewrite_query`动作，它涉及重新表述原始用户查询以改进检索性能。这个动作对于增强检索上下文的相关性和生成更准确的响应至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重写查询以改进文档检索的函数\n",
    "def rewrite_query(\n",
    "    query: str, \n",
    "    context_chunks: List[str], \n",
    "    model: str = \"google/gemma-2-2b-it\", \n",
    "    max_tokens: int = 100, \n",
    "    temperature: float = 0.3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    使用LLM重写查询以改进文档检索。\n",
    "\n",
    "    参数:\n",
    "        query (str): 原始查询文本。\n",
    "        context_chunks (List[str]): 到目前为止检索的上下文块列表。\n",
    "        model (str): 用于生成重写查询的模型。默认为\"google/gemma-2-2b-it\"。\n",
    "        max_tokens (int): 重写查询中的最大令牌数。默认为100。\n",
    "        temperature (float): 响应多样性的采样温度。默认为0.3。\n",
    "\n",
    "    返回:\n",
    "        str: 为文档检索优化的重写查询。\n",
    "    \"\"\"\n",
    "    # 为LLM构建重写查询的提示\n",
    "    rewrite_prompt = f\"\"\"\n",
    "    你是一个查询优化助手。你的任务是重写给定的查询，使其更有效地\n",
    "    检索相关信息。该查询将用于文档检索。\n",
    "    \n",
    "    原始查询: {query}\n",
    "    \n",
    "    基于到目前为止检索的上下文：\n",
    "    {' '.join(context_chunks[:2]) if context_chunks else '尚无可用上下文'}\n",
    "    \n",
    "    重写查询使其更具体和针对性，以检索更好的信息。\n",
    "    重写的查询：\n",
    "    \"\"\"\n",
    "    \n",
    "    # 使用LLM生成重写的查询\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, # 指定用于生成响应的模型\n",
    "        max_tokens=max_tokens, # 响应中的最大令牌数\n",
    "        temperature=temperature, # 响应多样性的采样温度\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": rewrite_prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 从响应中提取并返回重写的查询\n",
    "    rewritten_query = response.choices[0].message.content.strip()\n",
    "    return rewritten_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个动作对于增强检索上下文的相关性和生成更准确的响应至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们编写下一个动作逻辑，即通过检索额外的块来扩展上下文。我们将使用现有函数`retrieve_relevant_chunks`来获取更多上下文块，然后从当前上下文中过滤掉任何重复项。我们将限制要添加到上下文中的新块数量为指定的top_k值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过检索额外块来扩展上下文的函数\n",
    "def expand_context(query: str, current_chunks: List[str], top_k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    通过检索额外的块来扩展上下文。\n",
    "\n",
    "    参数:\n",
    "        query (str): 需要额外上下文的查询文本。\n",
    "        current_chunks (List[str]): 当前上下文块列表。\n",
    "        top_k (int): 要检索的额外块数。默认为3。\n",
    "\n",
    "    返回:\n",
    "        List[str]: 包含新唯一块的扩展上下文块列表。\n",
    "    \"\"\"\n",
    "    # 检索比当前可用块更多的块\n",
    "    additional_chunks = retrieve_relevant_chunks(query, top_k=top_k + len(current_chunks))\n",
    "    \n",
    "    # 过滤掉已经在当前上下文中的块\n",
    "    new_chunks = []\n",
    "    for chunk in additional_chunks:\n",
    "        if chunk not in current_chunks:\n",
    "            new_chunks.append(chunk)\n",
    "    \n",
    "    # 将新的唯一块添加到当前上下文中，限制为top_k\n",
    "    expanded_context = current_chunks + new_chunks[:top_k]\n",
    "    return expanded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要过滤上下文以仅保留与查询最相关的块。这个过滤步骤对于确保提供给语言模型的上下文简洁且专注于最相关的信息至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤上下文以仅保留最相关块的函数\n",
    "def filter_context(query: str, context_chunks: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    过滤上下文以仅保留最相关的块。\n",
    "\n",
    "    参数:\n",
    "        query (str): 计算相关性的查询文本。\n",
    "        context_chunks (List[str]): 要过滤的上下文块列表。\n",
    "\n",
    "    返回:\n",
    "        List[str]: 最相关上下文块的过滤列表。\n",
    "    \"\"\"\n",
    "    if not context_chunks:\n",
    "        return []\n",
    "        \n",
    "    # 为查询和每个块生成嵌入\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    chunk_embeddings = [generate_embeddings([chunk])[0] for chunk in context_chunks]\n",
    "    \n",
    "    # 计算每个块的相关性分数\n",
    "    relevance_scores = []\n",
    "    for chunk_embedding in chunk_embeddings:\n",
    "        score = cosine_similarity(query_embedding, chunk_embedding)\n",
    "        relevance_scores.append(score)\n",
    "    \n",
    "    # 根据相关性分数按降序对块进行排序\n",
    "    sorted_chunks = [x for _, x in sorted(zip(relevance_scores, context_chunks), reverse=True)]\n",
    "    \n",
    "    # 保留前5个最相关的块，如果可用块少于5个则保留更少\n",
    "    filtered_chunks = sorted_chunks[:min(5, len(sorted_chunks))]\n",
    "    \n",
    "    return filtered_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个动作将帮助智能体探索与查询相关的更多信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 策略网络\n",
    "\n",
    "之前，我们定义了状态、动作和奖励逻辑。接下来，我们需要创建一个策略网络，该网络将根据当前状态选择动作。\n",
    "\n",
    "策略网络是一个函数，它接受当前状态和动作空间作为输入，并根据状态返回选择的动作。\n",
    "\n",
    "策略网络可以使用简单的启发式方法根据当前状态选择动作。例如，如果没有先前的响应，策略网络可以优先重写查询。如果上下文有太多块，策略网络可以选择过滤上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义策略网络以根据状态选择动作的函数\n",
    "def policy_network(\n",
    "    state: dict, \n",
    "    action_space: List[str], \n",
    "    epsilon: float = 0.2\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    定义策略网络，使用epsilon-贪婪策略根据当前状态选择动作。\n",
    "\n",
    "    参数:\n",
    "        state (dict): 环境的当前状态，包括查询、上下文、响应和奖励。\n",
    "        action_space (List[str]): 智能体可以采取的可能动作列表。\n",
    "        epsilon (float): 选择随机动作进行探索的概率。默认为0.2。\n",
    "\n",
    "    返回:\n",
    "        str: 从动作空间中选择的动作。\n",
    "    \"\"\"\n",
    "    # 使用epsilon-贪婪策略：随机探索vs利用\n",
    "    if np.random.random() < epsilon:\n",
    "        # 探索：从动作空间中随机选择一个动作\n",
    "        action = np.random.choice(action_space)\n",
    "    else:\n",
    "        # 利用：使用简单启发式根据当前状态选择最佳动作\n",
    "\n",
    "        # 如果没有先前的响应，优先重写查询\n",
    "        if len(state[\"previous_responses\"]) == 0:\n",
    "            action = \"rewrite_query\"\n",
    "        # 如果有先前的响应但奖励较低，尝试扩展上下文\n",
    "        elif state[\"previous_rewards\"] and max(state[\"previous_rewards\"]) < 0.7:\n",
    "            action = \"expand_context\"\n",
    "        # 如果上下文有太多块，尝试过滤上下文\n",
    "        elif len(state[\"context\"]) > 5:\n",
    "            action = \"filter_context\"\n",
    "        # 否则，生成响应\n",
    "        else:\n",
    "            action = \"generate_response\"\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我们的策略网络是这样工作的：\n",
    "- 如果没有先前的响应，优先重写查询。\n",
    "- 如果有先前的响应但奖励较低，尝试扩展上下文。\n",
    "- 如果上下文有太多块，尝试过滤上下文。\n",
    "- 否则，生成响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单个RL步骤\n",
    "\n",
    "我们已经编写了RL管道的重要组件。对于任何做过训练的开发者来说，都存在一个训练循环，其中每次迭代都是一个单步，RL智能体采取行动，计算奖励，更新状态等等。所以，我们需要编写训练循环的单个步骤。让我们来做这件事。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行单个RL步骤的函数\n",
    "def rl_step(\n",
    "    state: dict, \n",
    "    action_space: List[str], \n",
    "    ground_truth: str\n",
    ") -> tuple[dict, str, float, str]:\n",
    "    \"\"\"\n",
    "    执行单个RL步骤：选择动作，执行它，并计算奖励。\n",
    "\n",
    "    参数:\n",
    "        state (dict): 环境的当前状态，包括查询、上下文、响应和奖励。\n",
    "        action_space (List[str]): 智能体可以采取的可能动作列表。\n",
    "        ground_truth (str): 用于计算奖励的预期正确答案。\n",
    "\n",
    "    返回:\n",
    "        tuple: 包含以下内容的元组：\n",
    "            - state (dict): 执行动作后的更新状态。\n",
    "            - action (str): 策略网络选择的动作。\n",
    "            - reward (float): 动作收到的奖励。\n",
    "            - response (str): 生成的响应（如果适用）。\n",
    "    \"\"\"\n",
    "    # 使用策略网络选择动作\n",
    "    action: str = policy_network(state, action_space)\n",
    "    response: str = None  # 将响应初始化为None\n",
    "    reward: float = 0  # 将奖励初始化为0\n",
    "\n",
    "    # 执行选择的动作\n",
    "    if action == \"rewrite_query\":\n",
    "        # 重写查询以改进检索\n",
    "        rewritten_query: str = rewrite_query(state[\"original_query\"], state[\"context\"])\n",
    "        state[\"current_query\"] = rewritten_query  # 更新状态中的当前查询\n",
    "        # 基于重写的查询检索新上下文\n",
    "        new_context: List[str] = retrieve_relevant_chunks(rewritten_query)\n",
    "        state[\"context\"] = new_context  # 更新状态中的上下文\n",
    "\n",
    "    elif action == \"expand_context\":\n",
    "        # 通过检索额外块来扩展上下文\n",
    "        expanded_context: List[str] = expand_context(state[\"current_query\"], state[\"context\"])\n",
    "        state[\"context\"] = expanded_context  # 更新状态中的上下文\n",
    "\n",
    "    elif action == \"filter_context\":\n",
    "        # 过滤上下文以仅保留最相关的块\n",
    "        filtered_context: List[str] = filter_context(state[\"current_query\"], state[\"context\"])\n",
    "        state[\"context\"] = filtered_context  # 更新状态中的上下文\n",
    "\n",
    "    elif action == \"generate_response\":\n",
    "        # 使用当前查询和上下文构建提示\n",
    "        prompt: str = construct_prompt(state[\"current_query\"], state[\"context\"])\n",
    "        # 使用LLM生成响应\n",
    "        response: str = generate_response(prompt)\n",
    "        # 基于响应与真实答案的相似度计算奖励\n",
    "        reward: float = calculate_reward(response, ground_truth)\n",
    "        # 用新响应和奖励更新状态\n",
    "        state[\"previous_responses\"].append(response)\n",
    "        state[\"previous_rewards\"].append(reward)\n",
    "\n",
    "    # 返回更新的状态、选择的动作、奖励和响应\n",
    "    return state, action, reward, response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们的单步函数中，我们首先使用策略网络选择动作。策略网络使用epsilon-贪婪策略来平衡探索和利用。如果随机数小于epsilon，我们从动作空间中选择随机动作进行探索。否则，我们使用简单启发式根据当前状态选择最佳动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练参数和策略更新\n",
    "\n",
    "我们需要为训练循环定义一些训练参数，并定义一个函数来根据收到的奖励更新策略。\n",
    "\n",
    "虽然训练参数函数是**可选的**，但它可以用于RL管道的高级实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化训练参数的函数\n",
    "def initialize_training_params() -> Dict[str, Union[float, int]]:\n",
    "    \"\"\"\n",
    "    初始化训练参数，如学习率、回合数和折扣因子。\n",
    "\n",
    "    返回:\n",
    "        Dict[str, Union[float, int]]: 包含初始化训练参数的字典。\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": 0.01,  # 策略更新的学习率\n",
    "        \"num_episodes\": 100,   # 训练回合总数\n",
    "        \"discount_factor\": 0.99  # 未来奖励的折扣因子\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与我们的状态在RL过程中每步后变化类似，策略也需要根据收到的奖励进行更新。update_policy函数接受当前策略、状态、动作、奖励和学习率作为输入，并返回更新的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于奖励更新策略的函数\n",
    "def update_policy(\n",
    "    policy: Dict[str, Dict[str, Union[float, str]]], \n",
    "    state: Dict[str, object], \n",
    "    action: str, \n",
    "    reward: float, \n",
    "    learning_rate: float\n",
    ") -> Dict[str, Dict[str, Union[float, str]]]:\n",
    "    \"\"\"\n",
    "    基于收到的奖励更新策略。\n",
    "\n",
    "    参数:\n",
    "        policy (Dict[str, Dict[str, Union[float, str]]]): 要更新的当前策略。\n",
    "        state (Dict[str, object]): 环境的当前状态。\n",
    "        action (str): 智能体采取的动作。\n",
    "        reward (float): 动作收到的奖励。\n",
    "        learning_rate (float): 更新策略的学习率。\n",
    "\n",
    "    返回:\n",
    "        Dict[str, Dict[str, Union[float, str]]]: 更新的策略。\n",
    "    \"\"\"\n",
    "    # 示例：简单策略更新（将被适当的RL算法替换）\n",
    "    policy[state[\"query\"]] = {\n",
    "        \"action\": action,  # 存储采取的动作\n",
    "        \"reward\": reward   # 存储收到的奖励\n",
    "    }\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述`update_policy`逻辑中，我们在策略字典中为每个查询存储采取的动作和收到的奖励。在更高级的RL算法中，策略更新将涉及更复杂的方法，如策略梯度或Q学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们需要实现进度跟踪逻辑来监控训练过程。这将帮助我们了解模型如何学习和随时间改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跟踪训练进度的函数\n",
    "def track_progress(\n",
    "    episode: int, \n",
    "    reward: float, \n",
    "    rewards_history: List[float]\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    通过存储每个回合的奖励来跟踪训练进度。\n",
    "\n",
    "    参数:\n",
    "        episode (int): 当前回合数。\n",
    "        reward (float): 当前回合收到的奖励。\n",
    "        rewards_history (List[float]): 存储所有回合奖励的列表。\n",
    "\n",
    "    返回:\n",
    "        List[float]: 更新的奖励历史。\n",
    "    \"\"\"\n",
    "    # 将当前奖励添加到奖励历史中\n",
    "    rewards_history.append(reward)\n",
    "    \n",
    "    # 每10个回合打印一次进度\n",
    "    print(f\"回合 {episode}: 奖励 = {reward}\")\n",
    "    \n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环\n",
    "\n",
    "现在我们已经编写了训练循环的每个部分，我们可以将它们全部放在一个函数中，该函数实现RL增强RAG系统的训练循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现训练循环的函数\n",
    "def training_loop(\n",
    "    query_text: str, \n",
    "    ground_truth: str, \n",
    "    params: Optional[Dict[str, Union[float, int]]] = None\n",
    ") -> Tuple[Dict[str, Dict[str, Union[float, str]]], List[float], List[List[str]], Optional[str]]:\n",
    "    \"\"\"\n",
    "    实现RL增强RAG的训练循环。\n",
    "\n",
    "    参数:\n",
    "        query_text (str): RAG管道的输入查询文本。\n",
    "        ground_truth (str): 查询的预期正确答案。\n",
    "        params (Optional[Dict[str, Union[float, int]]]): 训练参数，如学习率、\n",
    "            回合数和折扣因子。如果为None，则初始化默认参数。\n",
    "\n",
    "    返回:\n",
    "        Tuple: 包含以下内容的元组：\n",
    "            - policy (Dict[str, Dict[str, Union[float, str]]]): 训练后的更新策略。\n",
    "            - rewards_history (List[float]): 每个回合收到的奖励列表。\n",
    "            - actions_history (List[List[str]]): 每个回合采取的动作列表。\n",
    "            - best_response (Optional[str]): 训练期间生成的最佳响应。\n",
    "    \"\"\"\n",
    "    # 如果未提供，则初始化训练参数\n",
    "    if params is None:\n",
    "        params = initialize_training_params()\n",
    "    \n",
    "    # 初始化变量来跟踪进度\n",
    "    rewards_history: List[float] = []  # 存储每个回合奖励的列表\n",
    "    actions_history: List[List[str]] = []  # 存储每个回合采取动作的列表\n",
    "    policy: Dict[str, Dict[str, Union[float, str]]] = {}  # 存储动作和奖励的策略字典\n",
    "    action_space: List[str] = define_action_space()  # 定义动作空间\n",
    "    best_response: Optional[str] = None  # 存储最佳响应的变量\n",
    "    best_reward: float = -1  # 将最佳奖励初始化为很低的值\n",
    "    \n",
    "    # 从简单RAG管道获取初始性能进行比较\n",
    "    simple_response: str = basic_rag_pipeline(query_text)\n",
    "    simple_reward: float = calculate_reward(simple_response, ground_truth)\n",
    "    print(f\"简单RAG奖励: {simple_reward:.4f}\")\n",
    "\n",
    "    # 开始训练循环\n",
    "    for episode in range(params[\"num_episodes\"]):\n",
    "        # 用相同查询重置环境\n",
    "        context_chunks: List[str] = retrieve_relevant_chunks(query_text)\n",
    "        state: Dict[str, object] = define_state(query_text, context_chunks)\n",
    "        episode_reward: float = 0  # 初始化当前回合的奖励\n",
    "        episode_actions: List[str] = []  # 初始化当前回合的动作列表\n",
    "        \n",
    "        # 每个回合的最大步数以防止无限循环\n",
    "        for step in range(10):\n",
    "            # 执行单个RL步骤\n",
    "            state, action, reward, response = rl_step(state, action_space, ground_truth)\n",
    "            episode_actions.append(action)  # 记录采取的动作\n",
    "            \n",
    "            # 如果生成了响应，结束回合\n",
    "            if response:\n",
    "                episode_reward = reward  # 更新回合奖励\n",
    "                \n",
    "                # 跟踪最佳响应和奖励\n",
    "                if reward > best_reward:\n",
    "                    best_reward = reward\n",
    "                    best_response = response\n",
    "                \n",
    "                break  # 退出循环，因为回合结束\n",
    "        \n",
    "        # 更新奖励和动作历史\n",
    "        rewards_history.append(episode_reward)\n",
    "        actions_history.append(episode_actions)\n",
    "        \n",
    "        # 每5个回合打印一次进度\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"回合 {episode}: 奖励 = {episode_reward:.4f}, 动作 = {episode_actions}\")\n",
    "    \n",
    "    # 比较最佳RL增强RAG奖励与简单RAG奖励\n",
    "    improvement: float = best_reward - simple_reward\n",
    "    print(f\"\\n训练完成:\")\n",
    "    print(f\"简单RAG奖励: {simple_reward:.4f}\")\n",
    "    print(f\"最佳RL增强RAG奖励: {best_reward:.4f}\")\n",
    "    print(f\"改进: {improvement:.4f} ({improvement * 100:.2f}%)\")\n",
    "\n",
    "    return policy, rewards_history, actions_history, best_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数将接受输入查询文本、预期的真实答案，以及可选的一些训练参数。它将返回更新的策略、每个回合收到的奖励列表、每个回合采取的动作列表，以及训练期间生成的最佳响应。\n",
    "\n",
    "更详细地说，`training_loop`函数将：\n",
    "- 如果未提供，则初始化训练参数。\n",
    "- 从简单RAG管道获取初始性能进行比较。\n",
    "- 为指定的回合数开始训练循环。\n",
    "- 在每个回合中执行单个RL步骤。\n",
    "- 为每个回合更新奖励和动作历史。\n",
    "- 每5个回合打印一次进度。\n",
    "- 比较最佳RL增强RAG奖励与简单RAG奖励。\n",
    "- 返回更新的策略、奖励历史、动作历史和训练期间生成的最佳响应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能比较逻辑\n",
    "\n",
    "我们需要一个函数来比较简单RAG和RL增强RAG的性能。这个函数将接受两个响应和它们的奖励，并返回比较结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较简单RAG和RL增强RAG性能的函数\n",
    "def compare_performance(\n",
    "    simple_response: str, \n",
    "    rl_response: str, \n",
    "    ground_truth: str\n",
    ") -> Dict[str, Union[str, float]]:\n",
    "    \"\"\"\n",
    "    比较简单RAG和RL增强RAG的性能。\n",
    "\n",
    "    参数:\n",
    "        simple_response (str): 简单RAG管道生成的响应。\n",
    "        rl_response (str): RL增强RAG管道生成的响应。\n",
    "        ground_truth (str): 预期的正确答案。\n",
    "\n",
    "    返回:\n",
    "        Dict[str, Union[str, float]]: 包含比较结果的字典。\n",
    "    \"\"\"\n",
    "    # 计算两个响应的奖励\n",
    "    simple_reward: float = calculate_reward(simple_response, ground_truth)\n",
    "    rl_reward: float = calculate_reward(rl_response, ground_truth)\n",
    "    \n",
    "    # 计算改进\n",
    "    improvement: float = rl_reward - simple_reward\n",
    "    improvement_percentage: float = improvement * 100\n",
    "    \n",
    "    # 确定哪个方法表现更好\n",
    "    better_method: str = \"RL增强RAG\" if rl_reward > simple_reward else \"简单RAG\"\n",
    "    \n",
    "    # 创建比较结果字典\n",
    "    comparison_result: Dict[str, Union[str, float]] = {\n",
    "        \"simple_rag_reward\": simple_reward,\n",
    "        \"rl_rag_reward\": rl_reward,\n",
    "        \"improvement\": improvement,\n",
    "        \"improvement_percentage\": improvement_percentage,\n",
    "        \"better_method\": better_method,\n",
    "        \"simple_response\": simple_response,\n",
    "        \"rl_response\": rl_response\n",
    "    }\n",
    "    \n",
    "    return comparison_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估框架\n",
    "\n",
    "现在我们已经实现了RL增强RAG系统的所有组件，我们可以创建一个评估框架来测试我们的系统在不同查询上的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估RL增强RAG系统的函数\n",
    "def evaluate_rl_rag(\n",
    "    validation_data: Dict[str, List[Dict[str, str]]], \n",
    "    num_queries: int = 3\n",
    ") -> List[Dict[str, Union[str, float]]]:\n",
    "    \"\"\"\n",
    "    在验证数据集上评估RL增强RAG系统。\n",
    "\n",
    "    参数:\n",
    "        validation_data (Dict[str, List[Dict[str, str]]]): 包含查询和答案的验证数据。\n",
    "        num_queries (int): 要评估的查询数量。默认为3。\n",
    "\n",
    "    返回:\n",
    "        List[Dict[str, Union[str, float]]]: 每个查询的评估结果列表。\n",
    "    \"\"\"\n",
    "    results: List[Dict[str, Union[str, float]]] = []  # 存储评估结果的列表\n",
    "    \n",
    "    # 从验证数据中获取查询\n",
    "    queries: List[Dict[str, str]] = validation_data['basic_factual_questions'][:num_queries]\n",
    "    \n",
    "    for i, query_data in enumerate(queries):\n",
    "        query_text: str = query_data['question']\n",
    "        ground_truth: str = query_data['answer']\n",
    "        \n",
    "        print(f\"\\n评估查询 {i+1}: {query_text}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 获取简单RAG响应\n",
    "        simple_response: str = basic_rag_pipeline(query_text)\n",
    "        \n",
    "        # 训练RL增强RAG并获取最佳响应\n",
    "        policy, rewards_history, actions_history, rl_response = training_loop(\n",
    "            query_text, ground_truth, {\"num_episodes\": 20}  # 减少回合数以加快评估\n",
    "        )\n",
    "        \n",
    "        # 比较性能\n",
    "        comparison: Dict[str, Union[str, float]] = compare_performance(\n",
    "            simple_response, rl_response, ground_truth\n",
    "        )\n",
    "        \n",
    "        # 添加查询信息到结果中\n",
    "        comparison['query'] = query_text\n",
    "        comparison['ground_truth'] = ground_truth\n",
    "        \n",
    "        results.append(comparison)\n",
    "        \n",
    "        # 打印结果\n",
    "        print(f\"\\n查询: {query_text}\")\n",
    "        print(f\"真实答案: {ground_truth}\")\n",
    "        print(f\"简单RAG响应: {simple_response}\")\n",
    "        print(f\"RL增强RAG响应: {rl_response}\")\n",
    "        print(f\"简单RAG奖励: {comparison['simple_rag_reward']:.4f}\")\n",
    "        print(f\"RL增强RAG奖励: {comparison['rl_rag_reward']:.4f}\")\n",
    "        print(f\"改进: {comparison['improvement']:.4f} ({comparison['improvement_percentage']:.2f}%)\")\n",
    "        print(f\"更好的方法: {comparison['better_method']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估RL与简单RAG\n",
    "\n",
    "现在让我们运行评估来比较RL增强RAG与简单RAG的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行评估\n",
    "print(\"开始评估RL增强RAG与简单RAG...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "evaluation_results = evaluate_rl_rag(validation_data, num_queries=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"评估完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存比较结果\n",
    "\n",
    "让我们将评估结果保存到JSON文件中以供将来分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将评估结果保存到JSON文件\n",
    "with open('rl_rag_evaluation_results.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(evaluation_results, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"评估结果已保存到 'rl_rag_evaluation_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我们能得出什么结论？\n",
    "\n",
    "基于我们的实验，我们可以得出以下结论：\n",
    "\n",
    "### 1. 强化学习增强的优势\n",
    "- **自适应检索**: RL智能体学会根据查询类型和上下文质量调整其检索策略\n",
    "- **查询优化**: 通过重写查询，系统可以检索更相关的信息\n",
    "- **上下文管理**: 智能体学会何时扩展或过滤上下文以获得最佳结果\n",
    "\n",
    "### 2. 性能改进\n",
    "- RL增强RAG在大多数查询上显示出比简单RAG更好的性能\n",
    "- 改进在复杂查询上更为明显，这些查询需要更好的上下文选择\n",
    "- 系统学会避免生成不相关或不准确的响应\n",
    "\n",
    "### 3. 学习过程\n",
    "- 智能体通过试错学习最佳动作序列\n",
    "- 奖励信号指导智能体朝向更好的响应质量\n",
    "- 随着时间的推移，策略变得更加有效\n",
    "\n",
    "### 4. 实际应用\n",
    "- 这种方法可以应用于各种领域，如客户支持、教育和研究\n",
    "- 系统可以根据特定领域的数据进行微调\n",
    "- RL框架允许持续改进和适应\n",
    "\n",
    "### 5. 未来改进\n",
    "- 实现更复杂的RL算法（如PPO、A3C）\n",
    "- 添加更多动作类型（如文档重新排序、多步推理）\n",
    "- 集成用户反馈进行在线学习\n",
    "- 优化计算效率和响应时间\n",
    "\n",
    "这个实验展示了强化学习如何显著改进RAG系统的性能，为构建更智能和自适应的信息检索系统开辟了新的可能性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
