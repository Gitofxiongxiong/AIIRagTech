{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用于增强 RAG 系统的查询转换\n",
    "\n",
    "通过三种查询转换技术，在不依赖 LangChain 等专门库的情况下，提升 RAG 系统中的检索性能。通过修改用户查询，我们可以显著提高检索信息的关联性和全面性。\n",
    "\n",
    "## 关键转换方法\n",
    "\n",
    "1.  **查询重写**：使查询更具体、更详细，以提高搜索精度。\n",
    "2.  **退后提示**：生成更广泛的查询，以检索有用的上下文信息。\n",
    "3.  **子查询分解**：将复杂查询分解为更简单的组件，以实现全面检索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 百炼服务的base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请求转换方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一：请求重写\n",
    "def rewrite_query(original_query, model=\"qwen-plus\"):\n",
    "    \"\"\"\n",
    "    重写用户查询以提高搜索结果的准确性。\n",
    "    参数：\n",
    "    original_query (str)：用户的原始查询。\n",
    "    model (str)：用于重写查询的模型名称。\n",
    "\n",
    "    返回：\n",
    "    str：重写后的查询。\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.0,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二：后退提示词\n",
    "def generate_step_back_query(original_query, model=\"qwen-plus\"):\n",
    "    \"\"\"\n",
    "    生成后退提示词\n",
    "    参数：\n",
    "    original_query (str): 原始用户查询\n",
    "    model (str): 用于生成后退提示词的模型\n",
    "\n",
    "    返回：\n",
    "    str: 后退提示词\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n",
    "    \n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a broader, more general version of the following query that could help retrieve useful background information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Step-back query:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法三：分解为子查询\n",
    "\n",
    "def decompose_query(original_query, num_subqueries=4, model=\"qwen-plus\"):\n",
    "    \"\"\"\n",
    "    分解复杂查询为子查询\n",
    "    参数：\n",
    "    original_query (str): 原始查询\n",
    "    num_subqueries (int): 生成的子查询数量\n",
    "    model (str): 用于生成子查询的模型\n",
    "\n",
    "    返回：\n",
    "    list: 生成的子查询列表\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
    "    1. [First sub-query]\n",
    "    2. [Second sub-query]\n",
    "    And so on...\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用查询转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What are the impacts of AI on job automation and employment?\n",
      "\n",
      "1. Rewritten Query:\n",
      "What are the specific impacts of artificial intelligence (AI) on job automation across various industries, and how does it affect employment rates, job displacement, skill requirements, and workforce adaptation? Additionally, what roles do machine learning, natural language processing, and robotics play in accelerating or mitigating these effects, and what strategies are being implemented to address potential economic and social challenges?\n",
      "\n",
      "2. Step-back Query:\n",
      "What are the economic, social, and technological implications of automation and artificial intelligence on workforce dynamics and employment trends?\n",
      "\n",
      "3. Sub-queries:\n",
      "   1. How does AI contribute to job automation in various industries?\n",
      "   2. Which types of jobs are most at risk due to AI-driven automation?\n",
      "   3. How might AI create new employment opportunities or roles?\n",
      "   4. What are the societal and economic implications of AI on overall employment levels?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_query = \"What are the impacts of AI on job automation and employment?\"\n",
    "\n",
    "\n",
    "print(\"Original Query:\", original_query)\n",
    "\n",
    "\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. Rewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. Step-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\n3. Sub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简易向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    简易的向量存储库。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        添加一个新的项到存储库。\n",
    "\n",
    "        参数:\n",
    "        text (str): 文本内容。\n",
    "        embedding (List[float]): 文本的嵌入向量。\n",
    "        metadata (Dict, optional): 与文本相关的元数据。\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入向量最相似的文本。\n",
    "\n",
    "        参数:\n",
    "        query_embedding (List[float]): 查询的嵌入向量。\n",
    "        k (int, optional): 返回最相似的k个结果。\n",
    "\n",
    "        返回:\n",
    "        List[Dict]: 最相似的文本及其相关信息。\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_in_batches(text_chunks, model=\"text-embedding-v3\", batch_size_limit=10): # 我改成了官方模型名，你可以换回 \"text-embedding-v3\"\n",
    "    \"\"\"\n",
    "    调用 OpenAI 的 Embedding API 来创建文本列表的嵌入向量，处理批处理大小限制。\n",
    "\n",
    "    参数:\n",
    "    text_chunks (List[str]): 需要创建嵌入的文本字符串列表。\n",
    "    model (str): 使用的嵌入模型。\n",
    "    batch_size_limit (int): API 允许的最大批处理大小。根据错误信息，这里是10。\n",
    "\n",
    "    返回:\n",
    "    List[List[float]]: 所有文本的嵌入向量列表。\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    if not text_chunks:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(text_chunks, list): # 确保输入是列表\n",
    "        text_chunks = [text_chunks]\n",
    "\n",
    "    for i in range(0, len(text_chunks), batch_size_limit):\n",
    "        batch = text_chunks[i:i + batch_size_limit]\n",
    "        try:\n",
    "            #print(f\"Processing batch {i//batch_size_limit + 1}, size: {len(batch)}\")\n",
    "            response = client.embeddings.create(\n",
    "                input=batch,\n",
    "                model=model,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "            # 从响应中提取该批次的嵌入向量\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting with chunk: '{batch[0][:50]}...'\")\n",
    "            print(f\"API Error: {e}\")\n",
    "\n",
    "            raise e \n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "def create_embeddings(text, model=\"text-embedding-v3\"):\n",
    "    \"\"\"\n",
    "    字符串向量化\n",
    "    参数:\n",
    "    text (str): 需要创建嵌入的文本字符串。\n",
    "    model (str): 使用的嵌入模型。\n",
    "\n",
    "    返回:\n",
    "    List[float]: 文本的嵌入向量。\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从pdf提取文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    提取PDF文件中的文本并打印前`num_chars`个字符。\n",
    "\n",
    "    参数：\n",
    "    pdf_path (str): PDF文件的路径。\n",
    "\n",
    "    返回：\n",
    "    str: 从PDF中提取的文本。\n",
    "\n",
    "    \"\"\"\n",
    "    # 打开PDF文件\n",
    "    mypdf = pymupdf.open(pdf_path)\n",
    "    all_text = \"\"  # 初始化一个空字符串来存储提取的文本\n",
    "\n",
    "    # 迭代PDF中的每个页面\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 获取页面\n",
    "        text = page.get_text(\"text\")  # 从页面中提取文本\n",
    "        all_text += text  # 将提取的文本附加到all_text字符串\n",
    "\n",
    "    return all_text  # 返回提取的文本\n",
    "\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    将文本分割为多个块，每个块的大小为n，重叠部分为overlap。\n",
    "    参数：\n",
    "    text: 输入的文本\n",
    "    n: 每个块的大小\n",
    "    overlap: 相邻块之间的重叠部分大小\n",
    "\n",
    "    返回：\n",
    "    文本块列表\n",
    "    \"\"\"\n",
    "    chunks = []  \n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        \n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化并存储文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    处理文档以进行 RAG。\n",
    "\n",
    "    参数：\n",
    "    pdf_path (str): PDF 文件的路径。\n",
    "    chunk_size (int): 每个块的字符数。\n",
    "    chunk_overlap (int): 块之间的字符重叠。\n",
    "\n",
    "    返回：\n",
    "    SimpleVectorStore: 包含文档块及其嵌入的向量存储。\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    \n",
    "    chunk_embeddings = create_embeddings_in_batches(chunks)\n",
    "    \n",
    "    \n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    \n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用查询变换进行检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    查询转换后的query\n",
    "    \"\"\"\n",
    "    print(f\"Transformation type: {transformation_type}\")\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        \n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"Rewritten query: {transformed_query}\")\n",
    "        \n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        \n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"Step-back query: {transformed_query}\")\n",
    "    \n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        \n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"Decomposed into sub-queries:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "        \n",
    "        print(\"sub_queries:\\n\", sub_queries)\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        print(\"sub_query_embeddings:\\n\", sub_query_embeddings)\n",
    "        \n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            print(f\"Sub-query {i+1} embedding: {embedding}\")\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2) \n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        \n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "        \n",
    "        \n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"qwen3-4b\"):\n",
    "    \n",
    "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please provide a comprehensive answer based only on the context above.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "         extra_body={\"enable_thinking\": False}\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整RAG流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
    "    \n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    if transformation_type:\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_responses(results, reference_answer, model=\"qwen-plus\"):\n",
    "    \"\"\"\n",
    "    比较不同查询转换技术的响应并评估它们的性能。\n",
    "\n",
    "    参数：\n",
    "    results (dict): 包含不同查询转换技术的响应的字典。\n",
    "    reference_answer (str): 参考答案。\n",
    "    model (str): 用于生成比较结果的模型。\n",
    "\n",
    "    返回：\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. \n",
    "    Your task is to compare different responses generated using various query transformation techniques \n",
    "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
    "    \n",
    "\n",
    "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
    "    \n",
    "    for technique, result in results.items():\n",
    "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n",
    "    \n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "    \n",
    "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
    "    \n",
    "    For each technique (original, rewrite, step_back, decompose):\n",
    "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
    "    2. Identify strengths and weaknesses\n",
    "    \n",
    "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    评估不同的查询转换技术。\n",
    "\n",
    "    参数：\n",
    "        pdf_path (str): PDF文档的路径\n",
    "        query (str): 要评估的查询\n",
    "        reference_answer (str): 可选的参考答案用于比较\n",
    "\n",
    "    返回：\n",
    "        Dict: 评估结果\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "    \n",
    "\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
    "        \n",
    "\n",
    "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "        \n",
    "\n",
    "        print(f\"Response with {type_name} query:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Response with original query:\n",
      "Explainable AI (XAI) is a set of techniques designed to make AI decision-making processes more understandable and transparent. The goal of XAI is to provide insights into how AI models arrive at their decisions, which helps users assess the reliability, fairness, and accuracy of these decisions. This transparency is crucial for building trust in AI systems, as it allows users to understand the rationale behind AI outputs and verify that they are making fair and accurate judgments.\n",
      "\n",
      "XAI is considered important for several reasons. Firstly, it enhances accountability and responsibility by making AI systems more transparent, which is essential for addressing potential harms and ensuring ethical behavior. Secondly, it supports the principles of fairness and accuracy by enabling users to evaluate the decisions made by AI systems. Lastly, XAI contributes to the broader goal of building trust in AI, which is vital for the responsible development and deployment of AI technologies across various domains.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Rewritten query: What is Explainable AI (XAI), and how does it differ from traditional AI in terms of transparency and interpretability? What are the key techniques and methodologies used in XAI, such as Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive exPlanations (SHAP)? Why is Explainable AI considered crucial in domains like healthcare, finance, and autonomous systems, where trust, accountability, and regulatory compliance are paramount? Additionally, what challenges are associated with implementing XAI, and how do these challenges impact its adoption across various industries?\n",
      "Response with rewrite query:\n",
      "Explainable AI (XAI) is a set of techniques designed to make AI decision-making processes more understandable and transparent. The goal of XAI is to provide insights into how AI models arrive at their decisions, which helps users assess the reliability, fairness, and accuracy of these decisions. This transparency is crucial for building trust in AI systems, as it allows users to understand the rationale behind AI outputs and verify that they are making fair and accurate judgments.\n",
      "\n",
      "XAI is considered important for several reasons. Firstly, it enhances accountability and responsibility by making AI systems more interpretable, which is essential for addressing potential harms and ensuring ethical behavior. Secondly, it supports transparency and explainability, which are key to building trust in AI. By making AI systems understandable, users can better assess their reliability and fairness. Additionally, XAI contributes to the responsible use of AI by enabling users to understand how AI systems operate, which is particularly important in contexts where AI decisions have significant consequences, such as in healthcare, law enforcement, and environmental monitoring. Overall, XAI plays a vital role in ensuring that AI systems are not only effective but also trustworthy and ethically sound.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Step-back query: What are the key concepts and significance of transparency and interpretability in artificial intelligence systems?\n",
      "Response with step_back query:\n",
      "Explainable AI (XAI) refers to techniques designed to make AI decisions more understandable. It aims to provide insights into the decision-making processes of AI systems, enabling users to assess the fairness and accuracy of these decisions. XAI is considered important because it contributes to building trust in AI systems by making them more transparent and reliable. By allowing users to understand how AI arrives at its conclusions, XAI helps in evaluating the reliability and fairness of AI decisions, which is crucial for ensuring ethical behavior and accountability in the use of AI technologies. Additionally, explainability is a key component in addressing bias in AI systems and ensuring that AI systems align with societal values.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What is 'Explainable AI' and why is it considered important?\n",
      "Decomposed into sub-queries:\n",
      "1. What is the definition of Explainable AI?\n",
      "2. How does Explainable AI differ from traditional AI?\n",
      "3. Why is transparency in AI decision-making important?\n",
      "4. In which real-world applications is Explainable AI particularly valuable?\n",
      "sub_queries type: <class 'list'>\n",
      "sub_query_embeddings type: <class 'list'>\n",
      "Sub-query 1 embedding: -0.049844685941934586\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/AI_Information.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_answer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m, in \u001b[0;36mevaluate_transformations\u001b[1;34m(pdf_path, query, reference_answer)\u001b[0m\n\u001b[0;32m     20\u001b[0m type_name \u001b[38;5;241m=\u001b[39m transformation_type \u001b[38;5;28;01mif\u001b[39;00m transformation_type \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Running RAG with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m query =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_with_query_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m results[type_name] \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m query:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mrag_with_query_transformation\u001b[1;34m(pdf_path, query, transformation_type)\u001b[0m\n\u001b[0;32m      3\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m process_document(pdf_path)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformation_type:\n\u001b[1;32m----> 6\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m create_embeddings(query)\n",
      "Cell \u001b[1;32mIn[24], line 42\u001b[0m, in \u001b[0;36mtransformed_search\u001b[1;34m(query, vector_store, transformation_type, top_k)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sub_query_embeddings):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSub-query \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embedding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m     sub_results \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     43\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mextend(sub_results)\n\u001b[0;32m     46\u001b[0m seen_texts \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m, in \u001b[0;36mSimpleVectorStore.similarity_search\u001b[1;34m(self, query_embedding, k)\u001b[0m\n\u001b[0;32m     44\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(query_vector, vector) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(query_vector) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(vector))\n\u001b[0;32m     45\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend((i, similarity))\n\u001b[1;32m---> 48\u001b[0m \u001b[43msimilarities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(k, \u001b[38;5;28mlen\u001b[39m(similarities))):\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Extract the reference answer from the validation data\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_transformations(pdf_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
