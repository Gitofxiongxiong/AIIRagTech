{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Graph RAG: 图增强检索增强生成\n",
    "\n",
    "在这个笔记本中，我实现了 Graph RAG - 一种通过将知识组织为连接图而不是平面文档集合来增强传统 RAG 系统的技术。这使得系统能够导航相关概念并检索比标准向量相似性方法更具上下文相关性的信息。\n",
    "\n",
    "Graph RAG 的主要优势\n",
    "\n",
    "- 保留信息片段之间的关系\n",
    "- 能够通过连接的概念进行遍历以找到相关上下文\n",
    "- 改善对复杂、多部分查询的处理\n",
    "- 通过可视化知识路径提供更好的可解释性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置环境\n",
    "我们首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置 OpenAI API 客户端\n",
    "我们初始化 OpenAI 客户端来生成嵌入向量和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用基础 URL 和 API 密钥初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 从环境变量中获取 API 密钥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    从 PDF 文件中提取文本内容。\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 文件路径\n",
    "        \n",
    "    Returns:\n",
    "        str: 提取的文本内容\n",
    "    \"\"\"\n",
    "    print(f\"正在从 {pdf_path} 提取文本...\")  # 打印正在处理的 PDF 路径\n",
    "    pdf_document = fitz.open(pdf_path)  # 使用 PyMuPDF 打开 PDF 文件\n",
    "    text = \"\"  # 初始化空字符串来存储提取的文本\n",
    "    \n",
    "    # 遍历 PDF 中的每一页\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]  # 获取页面对象\n",
    "        text += page.get_text()  # 从页面提取文本并追加到文本字符串中\n",
    "    \n",
    "    return text  # 返回提取的文本内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    将文本分割为重叠的块。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 要分块的输入文本\n",
    "        chunk_size (int): 每个块的字符大小\n",
    "        overlap (int): 块之间的重叠字符数\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 包含元数据的块列表\n",
    "    \"\"\"\n",
    "    chunks = []  # 初始化空列表来存储块\n",
    "    \n",
    "    # 以 (chunk_size - overlap) 的步长遍历文本\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        # 从当前位置提取一个文本块\n",
    "        chunk_text = text[i:i + chunk_size]\n",
    "        \n",
    "        # 确保不添加空块\n",
    "        if chunk_text:\n",
    "            # 将块及其元数据追加到列表中\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,  # 文本块\n",
    "                \"index\": len(chunks),  # 块的索引\n",
    "                \"start_pos\": i,  # 块在原始文本中的起始位置\n",
    "                \"end_pos\": i + len(chunk_text)  # 块在原始文本中的结束位置\n",
    "            })\n",
    "    \n",
    "    # 打印创建的块数量\n",
    "    print(f\"创建了 {len(chunks)} 个文本块\")\n",
    "    \n",
    "    return chunks  # 返回块列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    为给定的文本创建嵌入向量。\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): 输入文本\n",
    "        model (str): 嵌入模型名称\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: 嵌入向量\n",
    "    \"\"\"\n",
    "    # 处理空输入\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # 如果需要，分批处理（OpenAI API 限制）\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # 分批遍历输入文本\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # 获取当前批次的文本\n",
    "        \n",
    "        # 为当前批次创建嵌入向量\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # 从响应中提取嵌入向量\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # 将批次嵌入向量添加到列表中\n",
    "    \n",
    "    return all_embeddings  # 返回所有嵌入向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 知识图谱构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_concepts(text):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的 API 从文本中提取关键概念。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 要提取概念的文本\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: 概念列表\n",
    "    \"\"\"\n",
    "    # 指导模型执行任务的系统消息\n",
    "    system_message = \"\"\"从提供的文本中提取关键概念和实体。\n",
    "只返回 5-10 个在此文本中最重要的关键术语、实体或概念的列表。\n",
    "将您的响应格式化为字符串的 JSON 数组。\"\"\"\n",
    "\n",
    "    # 向 OpenAI API 发出请求\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"从以下内容中提取关键概念：\\n\\n{text[:3000]}\"}  # API 限制\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 从响应中解析概念\n",
    "        concepts_json = json.loads(response.choices[0].message.content)\n",
    "        concepts = concepts_json.get(\"concepts\", [])\n",
    "        if not concepts and \"concepts\" not in concepts_json:\n",
    "            # 尝试获取响应中的任何数组\n",
    "            for key, value in concepts_json.items():\n",
    "                if isinstance(value, list):\n",
    "                    concepts = value\n",
    "                    break\n",
    "        return concepts\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        # 如果 JSON 解析失败的回退方案\n",
    "        content = response.choices[0].message.content\n",
    "        # 尝试提取任何看起来像列表的内容\n",
    "        matches = re.findall(r'\\[(.*?)\\]', content, re.DOTALL)\n",
    "        if matches:\n",
    "            items = re.findall(r'\"([^\"]*)\"', matches[0])\n",
    "            return items\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(chunks):\n",
    "    \"\"\"\n",
    "    从文本块构建知识图谱。\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): 包含元数据的文本块列表\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[nx.Graph, List[np.ndarray]]: 知识图谱和块嵌入向量\n",
    "    \"\"\"\n",
    "    print(\"正在构建知识图谱...\")\n",
    "    \n",
    "    # 创建图\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # 提取块文本\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    \n",
    "    # 为所有块创建嵌入向量\n",
    "    print(\"正在为块创建嵌入向量...\")\n",
    "    embeddings = create_embeddings(texts)\n",
    "    \n",
    "    # 向图中添加节点\n",
    "    print(\"正在向图中添加节点...\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # 从块中提取概念\n",
    "        print(f\"正在为块 {i+1}/{len(chunks)} 提取概念...\")\n",
    "        concepts = extract_concepts(chunk[\"text\"])\n",
    "        \n",
    "        # 添加带有属性的节点\n",
    "        graph.add_node(i, \n",
    "                      text=chunk[\"text\"], \n",
    "                      concepts=concepts,\n",
    "                      embedding=embeddings[i])\n",
    "    \n",
    "    # 基于共享概念连接节点\n",
    "    print(\"正在创建节点之间的边...\")\n",
    "    for i in range(len(chunks)):\n",
    "        node_concepts = set(graph.nodes[i][\"concepts\"])\n",
    "        \n",
    "        for j in range(i + 1, len(chunks)):\n",
    "            # 计算概念重叠\n",
    "            other_concepts = set(graph.nodes[j][\"concepts\"])\n",
    "            shared_concepts = node_concepts.intersection(other_concepts)\n",
    "            \n",
    "            # 如果它们共享概念，添加一条边\n",
    "            if shared_concepts:\n",
    "                # 使用嵌入向量计算语义相似性\n",
    "                similarity = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "                \n",
    "                # 基于概念重叠和语义相似性计算边权重\n",
    "                concept_score = len(shared_concepts) / min(len(node_concepts), len(other_concepts))\n",
    "                edge_weight = 0.7 * similarity + 0.3 * concept_score\n",
    "                \n",
    "                # 只添加具有显著关系的边\n",
    "                if edge_weight > 0.6:\n",
    "                    graph.add_edge(i, j, \n",
    "                                  weight=edge_weight,\n",
    "                                  similarity=similarity,\n",
    "                                  shared_concepts=list(shared_concepts))\n",
    "    \n",
    "    print(f\"知识图谱构建完成，包含 {graph.number_of_nodes()} 个节点和 {graph.number_of_edges()} 条边\")\n",
    "    return graph, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图遍历和查询处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_graph(query, graph, embeddings, top_k=5, max_depth=3):\n",
    "    \"\"\"\n",
    "    遍历知识图谱以找到查询的相关信息。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户的问题\n",
    "        graph (nx.Graph): 知识图谱\n",
    "        embeddings (List): 节点嵌入向量列表\n",
    "        top_k (int): 要考虑的初始节点数量\n",
    "        max_depth (int): 最大遍历深度\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 从图遍历中获得的相关信息\n",
    "    \"\"\"\n",
    "    print(f\"正在为查询遍历图：{query}\")\n",
    "    \n",
    "    # 获取查询嵌入向量\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # 计算查询与所有节点之间的相似性\n",
    "    similarities = []\n",
    "    for i, node_embedding in enumerate(embeddings):\n",
    "        similarity = np.dot(query_embedding, node_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))\n",
    "        similarities.append((i, similarity))\n",
    "    \n",
    "    # 按相似性排序（降序）\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 获取前 k 个最相似的节点作为起始点\n",
    "    starting_nodes = [node for node, _ in similarities[:top_k]]\n",
    "    print(f\"从 {len(starting_nodes)} 个节点开始遍历\")\n",
    "    \n",
    "    # 初始化遍历\n",
    "    visited = set()  # 用于跟踪已访问节点的集合\n",
    "    traversal_path = []  # 存储遍历路径的列表\n",
    "    results = []  # 存储结果的列表\n",
    "    \n",
    "    # 使用优先队列进行遍历\n",
    "    queue = []\n",
    "    for node in starting_nodes:\n",
    "        heapq.heappush(queue, (-similarities[node][1], node))  # 负数用于最大堆\n",
    "    \n",
    "    # 使用带优先级的修改广度优先搜索遍历图\n",
    "    while queue and len(results) < (top_k * 3):  # 将结果限制为 top_k * 3\n",
    "        _, node = heapq.heappop(queue)\n",
    "        \n",
    "        if node in visited:\n",
    "            continue\n",
    "        \n",
    "        # 标记为已访问\n",
    "        visited.add(node)\n",
    "        traversal_path.append(node)\n",
    "        \n",
    "        # 将当前节点的文本添加到结果中\n",
    "        results.append({\n",
    "            \"text\": graph.nodes[node][\"text\"],\n",
    "            \"concepts\": graph.nodes[node][\"concepts\"],\n",
    "            \"node_id\": node\n",
    "        })\n",
    "        \n",
    "        # 如果我们还没有达到最大深度，探索邻居\n",
    "        if len(traversal_path) < max_depth:\n",
    "            neighbors = [(neighbor, graph[node][neighbor][\"weight\"]) \n",
    "                        for neighbor in graph.neighbors(node)\n",
    "                        if neighbor not in visited]\n",
    "            \n",
    "            # 基于边权重将邻居添加到队列中\n",
    "            for neighbor, weight in sorted(neighbors, key=lambda x: x[1], reverse=True):\n",
    "                heapq.heappush(queue, (-weight, neighbor))\n",
    "    \n",
    "    print(f\"图遍历找到了 {len(results)} 个相关块\")\n",
    "    return results, traversal_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 响应生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context_chunks):\n",
    "    \"\"\"\n",
    "    使用检索到的上下文生成响应。\n",
    "    \n",
    "    Args:\n",
    "        query (str): 用户的问题\n",
    "        context_chunks (List[Dict]): 从图遍历中获得的相关块\n",
    "        \n",
    "    Returns:\n",
    "        str: 生成的响应\n",
    "    \"\"\"\n",
    "    # 从上下文中的每个块提取文本\n",
    "    context_texts = [chunk[\"text\"] for chunk in context_chunks]\n",
    "    \n",
    "    # 将提取的文本合并为单个上下文字符串，用 \"---\" 分隔\n",
    "    combined_context = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    \n",
    "    # 定义上下文的最大允许长度（OpenAI 限制）\n",
    "    max_context = 14000\n",
    "    \n",
    "    # 如果合并的上下文超过最大长度，则截断\n",
    "    if len(combined_context) > max_context:\n",
    "        combined_context = combined_context[:max_context] + \"... [已截断]\"\n",
    "    \n",
    "    # 定义指导 AI 助手的系统消息\n",
    "    system_message = \"\"\"您是一个有用的 AI 助手。根据提供的上下文回答用户的问题。\n",
    "如果信息不在上下文中，请说明。在可能的情况下，在您的答案中引用上下文的特定部分。\"\"\"\n",
    "\n",
    "    # 使用 OpenAI API 生成响应\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # 指定要使用的模型\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # 指导助手的系统消息\n",
    "            {\"role\": \"user\", \"content\": f\"上下文：\\n{combined_context}\\n\\n问题：{query}\"}  # 包含上下文和查询的用户消息\n",
    "        ],\n",
    "        temperature=0.2  # 设置响应生成的温度\n",
    "    )\n",
    "    \n",
    "    # 返回生成的响应内容\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph_traversal(graph, traversal_path):\n",
    "    \"\"\"\n",
    "    可视化知识图谱和遍历路径。\n",
    "    \n",
    "    Args:\n",
    "        graph (nx.Graph): 知识图谱\n",
    "        traversal_path (List): 按遍历顺序的节点列表\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))  # 设置图形大小\n",
    "    \n",
    "    # 定义节点颜色，默认为浅蓝色\n",
    "    node_color = ['lightblue'] * graph.number_of_nodes()\n",
    "    \n",
    "    # 将遍历路径节点高亮为浅绿色\n",
    "    for node in traversal_path:\n",
    "        node_color[node] = 'lightgreen'\n",
    "    \n",
    "    # 将起始节点高亮为绿色，结束节点高亮为红色\n",
    "    if traversal_path:\n",
    "        node_color[traversal_path[0]] = 'green'\n",
    "        node_color[traversal_path[-1]] = 'red'\n",
    "    \n",
    "    # 使用弹簧布局为所有节点创建位置\n",
    "    pos = nx.spring_layout(graph, k=0.5, iterations=50, seed=42)\n",
    "    \n",
    "    # 绘制图节点\n",
    "    nx.draw_networkx_nodes(graph, pos, node_color=node_color, node_size=500, alpha=0.8)\n",
    "    \n",
    "    # 绘制边，宽度与权重成比例\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        weight = data.get('weight', 1.0)\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=[(u, v)], width=weight*2, alpha=0.6)\n",
    "    \n",
    "    # 用红色虚线绘制遍历路径\n",
    "    traversal_edges = [(traversal_path[i], traversal_path[i+1]) \n",
    "                      for i in range(len(traversal_path)-1)]\n",
    "    \n",
    "    nx.draw_networkx_edges(graph, pos, edgelist=traversal_edges, \n",
    "                          width=3, alpha=0.8, edge_color='red', \n",
    "                          style='dashed', arrows=True)\n",
    "    \n",
    "    # 为每个节点添加第一个概念的标签\n",
    "    labels = {}\n",
    "    for node in graph.nodes():\n",
    "        concepts = graph.nodes[node]['concepts']\n",
    "        label = concepts[0] if concepts else f\"节点 {node}\"\n",
    "        labels[node] = f\"{node}: {label}\"\n",
    "    \n",
    "    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=8)\n",
    "    \n",
    "    plt.title(\"带遍历路径的知识图谱\")  # 设置图标题\n",
    "    plt.axis('off')  # 关闭坐标轴\n",
    "    plt.tight_layout()  # 调整布局\n",
    "    plt.show()  # 显示图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的 Graph RAG 流水线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_rag_pipeline(pdf_path, query, chunk_size=1000, chunk_overlap=200, top_k=3):\n",
    "    \"\"\"\n",
    "    从文档到答案的完整 Graph RAG 流水线。\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 文档路径\n",
    "        query (str): 用户的问题\n",
    "        chunk_size (int): 文本块大小\n",
    "        chunk_overlap (int): 块之间的重叠\n",
    "        top_k (int): 遍历时要考虑的顶级节点数量\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 包括答案和图可视化数据的结果\n",
    "    \"\"\"\n",
    "    # 从 PDF 文档中提取文本\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 将提取的文本分割为重叠的块\n",
    "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # 从文本块构建知识图谱\n",
    "    graph, embeddings = build_knowledge_graph(chunks)\n",
    "    \n",
    "    # 遍历知识图谱以找到查询的相关信息\n",
    "    relevant_chunks, traversal_path = traverse_graph(query, graph, embeddings, top_k)\n",
    "    \n",
    "    # 基于查询和相关块生成响应\n",
    "    response = generate_response(query, relevant_chunks)\n",
    "    \n",
    "    # 可视化图遍历路径\n",
    "    visualize_graph_traversal(graph, traversal_path)\n",
    "    \n",
    "    # 返回查询、响应、相关块、遍历路径和图\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevant_chunks\": relevant_chunks,\n",
    "        \"traversal_path\": traversal_path,\n",
    "        \"graph\": graph\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_graph_rag(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    在多个测试查询上评估 Graph RAG。\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF 文档路径\n",
    "        test_queries (List[str]): 测试查询列表\n",
    "        reference_answers (List[str], optional): 用于比较的参考答案\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 评估结果\n",
    "    \"\"\"\n",
    "    # 从 PDF 中提取文本\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 将文本分割为块\n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    # 构建知识图谱（对所有查询只做一次）\n",
    "    graph, embeddings = build_knowledge_graph(chunks)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== 评估查询 {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"查询：{query}\")\n",
    "        \n",
    "        # 遍历图以找到相关信息\n",
    "        relevant_chunks, traversal_path = traverse_graph(query, graph, embeddings)\n",
    "        \n",
    "        # 生成响应\n",
    "        response = generate_response(query, relevant_chunks)\n",
    "        \n",
    "        # 如果有参考答案，与之比较\n",
    "        reference = None\n",
    "        comparison = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "            comparison = compare_with_reference(response, reference, query)\n",
    "        \n",
    "        # 为当前查询追加结果\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison,\n",
    "            \"traversal_path_length\": len(traversal_path),\n",
    "            \"relevant_chunks_count\": len(relevant_chunks)\n",
    "        })\n",
    "        \n",
    "        # 显示结果\n",
    "        print(f\"\\n响应：{response}\\n\")\n",
    "        if comparison:\n",
    "            print(f\"比较：{comparison}\\n\")\n",
    "    \n",
    "    # 返回评估结果和图统计信息\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"graph_stats\": {\n",
    "            \"nodes\": graph.number_of_nodes(),\n",
    "            \"edges\": graph.number_of_edges(),\n",
    "            \"avg_degree\": sum(dict(graph.degree()).values()) / graph.number_of_nodes()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_reference(response, reference, query):\n",
    "    \"\"\"\n",
    "    将生成的响应与参考答案进行比较。\n",
    "    \n",
    "    Args:\n",
    "        response (str): 生成的响应\n",
    "        reference (str): 参考答案\n",
    "        query (str): 原始查询\n",
    "        \n",
    "    Returns:\n",
    "        str: 比较分析\n",
    "    \"\"\"\n",
    "    # 指导模型如何比较响应的系统消息\n",
    "    system_message = \"\"\"比较 AI 生成的响应与参考答案。\n",
    "基于以下方面进行评估：正确性、完整性和与查询的相关性。\n",
    "提供简要分析（2-3 句话），说明生成的响应与参考答案的匹配程度。\"\"\"\n",
    "\n",
    "    # 构建包含查询、AI 生成响应和参考答案的提示\n",
    "    prompt = f\"\"\"\n",
    "查询：{query}\n",
    "\n",
    "AI 生成的响应：\n",
    "{response}\n",
    "\n",
    "参考答案：\n",
    "{reference}\n",
    "\n",
    "AI 响应与参考答案的匹配程度如何？\n",
    "\"\"\"\n",
    "\n",
    "    # 向 OpenAI API 发出请求以生成比较分析\n",
    "    comparison = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # 指导助手的系统消息\n",
    "            {\"role\": \"user\", \"content\": prompt}  # 包含提示的用户消息\n",
    "        ],\n",
    "        temperature=0.0  # 设置响应生成的温度\n",
    "    )\n",
    "    \n",
    "    # 返回生成的比较分析\n",
    "    return comparison.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在示例 PDF 文档上评估 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义 PDF 路径和查询\n",
    "    pdf_path = \"data/AI_Information.pdf\"\n",
    "    query = \"Transformer 在自然语言处理中的关键应用有哪些？\"\n",
    "    \n",
    "    # 运行 Graph RAG 流水线\n",
    "    result = graph_rag_pipeline(pdf_path, query)\n",
    "    \n",
    "    print(f\"查询：{result['query']}\")\n",
    "    print(f\"响应：{result['response']}\")\n",
    "    print(f\"遍历路径长度：{len(result['traversal_path'])}\")\n",
    "    print(f\"相关块数量：{len(result['relevant_chunks'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "Graph RAG 通过以下方式增强了传统的 RAG 系统：\n",
    "\n",
    "1. **结构化知识表示**：将文档组织为连接的概念图，而不是独立的文本块\n",
    "2. **关系感知检索**：利用概念之间的关系来找到更相关的信息\n",
    "3. **智能遍历**：通过图结构导航以发现相关但可能不直接相似的信息\n",
    "4. **可解释性**：提供清晰的推理路径，显示信息是如何连接的\n",
    "\n",
    "这种方法特别适用于：\n",
    "- 复杂的多步推理查询\n",
    "- 需要综合多个相关概念的问题\n",
    "- 要求高可解释性的应用场景\n",
    "- 处理具有丰富内部关系的文档集合\n",
    "\n",
    "Graph RAG 代表了检索增强生成技术的重要进步，为构建更智能、更具上下文感知能力的问答系统提供了强大的框架。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}