{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用于增强 RAG 系统的上下文压缩\n",
    "\n",
    "上下文压缩技术来提升 RAG 系统的效率。对检索到的文本块进行筛选和压缩，只保留最相关的部分，从而减少噪音并提高响应质量。\n",
    "\n",
    "在为 RAG 检索文档时，可以得到的文本块常常同时包含相关和不相关的信息。上下文压缩有助于：\n",
    "\n",
    "-   移除不相关的句子和段落\n",
    "-   仅聚焦于和查询相关的信息\n",
    "-   最大化我们上下文窗口中的有效信号\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取pdf文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    提取PDF文件中的文本并打印前`num_chars`个字符。\n",
    "\n",
    "    参数：\n",
    "    pdf_path (str): PDF文件的路径。\n",
    "\n",
    "    返回：\n",
    "    str: 从PDF中提取的文本。\n",
    "\n",
    "    \"\"\"\n",
    "    # 打开PDF文件\n",
    "    mypdf = pymupdf.open(pdf_path)\n",
    "    all_text = \"\"  # 初始化一个空字符串来存储提取的文本\n",
    "\n",
    "    # 迭代PDF中的每个页面\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 获取页面\n",
    "        text = page.get_text(\"text\")  # 从页面中提取文本\n",
    "        all_text += text  # 将提取的文本附加到all_text字符串\n",
    "\n",
    "    return all_text  # 返回提取的文本\n",
    "\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    将文本分割为多个块，每个块的大小为n，重叠部分为overlap。\n",
    "    参数：\n",
    "    text: 输入的文本\n",
    "    n: 每个块的大小\n",
    "    overlap: 相邻块之间的重叠部分大小\n",
    "\n",
    "    返回：\n",
    "    文本块列表\n",
    "    \"\"\"\n",
    "    chunks = []  \n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        \n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 百炼服务的base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简易向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    简易的向量存储库。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        添加一个新的项到存储库。\n",
    "\n",
    "        参数:\n",
    "        text (str): 文本内容。\n",
    "        embedding (List[float]): 文本的嵌入向量。\n",
    "        metadata (Dict, optional): 与文本相关的元数据。\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        查找与查询嵌入向量最相似的文本。\n",
    "\n",
    "        参数:\n",
    "        query_embedding (List[float]): 查询的嵌入向量。\n",
    "        k (int, optional): 返回最相似的k个结果。\n",
    "\n",
    "        返回:\n",
    "        List[Dict]: 最相似的文本及其相关信息。\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_in_batches(text_chunks, model=\"text-embedding-v3\", batch_size_limit=10): # 我改成了官方模型名，你可以换回 \"text-embedding-v3\"\n",
    "    \"\"\"\n",
    "    调用 OpenAI 的 Embedding API 来创建文本列表的嵌入向量，处理批处理大小限制。\n",
    "\n",
    "    参数:\n",
    "    text_chunks (List[str]): 需要创建嵌入的文本字符串列表。\n",
    "    model (str): 使用的嵌入模型。\n",
    "    batch_size_limit (int): API 允许的最大批处理大小。根据错误信息，这里是10。\n",
    "\n",
    "    返回:\n",
    "    List[List[float]]: 所有文本的嵌入向量列表。\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    if not text_chunks:\n",
    "        return []\n",
    "\n",
    "    if not isinstance(text_chunks, list): # 确保输入是列表\n",
    "        text_chunks = [text_chunks]\n",
    "\n",
    "    for i in range(0, len(text_chunks), batch_size_limit):\n",
    "        batch = text_chunks[i:i + batch_size_limit]\n",
    "        try:\n",
    "            #print(f\"Processing batch {i//batch_size_limit + 1}, size: {len(batch)}\")\n",
    "            response = client.embeddings.create(\n",
    "                input=batch,\n",
    "                model=model,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "            # 从响应中提取该批次的嵌入向量\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting with chunk: '{batch[0][:50]}...'\")\n",
    "            print(f\"API Error: {e}\")\n",
    "\n",
    "            raise e \n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "def create_embeddings(text, model=\"text-embedding-v3\"):\n",
    "    \"\"\"\n",
    "    字符串向量化\n",
    "    参数:\n",
    "    text (str): 需要创建嵌入的文本字符串。\n",
    "    model (str): 使用的嵌入模型。\n",
    "\n",
    "    返回:\n",
    "    List[float]: 文本的嵌入向量。\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings_in_batches(chunks)\n",
    "\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上下文压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_chunk(chunk, query, compression_type=\"selective\", model=\"qwen-turbo\"):\n",
    "    \"\"\"\n",
    "    压缩给定的文本块以回答特定查询。\n",
    "\n",
    "    参数:\n",
    "    - chunk (str): 要压缩的文本块。\n",
    "    - query (str): 用户的查询。\n",
    "    - compression_type (str): 压缩类型，可选值为 \"selective\", \"summary\", \"extraction\"。\n",
    "    - model (str): 用于生成压缩文本的模型名称。\n",
    "\n",
    "    返回:\n",
    "    - str: 压缩后的文本块。\n",
    "    \"\"\"\n",
    " \n",
    "    if compression_type == \"selective\":\n",
    "        system_prompt = \"\"\"You are an expert at information filtering. \n",
    "        Your task is to analyze a document chunk and extract ONLY the sentences or paragraphs that are directly \n",
    "        relevant to the user's query. Remove all irrelevant content.\n",
    "\n",
    "        Your output should:\n",
    "        1. ONLY include text that helps answer the query\n",
    "        2. Preserve the exact wording of relevant sentences (do not paraphrase)\n",
    "        3. Maintain the original order of the text\n",
    "        4. Include ALL relevant content, even if it seems redundant\n",
    "        5. EXCLUDE any text that isn't relevant to the query\n",
    "\n",
    "        Format your response as plain text with no additional comments.\"\"\"\n",
    "    elif compression_type == \"summary\":\n",
    "        system_prompt = \"\"\"You are an expert at summarization. \n",
    "        Your task is to create a concise summary of the provided chunk that focuses ONLY on \n",
    "        information relevant to the user's query.\n",
    "\n",
    "        Your output should:\n",
    "        1. Be brief but comprehensive regarding query-relevant information\n",
    "        2. Focus exclusively on information related to the query\n",
    "        3. Omit irrelevant details\n",
    "        4. Be written in a neutral, factual tone\n",
    "\n",
    "        Format your response as plain text with no additional comments.\"\"\"\n",
    "    else: \n",
    "        system_prompt = \"\"\"You are an expert at information extraction.\n",
    "        Your task is to extract ONLY the exact sentences from the document chunk that contain information relevant \n",
    "        to answering the user's query.\n",
    "\n",
    "        Your output should:\n",
    "        1. Include ONLY direct quotes of relevant sentences from the original text\n",
    "        2. Preserve the original wording (do not modify the text)\n",
    "        3. Include ONLY sentences that directly relate to the query\n",
    "        4. Separate extracted sentences with newlines\n",
    "        5. Do not add any commentary or additional text\n",
    "\n",
    "        Format your response as plain text with no additional comments.\"\"\"\n",
    "\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Document Chunk:\n",
    "        {chunk}\n",
    "\n",
    "        Extract only the content relevant to answering this query.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        extra_body={\n",
    "            \"enable_thinking\": False,\n",
    "            \"temperature\": 0\n",
    "            }\n",
    "    )\n",
    "    \n",
    "    compressed_chunk = response.choices[0].message.content.strip()\n",
    "    \n",
    "    original_length = len(chunk)\n",
    "    compressed_length = len(compressed_chunk)\n",
    "    compression_ratio = (original_length - compressed_length) / original_length * 100\n",
    "    \n",
    "    return compressed_chunk, compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_compress_chunks(chunks, query, compression_type=\"selective\", model=\"qwen-turbo\"):\n",
    "    \"\"\"\n",
    "    压缩一批文本块。\n",
    "\n",
    "    Args:\n",
    "        chunks (list): 文本块列表\n",
    "        query (str): 用户查询\n",
    "        compression_type (str): 压缩类型（\"selective\"、\"summary\" 或 \"extraction\"）\n",
    "        model (str): 用于压缩的 LLM 模型\n",
    "\n",
    "    Returns:\n",
    "        list: 压缩后的文本块列表\n",
    "    \"\"\"\n",
    "    print(f\"Compressing {len(chunks)} chunks...\") \n",
    "    results = [] \n",
    "    total_original_length = 0  \n",
    "    total_compressed_length = 0  \n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Compressing chunk {i+1}/{len(chunks)}...\")  \n",
    "        \n",
    "        compressed_chunk, compression_ratio = compress_chunk(chunk, query, compression_type, model)\n",
    "        results.append((compressed_chunk, compression_ratio)) \n",
    "        \n",
    "        total_original_length += len(chunk) \n",
    "        total_compressed_length += len(compressed_chunk)  \n",
    "    \n",
    "    overall_ratio = (total_original_length - total_compressed_length) / total_original_length * 100\n",
    "    print(f\"Overall compression ratio: {overall_ratio:.2f}%\") \n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"qwen3-4b\"):\n",
    "    print(\"Generating response using relevant segments as context...\")\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
    "    The context consists of document segments that have been retrieved as relevant to the user's query.\n",
    "    Use the information from these segments to provide a comprehensive and accurate answer.\n",
    "    If the context doesn't contain relevant information to answer the question, say so clearly.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:{context}\n",
    "        Question: {query}\n",
    "        Please provide a helpful answer based on the context provided.\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \n",
    "        extra_body={\n",
    "            \"enable_thinking\": False,\n",
    "            \"temperature\": 0\n",
    "            }\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_compression(pdf_path, query, k=10, compression_type=\"selective\", model=\"qwen3-4b\"):\n",
    "\n",
    "    print(\"\\n=== RAG WITH CONTEXTUAL COMPRESSION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Compression type: {compression_type}\")\n",
    "    \n",
    "\n",
    "    vector_store = process_document(pdf_path)\n",
    "\n",
    "    query_embedding = create_embeddings(query)\n",
    "\n",
    "    print(f\"Retrieving top {k} chunks...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "\n",
    "    compressed_results = batch_compress_chunks(retrieved_chunks, query, compression_type, model)\n",
    "    compressed_chunks = [result[0] for result in compressed_results]\n",
    "    compression_ratios = [result[1] for result in compressed_results]\n",
    "\n",
    "    filtered_chunks = [(chunk, ratio) for chunk, ratio in zip(compressed_chunks, compression_ratios) if chunk.strip()]\n",
    "    \n",
    "    if not filtered_chunks:\n",
    "\n",
    "        print(\"Warning: All chunks were compressed to empty strings. Using original chunks.\")\n",
    "        filtered_chunks = [(chunk, 0.0) for chunk in retrieved_chunks]\n",
    "    else:\n",
    "        compressed_chunks, compression_ratios = zip(*filtered_chunks)\n",
    "    \n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(compressed_chunks)\n",
    "    \n",
    "\n",
    "    print(\"Generating response based on compressed chunks...\")\n",
    "    response = generate_response(query, context, model)\n",
    "\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"original_chunks\": retrieved_chunks,\n",
    "        \"compressed_chunks\": compressed_chunks,\n",
    "        \"compression_ratios\": compression_ratios,\n",
    "        \"context_length_reduction\": f\"{sum(compression_ratios)/len(compression_ratios):.2f}%\",\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比没有压缩的RAG流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_rag(pdf_path, query, k=10, model=\"qwen3-4b\"):\n",
    "\n",
    "    print(\"\\n=== STANDARD RAG ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    query_embedding = create_embeddings_in_batches(query)\n",
    "\n",
    "    print(f\"Retrieving top {k} chunks...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "\n",
    "    print(\"Generating response...\")\n",
    "    response = generate_response(query, context, model)\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, responses, reference_answer):\n",
    "\n",
    "\n",
    "    system_prompt = \"\"\"You are an objective evaluator of RAG responses. Compare different responses to the same query\n",
    "    and determine which is most accurate, comprehensive, and relevant to the query.\"\"\"\n",
    "    \n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "\n",
    "    Reference Answer: {reference_answer}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    for method, response in responses.items():\n",
    "        user_prompt += f\"\\n{method.capitalize()} Response:\\n{response}\\n\"\n",
    "    \n",
    "    # Add the evaluation criteria to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "    Please evaluate these responses based on:\n",
    "    1. Factual accuracy compared to the reference\n",
    "    2. Comprehensiveness - how completely they answer the query\n",
    "    3. Conciseness - whether they avoid irrelevant information\n",
    "    4. Overall quality\n",
    "\n",
    "    Rank the responses from best to worst with detailed explanations.\n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "    evaluation_response = client.chat.completions.create(\n",
    "        model=\"qwen-plus\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "\n",
    "    return evaluation_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_compression(pdf_path, query, reference_answer=None, compression_types=[\"selective\", \"summary\", \"extraction\"]):\n",
    "\n",
    "    print(\"\\n=== EVALUATING CONTEXTUAL COMPRESSION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "\n",
    "    standard_result = standard_rag(pdf_path, query)\n",
    "    \n",
    "\n",
    "    compression_results = {}\n",
    "    \n",
    "\n",
    "    for comp_type in compression_types:\n",
    "        print(f\"\\nTesting {comp_type} compression...\")\n",
    "        compression_results[comp_type] = rag_with_compression(pdf_path, query, compression_type=comp_type)\n",
    "    \n",
    "\n",
    "    responses = {\n",
    "        \"standard\": standard_result[\"response\"]\n",
    "    }\n",
    "    for comp_type in compression_types:\n",
    "        responses[comp_type] = compression_results[comp_type][\"response\"]\n",
    "    \n",
    "\n",
    "    if reference_answer:\n",
    "        evaluation = evaluate_responses(query, responses, reference_answer)\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(evaluation)\n",
    "    else:\n",
    "        evaluation = \"No reference answer provided for evaluation.\"\n",
    "    \n",
    "\n",
    "    metrics = {}\n",
    "    for comp_type in compression_types:\n",
    "        metrics[comp_type] = {\n",
    "            \"avg_compression_ratio\": f\"{sum(compression_results[comp_type]['compression_ratios'])/len(compression_results[comp_type]['compression_ratios']):.2f}%\",\n",
    "            \"total_context_length\": len(\"\\n\\n\".join(compression_results[comp_type]['compressed_chunks'])),\n",
    "            \"original_context_length\": len(\"\\n\\n\".join(standard_result['chunks']))\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"responses\": responses,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"metrics\": metrics,\n",
    "        \"standard_result\": standard_result,\n",
    "        \"compression_results\": compression_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATING CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "\n",
      "=== STANDARD RAG ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Retrieving top 10 chunks...\n",
      "Generating response...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The ethical concerns surrounding the use of AI in decision-making, as outlined in the context, include:\n",
      "\n",
      "1. **Bias and Fairness**: AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair or discriminatory outcomes. Ensuring fairness and mitigating bias in AI systems is a critical challenge.\n",
      "\n",
      "2. **Transparency and Explainability**: Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions. Enhancing transparency and explainability is crucial for building trust and accountability.\n",
      "\n",
      "3. **Privacy and Data Protection**: AI systems often rely on large amounts of data, raising concerns about privacy and data protection. Protecting sensitive information and ensuring responsible data handling are essential.\n",
      "\n",
      "4. **Accountability and Responsibility**: Establishing accountability and responsibility for AI systems is essential for addressing potential harms and ensuring ethical behavior. This includes defining roles and responsibilities for developers, deployers, and users of AI systems.\n",
      "\n",
      "5. **Autonomy and Control**: As AI systems become more autonomous, questions arise about control, accountability, and the potential for unintended consequences. Establishing clear guidelines and ethical frameworks for AI development and deployment is crucial.\n",
      "\n",
      "6. **Job Displacement**: The automation capabilities of AI have raised concerns about job displacement, particularly in industries with repetitive or routine tasks. Addressing the potential economic and social impacts of AI-driven automation is a key challenge.\n",
      "\n",
      "7. **Weaponization of AI**: The potential use of AI in autonomous weapons systems raises significant ethical and security concerns. International discussions and regulations are needed to address the risks associated with AI-powered weapons.\n",
      "\n",
      "These concerns highlight the need for careful consideration of ethical implications in the development and deployment of AI systems to ensure they are fair, transparent, accountable, and beneficial to society.\n",
      "\n",
      "Testing selective compression...\n",
      "\n",
      "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "Compression type: selective\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Retrieving top 10 chunks...\n",
      "Compressing 10 chunks...\n",
      "Compressing chunk 1/10...\n",
      "Compressing chunk 2/10...\n",
      "Compressing chunk 3/10...\n",
      "Compressing chunk 4/10...\n",
      "Compressing chunk 5/10...\n",
      "Compressing chunk 6/10...\n",
      "Compressing chunk 7/10...\n",
      "Compressing chunk 8/10...\n",
      "Compressing chunk 9/10...\n",
      "Compressing chunk 10/10...\n",
      "Overall compression ratio: 58.38%\n",
      "Generating response based on compressed chunks...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The ethical concerns surrounding the use of AI in decision-making include:\n",
      "\n",
      "1. **Bias and Fairness**: AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair or discriminatory outcomes. Ensuring fairness requires careful data collection, algorithm design, and ongoing monitoring.\n",
      "\n",
      "2. **Transparency and Explainability**: Many AI systems, especially deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions. Enhancing transparency and explainability is crucial for building trust and accountability.\n",
      "\n",
      "3. **Privacy and Data Security**: AI systems often rely on large amounts of data, raising concerns about privacy and data protection. Responsible data handling, privacy-preserving techniques, and compliance with data protection regulations are essential.\n",
      "\n",
      "4. **Accountability and Responsibility**: Establishing accountability for AI systems is essential. This includes defining roles and responsibilities for developers, deployers, and users to address potential harms and ensure ethical behavior.\n",
      "\n",
      "5. **Job Displacement**: The automation capabilities of AI raise concerns about job displacement, particularly in industries with repetitive or routine tasks. Addressing the economic and social impacts of AI-driven automation is a key challenge.\n",
      "\n",
      "6. **Autonomy and Control**: As AI systems become more autonomous, questions arise about control, accountability, and the potential for unintended consequences. Clear guidelines and ethical frameworks are needed to manage these risks.\n",
      "\n",
      "7. **Weaponization of AI**: The potential use of AI in autonomous weapons systems raises significant ethical and security concerns. International discussions and regulations are needed to address these risks.\n",
      "\n",
      "These concerns highlight the need for ethical guidelines, regulation, and governance to ensure that AI is developed and deployed responsibly, promoting fairness, transparency, and the protection of human rights.\n",
      "\n",
      "Testing summary compression...\n",
      "\n",
      "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "Compression type: summary\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Retrieving top 10 chunks...\n",
      "Compressing 10 chunks...\n",
      "Compressing chunk 1/10...\n",
      "Compressing chunk 2/10...\n",
      "Compressing chunk 3/10...\n",
      "Compressing chunk 4/10...\n",
      "Compressing chunk 5/10...\n",
      "Compressing chunk 6/10...\n",
      "Compressing chunk 7/10...\n",
      "Compressing chunk 8/10...\n",
      "Compressing chunk 9/10...\n",
      "Compressing chunk 10/10...\n",
      "Overall compression ratio: 61.89%\n",
      "Generating response based on compressed chunks...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The ethical concerns surrounding the use of AI in decision-making include the following:\n",
      "\n",
      "1. **Bias and Fairness**: AI systems may inherit and amplify biases from training data, leading to unfair or discriminatory outcomes. Ensuring fairness is crucial to prevent harm to individuals or groups.\n",
      "\n",
      "2. **Transparency and Explainability**: Many AI systems, especially deep learning models, act as \"black boxes,\" making it difficult to understand their decision-making processes. Techniques like Explainable AI (XAI) are used to enhance transparency and trust.\n",
      "\n",
      "3. **Privacy and Data Protection**: AI systems often rely on large datasets, raising concerns about privacy and data security. Protecting personal information is essential to maintain user trust.\n",
      "\n",
      "4. **Accountability and Responsibility**: The lack of transparency in AI decision-making can make it challenging to hold entities accountable for errors or harmful actions. Establishing clear lines of responsibility is important.\n",
      "\n",
      "5. **Safety and Robustness**: Ensuring that AI systems are reliable and safe is critical to prevent unintended consequences and potential harm.\n",
      "\n",
      "6. **Control and Autonomy**: As AI systems become more independent, there are concerns about human control and autonomy. Ethical frameworks are needed to address these issues.\n",
      "\n",
      "7. **Job Displacement**: AI's automation capabilities raise concerns about job loss and economic impact.\n",
      "\n",
      "8. **Weaponization**: The use of AI in autonomous weapons systems raises significant ethical and security concerns, necessitating international regulations.\n",
      "\n",
      "9. **Public Engagement and Education**: Involving the public in discussions about AI and providing education on its implications is important for building trust and ensuring ethical development.\n",
      "\n",
      "These concerns highlight the need for ethical guidelines, regulation, and international collaboration to ensure that AI is developed and used responsibly.\n",
      "\n",
      "Testing extraction compression...\n",
      "\n",
      "=== RAG WITH CONTEXTUAL COMPRESSION ===\n",
      "Query: What are the ethical concerns surrounding the use of AI in decision-making?\n",
      "Compression type: extraction\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Retrieving top 10 chunks...\n",
      "Compressing 10 chunks...\n",
      "Compressing chunk 1/10...\n",
      "Compressing chunk 2/10...\n",
      "Compressing chunk 3/10...\n",
      "Compressing chunk 4/10...\n",
      "Compressing chunk 5/10...\n",
      "Compressing chunk 6/10...\n",
      "Compressing chunk 7/10...\n",
      "Compressing chunk 8/10...\n",
      "Compressing chunk 9/10...\n",
      "Compressing chunk 10/10...\n",
      "Overall compression ratio: 65.48%\n",
      "Generating response based on compressed chunks...\n",
      "Generating response using relevant segments as context...\n",
      "\n",
      "=== RESPONSE ===\n",
      "The ethical concerns surrounding the use of AI in decision-making, as outlined in the context, include:\n",
      "\n",
      "1. **Bias and Fairness**: AI systems can inherit and amplify biases present in the data they are trained on, leading to unfair or discriminatory outcomes. Ensuring fairness and mitigating bias is a critical challenge.\n",
      "\n",
      "2. **Transparency and Explainability**: Many AI systems, particularly deep learning models, are \"black boxes,\" making it difficult to understand how they arrive at their decisions. Enhancing transparency and explainability is crucial for building trust and accountability.\n",
      "\n",
      "3. **Privacy and Data Protection**: AI systems often rely on large amounts of data, raising concerns about privacy and data protection. Responsible data handling, implementing privacy-preserving techniques, and complying with data protection regulations are essential.\n",
      "\n",
      "4. **Accountability and Responsibility**: Establishing accountability and responsibility for AI systems is essential. This includes defining roles and responsibilities for developers, deployers, and users of AI systems to address potential harms and ensure ethical behavior.\n",
      "\n",
      "5. **Autonomy and Control**: As AI systems become more autonomous, questions arise about control, accountability, and the potential for unintended consequences. Clear guidelines and ethical frameworks are needed for responsible development and deployment.\n",
      "\n",
      "6. **Safety and Security**: Ensuring the safety and security of AI systems is important to prevent misuse and protect sensitive information.\n",
      "\n",
      "These concerns highlight the need for ethical guidelines, regulation, and governance to ensure that AI systems are developed and used in a way that benefits society while minimizing harm.\n",
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "### Evaluation and Ranking of Responses\n",
      "\n",
      "#### **1. Standard Response**\n",
      "- **Factual Accuracy**: High. The response accurately covers all the key ethical concerns mentioned in the reference answer (bias, transparency, privacy, accountability, job displacement) and adds relevant additional points such as autonomy/control and weaponization of AI. These additions are well-grounded and align with broader discussions in AI ethics.\n",
      "- **Comprehensiveness**: Excellent. It provides a thorough breakdown of each concern, offering context and potential solutions (e.g., \"Enhancing transparency and explainability is crucial for building trust\"). It also includes a point about weaponization, which is an important but less commonly discussed issue.\n",
      "- **Conciseness**: Good. While it is detailed, it avoids unnecessary elaboration and stays focused on the query. Each point is concise yet informative.\n",
      "- **Overall Quality**: Very high. This response strikes a great balance between depth and clarity, making it highly suitable for someone looking for a comprehensive understanding of the topic.\n",
      "\n",
      "**Rank: 1**\n",
      "\n",
      "---\n",
      "\n",
      "#### **2. Summary Response**\n",
      "- **Factual Accuracy**: High. It mirrors the reference answer closely, covering bias, transparency, privacy, accountability, safety, control, job displacement, and weaponization. All points align with established ethical concerns in AI decision-making.\n",
      "- **Comprehensiveness**: Very good. It expands slightly beyond the reference by including safety/robustness and public engagement/education, which are valid considerations in AI ethics. However, it doesn’t delve into these topics as deeply as the Standard Response.\n",
      "- **Conciseness**: Slightly less concise than the Standard Response due to some repetition (e.g., discussing both \"control\" and \"autonomy,\" which overlap). Additionally, the inclusion of \"public engagement and education\" feels somewhat tangential to the core query.\n",
      "- **Overall Quality**: High. It offers a strong overview but could be streamlined further to avoid minor redundancies.\n",
      "\n",
      "**Rank: 2**\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. Extraction Response**\n",
      "- **Factual Accuracy**: High. It directly extracts the main points from the reference answer without introducing errors. Points like bias, transparency, privacy, accountability, autonomy, and safety/security are all correct.\n",
      "- **Comprehensiveness**: Moderate. While it covers most of the key areas, it omits certain nuances present in the reference answer, such as job displacement and the concentration of power among tech companies. It also lacks the additional insights provided by the Standard and Summary Responses.\n",
      "- **Conciseness**: Very good. It focuses strictly on the essential elements and avoids extraneous details.\n",
      "- **Overall Quality**: Solid but not exceptional. Its brevity makes it useful for quick summaries, but it sacrifices some depth compared to other responses.\n",
      "\n",
      "**Rank: 3**\n",
      "\n",
      "---\n",
      "\n",
      "#### **4. Selective Response**\n",
      "- **Factual Accuracy**: High. Like the Extraction Response, it faithfully reproduces the key points from the reference answer while adding minor elaborations.\n",
      "- **Comprehensiveness**: Moderate. Similar to the Extraction Response, it misses out on some nuances, such as the concentration of power among tech companies or specific examples of harm caused by biased AI. It also repeats some ideas (e.g., \"autonomy and control\" overlaps significantly with \"accountability\").\n",
      "- **Conciseness**: Adequate but could improve. Some sections feel repetitive or overly verbose (e.g., reiterating that ensuring fairness requires careful data collection), which detracts from its overall effectiveness.\n",
      "- **Overall Quality**: Decent but not outstanding. It does a satisfactory job of summarizing the main issues but lacks the polish and depth of higher-ranked responses.\n",
      "\n",
      "**Rank: 4**\n",
      "\n",
      "---\n",
      "\n",
      "### Final Rankings\n",
      "1. **Standard Response** – Most accurate, comprehensive, and well-balanced.\n",
      "2. **Summary Response** – Comprehensive and accurate but slightly redundant.\n",
      "3. **Extraction Response** – Accurate and concise but lacking in depth.\n",
      "4. **Selective Response** – Accurate but repetitive and less comprehensive.\n",
      "\n",
      "Each response has its strengths, but the **Standard Response** stands out as the best overall due to its depth, clarity, and inclusion of relevant supplementary information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_path = \"data/AI_Information.pdf\" \n",
    "\n",
    "\n",
    "query = \"What are the ethical concerns surrounding the use of AI in decision-making?\"  \n",
    "\n",
    "\n",
    "reference_answer = \"\"\"  \n",
    "The use of AI in decision-making raises several ethical concerns.  \n",
    "- Bias in AI models can lead to unfair or discriminatory outcomes, especially in critical areas like hiring, lending, and law enforcement.  \n",
    "- Lack of transparency and explainability in AI-driven decisions makes it difficult for individuals to challenge unfair outcomes.  \n",
    "- Privacy risks arise as AI systems process vast amounts of personal data, often without explicit consent.  \n",
    "- The potential for job displacement due to automation raises social and economic concerns.  \n",
    "- AI decision-making may also concentrate power in the hands of a few large tech companies, leading to accountability challenges.  \n",
    "- Ensuring fairness, accountability, and transparency in AI systems is essential for ethical deployment.  \n",
    "\"\"\"  \n",
    "\n",
    "results = evaluate_compression(  \n",
    "    pdf_path=pdf_path,  \n",
    "    query=query,  \n",
    "    reference_answer=reference_answer,  \n",
    "    compression_types=[\"selective\", \"summary\", \"extraction\"]  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
